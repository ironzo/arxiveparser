[
    {
        "id": "2508.00679v1",
        "title": "Segment First, Retrieve Better: Realistic Legal Search via Rhetorical\n  Role-Based Queries",
        "authors": [
            "Shubham Kumar Nigam",
            "Tanmay Dubey",
            "Noel Shallum",
            "Arnab Bhattacharya"
        ],
        "Abstract": "Abstract Legal precedent retrieval is a cornerstone of the common law system, governed by the principle of stare decisis, which demands consistency in judicial decisions. However, the growing complexity and volume of legal documents challenge traditional retrieval methods. TraceRetriever mirrors real-world legal search by operating with limited case information, extracting only rhetorically significant segments instead of requiring complete documents. Our pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining initial results through Reciprocal Rank Fusion before final re-ranking. Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets, TraceRetriever addresses growing document volume challenges while aligning with practical search constraints, reliable and scalable foundation for precedent retrieval enhancing legal research when only partial case knowledge is available.",
        "Main": {
            "1 Introduction": [
                "The common law system’s foundation rests upon the principle of stare decisis , mandating judicial adherence to precedents established in prior rulings when addressing analogous issues and facts within the same jurisdiction. As legal documentation grows in complexity and volume, sophisticated Natural Language Processing (NLP) techniques become indispensable for understanding, analyzing, and retrieving relevant precedents. TraceRetriever plays a crucial role in upholding stare decisis , facilitating the identification of past judgments with similar legal contexts to ensure consistent application of the law. The sheer volume of legal resources, including judgments, statutes, and regulations, poses a significant challenge for legal professionals seeking pertinent precedents, underscoring the urgent need for effective retrieval mechanisms.",
                "Figure 1: Illustration of rhetorical role segmentation in a legal document. The left side shows the original excerpt, while the right side displays the labeled segments. In our approach, only relevant segments such as Facts and Issue are retained to emulate real-world legal case retrieval scenarios, where complete information like Reasoning or Decision may not be available at query time (Nigam et al., 2025 ) .",
                "A notable limitation in much of the existing work on automated precedent retrieval is its reliance on using entire prior case documents as queries. This approach deviates significantly from real-world legal practice, where lawyers typically formulate search queries based on specific factual details and legal issues extracted from the case at hand, often with limited initial information. To address this gap, this paper tackles the challenge of mimicking real-world legal search scenarios in TraceRetriever by proposing a novel heuristic approach. Our methodology strategically integrates the complementary strengths of a keyword-based model (BM25), a semantic Vector Database, and a fine-grained Cross-Encoder for re-ranking. A key innovation of our work lies in utilizing a trained Hierarchical Bidirectional LSTM (HierBiLSTM) model by (Bhattacharya et al., 2019 ) to classify sentences within legal documents into distinct rhetorical roles. We then leverage the role segments, identified through this classification, as the query for our retrieval pipeline. This deliberate use of limited, rhetorically-informed query components directly mirrors the information scarcity often encountered in practical legal research. The core problem this paper addresses is therefore the development of a TraceRetriever system that effectively operates with limited, contextually relevant information, thereby more accurately reflecting real-world legal search processes.",
                "To evaluate the effectiveness of our proposed TraceRetriever pipeline, we conducted experiments on two established legal datasets: the Indian Legal Text Understanding and Reasoning (IL-PCR) dataset (Joshi et al., 2023 ) and the Competition on Legal Information Extraction and Entailment (COLIEE) 2025 dataset. Our pipeline employs a heuristic approach that strategically integrates the strengths of three distinct retrieval models: a semantic Vector Database, the BM25 algorithm, and a more nuanced Cross-Encoder. To further refine the initial retrieval results from the Vector Database and BM25, we implemented Reciprocal Rank Fusion (RRF), a robust re-ranking technique.",
                "In our TraceRetriever pipeline, we established BM25 as a robust baseline, representing a traditional keyword-based approach to information retrieval. To enhance the relevance and accuracy of our results, we implemented a sophisticated re-ranking strategy that leverages both semantic understanding and fine-grained interaction. Specifically, we employed Cross-Encoders to re-rank the top-k documents initially retrieved by two distinct methods: the lexical matching of BM25 and the semantic similarity captured by our Vector Database (a bi-encoder-based approach). This multi-faceted strategy effectively integrates the strengths of three complementary retrieval paradigms:",
                "Our key contributions are: 1. A realistic legal retrieval strategy using rhetorical role-based queries reflecting limited-information scenarios. 2. Development of TraceRetriever: A hybrid pipeline integrating BM25, vector search, and cross-encoder re-ranking.",
                "For the sake of reproducibility, we have made our dataset, code, and RAG-based pipeline implementation via an github repository 1 1 1 https://github.com/ShubhamKumarNigam/Legal_IR ."
            ],
            "2 Related Work": [
                "Legal case retrieval has witnessed a rapid transformation with the advent of LLMs, RAG pipelines, and rhetorical role labeling. Traditionally, legal information retrieval relied heavily on lexical matching (e.g., BM25), which struggled to handle the semantic and structural nuances of legal texts. Recent innovations focus on improving retrieval accuracy by leveraging domain-specific embeddings, legal document structures, and rhetorical role understanding.",
                "Several systems have explored enhancing legal QA and retrieval using hybrid architectures. (Wiratunga et al., 2024 ) integrates Case-Based Reasoning with RAG to improve contextual relevance and factual correctness in legal question-answering. Similarly, (Panchal et al., 2025 ) utilizes FAISS and DeepSeek embeddings to make Indian legal knowledge accessible through a chatbot interface.",
                "Another significant trend is the use of rhetorical roles in structuring legal texts. (Bhattacharya et al., 2019 ; Malik et al., 2022 ) pioneered rhetorical role classification in Indian legal judgments, showing that deep neural architectures such as BiLSTM-CRF and multi-task learning can outperform traditional methods. (Marino et al., 2023 ) further advanced this by stacking transformers over LEGAL-BERT to capture inter-sentence dependencies for rhetorical role classification across multilingual legal datasets. These works collectively demonstrate the feasibility and utility of segmenting legal documents into roles such as Facts , Issues , and Reasoning categories that are highly valuable for information extraction and retrieval. In recent studies, (Bhattacharya et al., 2019 ) proposed a CRF-BiLSTM model specifically for as signing rhetorical roles to sentences in Indian legal documents.",
                "In the context of document-to-document legal retrieval, methods like (Althammer et al., 2022 ) , (Ma et al., 2023 ) , and (Li et al., 2023 ) aim to overcome the challenges of long input lengths and weak semantic relevance by employing paragraph aggregation, structure-aware pretraining, and custom contrastive loss functions. Meanwhile, (Tang et al., 2023 ) and (Tang et al., 2024 ) take a graph based approach, modeling the connectivity between cases via attributed case graphs or global semantic networks to achieve state-of-the-art performance. (Nigam et al., 2022 ) presents a cascaded retrieval framework that integrates BM25 for lexical matching with Sentence BERT and Sent2Vec for semantic understanding. Interestingly, results show that BM25 alone often outperforms neural models, reaffirming the robustness and relevance of lexical approaches in legal case retrieval.",
                "Beyond traditional lexical and semantic methods, several recent studies have explored innovative architectures to enhance legal case retrieval by addressing challenges such as long document length, complex legal semantics, and noisy or sparse queries. (Hu et al., 2022 ) proposes a retrieval method grounded in legal facts by combining topic modeling with BERT-based paragraph aggregation, offering more accurate semantic representations tailored to the legal domain. Similarly, (Shao et al., 2020 ) focuses on paragraph-level interactions, modeling fine-grained relationships between query and candidate cases to improve relevance estimation using a cascade framework and BERT finetuned on legal entailment tasks. Addressing structural and causal reasoning, (Zhang et al., 2023 ) introduces a counterfactual graph learning approach, which transforms legal cases into graphs of legal elements and enhances retrieval via counterfactual data augmentation and relational graph neural networks. Meanwhile, (Zhou et al., 2023 ) employ large language models (LLMs) to distill salient query content, showing that query reformulation using LLMs improves retrieval even in long, noisy legal queries. Structural reasoning is also emphasized in SLR (Zhou et al., 2023 ) , which incorporates both internal (document segmentation into roles like Facts, Holding, Decision) and external (charge relationship graphs) structures to enhance retrieval accuracy via a learning-to-rank approach, (Santosh et al., 2025 ) enhances prior case retrieval by generating legal concepts from the factual section of a query case to capture semantic intent. Collectively, these works highlight a growing trend toward structurally aware, semantically enriched, and role-sensitive retrieval models supporting the need for rhetorical role-driven query formulations in real-world legal search settings.",
                "While these systems improve retrieval through structure, semantics, or scale, few explicitly address the limited-information retrieval scenario commonly encountered in real-world legal practice, where queries often arise from partial knowledge, such as only the Facts or Issues of a case. The (Deng et al., 2024 ) framework approaches this partially by reformulating legal documents into interpretable sub-facts using LLMs, but it does not explicitly tie these sub-facts to rhetorical roles.",
                "In contrast to general-purpose document retrieval, (Joshi et al., 2023 ) propose U-CREAT, an unsupervised retrieval framework that extracts and matches event tuples consisting of predicates and their arguments from entire legal documents. However, U-CREAT still requires parsing the full document to extract events and does not leverage explicit legal segmentation such as rhetorical roles."
            ],
            "3 Task Description": [
                "The goal of this task is to develop models capable of retrieving the most relevant prior legal cases for a given query case, with a novel emphasis on mimicking realistic legal reasoning workflows. Unlike previous work that provides entire case documents as input queries to retrieval models, we constrain the query representation by leveraging rhetorical role segmentation. This segmentation reflects how legal professionals typically reason over and search with focused portions of a case, such as facts, issues, or arguments, rather than the full text.",
                "Let Q = { q 1 , q 2 , … , q p } Q=\\{q_{1},q_{2},\\dots,q_{p}\\} italic_Q = { italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_q start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT } be a set of query legal cases, where each q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is a segmented case document composed of rhetorical roles:",
                "q i = { Facts i , Issues i , Arguments i , … } q_{i}=\\{\\texttt{Facts}_{i},\\texttt{Issues}_{i},\\texttt{Arguments}_{i},\\dots\\} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { Facts start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , Issues start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , Arguments start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , … }",
                "Rather than passing the full q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as a monolithic document, we present the segmented roles (individually or in combination) to retrieval models to enable fine-grained relevance modeling. This design encourages the system to focus on legally salient information while ignoring irrelevant or verbose content, thus improving efficiency and interpretability.",
                "Let D = { d 1 , d 2 , … , d n } D=\\{d_{1},d_{2},\\dots,d_{n}\\} italic_D = { italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } be a corpus of precedent legal documents. The objective is to retrieve a ranked list of k k italic_k relevant documents R i = { r i 1 , r i 2 , … , r i k } ⊆ D R_{i}=\\{r_{i1},r_{i2},\\dots,r_{ik}\\}\\subseteq D italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_r start_POSTSUBSCRIPT italic_i 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_i 2 end_POSTSUBSCRIPT , … , italic_r start_POSTSUBSCRIPT italic_i italic_k end_POSTSUBSCRIPT } ⊆ italic_D for each query q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , where documents are ranked by their relevance.",
                "We define a retrieval scoring function:",
                "g : Q × D → ℝ g:Q\\times D\\rightarrow\\mathbb{R} italic_g : italic_Q × italic_D → blackboard_R",
                "where g ( q i , d j ) g(q_{i},d_{j}) italic_g ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) outputs a relevance score indicating the degree to which the prior legal document d j d_{j} italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is relevant to the query q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . The retrieved list R i R_{i} italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT for a query q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is then constructed by selecting the top k k italic_k documents from D D italic_D based on their relevance scores: R i = top- k { d j ∈ D ∣ g ( q i , d j ) is high } R_{i}=\\text{top-}k\\{d_{j}\\in D\\mid g(q_{i},d_{j})\\text{ is high}\\} italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = top- italic_k { italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∈ italic_D ∣ italic_g ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) is high } The input to the system is a legal query q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , and the output is a ranked list of k k italic_k prior legal documents R i R_{i} italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ordered by their relevance to the query."
            ],
            "4 Dataset": [
                "To support research in the domain of Prior Case Retrieval (PCR), we utilize the IL-PCR (Indian Legal Prior Case Retrieval) corpus, a large-scale collection of Indian legal documents comprising 7,070 English-language case texts by (Joshi et al., 2023 ) . This corpus enables the development and benchmarking of retrieval systems specifically tailored to the Indian legal system.",
                "Dataset COLIEE’25 IL-PCR # Documents 9498 7070 Avg. Document Size 4759.79 8093.19 # Query Documents 2077 1182 Vocabulary Size 426,118 113,340 Total Citation Links 8640 8008 Avg. Citations per Query 4.16 6.775 Language English English Legal System Canadian Indian Table 1: Comparison of the IL-PCR corpus (Joshi et al., 2023 ) with the COLIEE’25 dataset.",
                {
                    "4.1 Overview of Dataset": [
                        "The IL-PCR corpus was created by collecting case documents from the public domain through the IndianKanoon website 2 2 2 https://indiankanoon.org/ . The initial set comprises the 100 most-cited Supreme Court of India (SCI) judgments, referred to as the zero-hop set . To increase citation density, cases cited within these judgments (the one-hop set ) were also collected. This hierarchical collection approach ensures that each document has multiple cited cases, allowing for robust retrieval evaluation (Joshi et al., 2023 ) . Following standard preprocessing, empty or invalid cases were discarded. The resulting corpus was partitioned into training (70%), validation (10%), and test (20%) splits."
                    ]
                },
                {
                    "4.2 Preprocessing": [
                        "The preprocessing pipeline includes named entity normalization using spaCy’s NER model, alongside a manually curated gazetteer. This standardization improves the generalizability of learned representations. Hyperlinked citations in the documents were replaced with a standardized token <CITATION> , while references to statutes and laws were retained, aligning with the task focus on case retrieval rather than statute retrieval. Additionally, an alternate version of the dataset removes entire sentences containing citations, as discussed in (Joshi et al., 2023 ) ."
                    ]
                }
            ],
            "5 Methodology": [
                "This section elucidates the TraceRetriever methodology, a multi-stage framework designed for effective prior case retrieval, particularly when initiated with partial case details. Our approach integrates advanced NLP techniques, starting with rhetorical role annotation to enable targeted querying of key document sections. We then employ a hybrid retrieval strategy, combining semantic vector search with lexical BM25 matching on a focused candidate set. The resulting ranked lists are fused using RRF, followed by a deep semantic re-ranking via a cross-encoder.",
                {
                    "5.1 Rhetorical Role Annotation of Legal Documents": [
                        "The initial stage of our methodology involves enriching legal documents with rhetorical role annotations at the sentence level. To achieve this, we first perform sentence segmentation using the spaCy library. We implement the BiLSTM-CRF architecture introduced by (Bhattacharya et al., 2019 ) , which integrates a BiLSTM network with a Conditional Random Field (CRF) layer. The model takes as input sentence embeddings generated using a sent2vec model trained specifically on Indian Supreme Court judgments. These embeddings are processed by the BiLSTM to capture the sequential context across sentences. The CRF layer then models the dependencies between adjacent labels, enabling the output to follow the inherent structural patterns present in legal documents. By leveraging contextual cues from surrounding sentences, the model assigns a rhetorical role label to each sentence in a coherent and structured manner. The output of this stage is a corpus of legal documents where each sentence is associated with a predicted rhetorical role, forming the foundation for subsequent information retrieval experiments."
                    ]
                },
                {
                    "5.2 Vector Database Construction and Candidate Retrieval": [
                        "To enable efficient semantic retrieval of legal documents, we employed Milvus to store and query dense vector representations. Each entry in the collection comprised a unique id , a 768-dimensional embedding generated using the Snowflake Arctic Embed v2.0 model, and the original document text (limited to 60,000 characters). An IVF-FLAT index, configured with nlist = 2048 and using L2 distance, was built to facilitate rapid approximate nearest neighbor search. Query vectors, embedded using the same model, were matched against the collection, with the nprobe parameter controlling the search depth across partitions. The top- k k italic_k semantically similar documents were retrieved based on L2 distance, forming the candidate set for downstream re-ranking via cross-encoders. This stage ensures that initial retrieval captures documents with high semantic alignment to the input query.",
                        "Figure 2: TraceRetriever Pipeline"
                    ]
                },
                {
                    "5.3 BM25 Retrieval on Vector Database Candidates": [
                        "To complement semantic similarity with lexical matching, BM25 is applied but only to a reduced candidate set to avoid high computational costs. These candidates are pre-selected using vector-based retrieval, ensuring that BM25 is run only on semantically relevant documents, balancing efficiency and retrieval accuracy. The process begins by selecting the top- k k italic_k candidates from the vector search. The parameter k k italic_k controls the trade-off between recall and efficiency larger k k italic_k may improve recall but increases computational load. We selected k k italic_k as 1000 to maintain this balance. BM25 then scores each candidate based on term frequency (TF) and inverse document frequency (IDF), ranking documents where rare, frequent query terms appear. This yields a refined list of documents ranked by lexical relevance. By applying BM25 only to vector-selected candidates, the system enhances semantic matching with precise lexical signals."
                    ]
                },
                {
                    "5.4 Reciprocal Rank Fusion (RRF)": [
                        "To combine the ranked outputs from vector-based and BM25 retrieval, we employ Reciprocal Rank Fusion (RRF), a rank aggregation technique that leverages the complementary strengths of different retrieval methods for improved performance. Each document in the ranked lists receives a numerical rank (1 for top, 2 for second, etc.). Its reciprocal rank is computed as 1 rank + k \\frac{1}{\\text{rank}+k} divide start_ARG 1 end_ARG start_ARG rank + italic_k end_ARG , where k k italic_k is a constant used to reduce the influence of lower-ranked results. We selected an optimal k k italic_k to balance influence across both retrieval methods. For each document, its reciprocal ranks across all lists are summed to generate an aggregated RRF score. Documents are then sorted in descending order of this score, producing a fused ranking that integrates both semantic similarity (from the vector DB) and lexical relevance (from BM25). RRF enhances retrieval by combining diverse signals, resulting in a more robust and accurate final document ranking than either method alone."
                    ]
                },
                {
                    "5.5 Cross-Encoder Re-ranking": [
                        "To refine the ranking of candidate documents and prioritize the most relevant prior cases, we use a cross-encoder model. Unlike bi-encoders used in the initial retrieval, cross-encoders attend to both the query and document simultaneously. The process begins by forming (query, document) pairs from the top results obtained via Reciprocal Rank Fusion (RRF). This narrows the focus to promising candidates. Each pair is scored using the pre-trained bge-reranker-v2-m3 model, which excels at capturing fine-grained semantic interactions. For long documents exceeding the model’s input limits, a chunking strategy is applied. Each chunk is scored individually, and a final relevance score is computed using a weighted average of chunk scores. Other aggregation strategies like max or mean can also be used. Finally, documents are re-ranked based on these cross-encoder scores. This yields a final ranked list where the most semantically relevant cases are prioritized, enhancing retrieval quality by leveraging the model’s deep understanding of query-document relations."
                    ]
                },
                {
                    "5.6 TraceRetriever: A Hybrid Legal Case Retrieval Framework": [
                        "The TraceRetriever pipeline combines rhetorical role segmentation, vector-based retrieval, keyword-based retrieval (BM25), reciprocal rank fusion (RRF), and cross-encoders to perform effective and realistic legal case retrieval. It begins by segmenting the query legal document into sentences and classifying each into rhetorical roles (e.g., Facts , Issue , Argument , Reasoning , and Decision ) using a pre-trained Hierarchical BiLSTM. This segmentation supports role-specific querying, reflecting real-world scenarios where legal practitioners often search based on partial case descriptions. To retrieve initial candidates efficiently, a bi-encoder is used to encode both the rhetorically-filtered query and documents into dense embeddings. A vector database is then queried to retrieve the top- k k italic_k semantically relevant documents. Since applying BM25 across the entire corpus is computationally expensive, it is selectively applied only to this subset of vector-retrieved documents to capture lexical overlap. To unify the strengths of semantic and lexical signals, the results from the vector search and BM25 are merged using Reciprocal Rank Fusion (RRF), which produces a single ranked list. Finally, a cross-encoder re-ranks this list by jointly encoding each query-document pair to compute fine-grained relevance scores. Through this multi-stage approach, TraceRetriever effectively combines semantic understanding, lexical precision, and deep relevance modeling addressing the challenges of prior case retrieval under limited-information conditions."
                    ]
                }
            ],
            "6 Evaluation Metrics": [
                "To evaluate the effectiveness of our information retrieval models, we employ a standard set of metrics commonly used in retrieval tasks.",
                "Our primary evaluation relies on Precision@k, Recall@k, Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), and F1@k. Precision@k quantifies the fraction of relevant documents within the top-k retrieved results, whereas Recall@k assesses the system’s capability to identify all relevant documents within the top-k. MAP offers an overall performance measure by averaging the precision at each rank where a relevant document is found, across all queries. MRR focuses on the rank of the first relevant document in the result list. Finally, F1@k calculates the harmonic mean of Precision@k and Recall@k, providing a balanced evaluation of both aspects. Collectively, these metrics offer a thorough evaluation framework for assessing the ranking effectiveness and retrieval performance of the models. Here, we introduce the results of our experiments and discuss the performance of various models. Table 2 provides a summary of evaluation metrics for every model."
            ],
            "7 Results Analysis": [
                "Our experimental evaluation demonstrates significant variations in retrieval performance across different query formulations based on rhetorical roles and retrieval methodologies. Table 2 presents a comprehensive comparison of precision, recall, F1-score, Mean Average Precision (MAP), and Mean Reciprocal Rank (MRR) across all experimental configurations.",
                {
                    "7.1 Retrieval Method Performance": [
                        "The empirical results reveal distinct performance characteristics among the three retrieval methods. BM25, a traditional lexical matching approach, consistently underperforms compared to the semantic-based methods across all query configurations. This performance gap underscores the limitations of term-frequency based approaches in capturing the nuanced legal semantics present in case documents. Vector DB demonstrates superior performance in precision-oriented metrics, achieving the highest MAP (0.3783) and MRR (0.3924) scores with the Facts+Issue+Reasoning configuration. Notably, Vector DB consistently requires lower optimal k k italic_k values (typically 5–7), indicating its strong ability to position relevant documents at higher ranks. This characteristic makes Vector DB particularly suitable for applications where precision at lower ranks is prioritized. The Cross-encoder model exhibits different performance characteristics, consistently achieving higher recall values but requiring larger k k italic_k values (7–11) to reach optimal performance. For instance, with the Facts+Issue+Reasoning configuration, the Cross-encoder achieves the highest recall (0.2815) among all methods but at k = 11 k=11 italic_k = 11 . This suggests that Cross-encoder captures a broader range of relevant documents but with less precise ranking capability compared to Vector DB.",
                        "Dataset Model Precision@k Recall@k F1-score@k MAP MRR k k italic_k Full Query (IL-PCR) BM25 0.0819 0.1023 0.0740 0.2116 0.2182 6 Vector DB 0.1715 0.1754 0.1419 0.3484 0.3585 5 Cross-encoder 0.1459 0.1858 0.1301 0.3480 0.3339 6 Facts (IL-PCR) BM25 0.0797 0.0835 0.0694 0.1599 0.1684 5 Vector DB 0.1093 0.1574 0.1097 0.2566 0.2783 7 Cross-encoder 0.0916 0.2050 0.1082 0.2364 0.2725 11 Facts+ Issue (IL-PCR) BM25 0.0803 0.1152 0.0800 0.1907 0.2014 7 Vector DB 0.1281 0.1606 0.1200 0.2880 0.3055 6 Cross-encoder 0.1134 0.1723 0.1143 0.2554 0.2733 7 Facts+ Issue+ Arguments (IL-PCR) BM25 0.0900 0.1328 0.0908 0.2111 0.2259 7 Vector DB 0.1630 0.1775 0.1418 0.3291 0.3431 5 Cross-encoder 0.1121 0.2295 0.1277 0.2680 0.3045 10 Facts+ Issue+ Reasoning (IL-PCR) BM25 0.0947 0.1034 0.0824 0.2081 0.2144 5 Vector DB 0.1843 0.2088 0.1636 0.3783 0.3924 5 Cross-encoder 0.1223 0.2815 0.1436 0.2973 0.3316 11 Facts+ Issue+ Decision (IL-PCR) BM25 0.0884 0.1115 0.0833 0.1864 0.1926 6 Vector DB 0.121 0.1747 0.1212 0.2931 0.3157 7 Cross-encoder 0.1006 0.2235 0.1179 0.265 0.2991 11 Coliee Dataset BM25 0.0549 0.1139 0.0661 0.1410 0.1440 6 Vector DB 0.0515 0.1795 0.0720 0.1695 0.1786 11 Cross-encoder 0.0587 0.1545 0.0754 0.1574 0.1638 8 Table 2: Performance comparison across different query configurations and models on IL-PCR and COLIEE datasets"
                    ]
                },
                {
                    "7.2 Impact of Rhetorical Role Configurations": [
                        "The experimental results demonstrate that query formulation using specific rhetorical roles significantly impacts retrieval effectiveness. Several key observations emerge:",
                        "Using only factual components ( Facts ) yields the lowest performance across all retrieval methods, with Vector DB achieving MAP of 0.2566 and MRR of 0.2783. This finding suggests that factual information alone provides insufficient context for effective legal case retrieval. The addition of issue information ( Facts+Issue ) produces modest improvements across all models, with Vector DB showing MAP of 0.2880 and MRR of 0.3055. This improvement indicates that legal issues provide important discriminative information beyond mere facts. When argumentative elements are incorporated ( Facts+Issue+Arguments ), we observe substantial performance gains, particularly for Vector DB (MAP: 0.3291, MRR: 0.3431) and Cross-encoder (Recall@k: 0.2295). This suggests that arguments contain substantive information about legal reasoning that aids in identifying relevant precedents. The Facts+Issue+Reasoning configuration consistently yields the best performance across all retrieval methods, with Vector DB achieving the highest overall MAP (0.3783) and MRR (0.3924). This finding highlights the critical importance of legal reasoning components in determining case relevance. It suggests that the explicit reasoning articulated by judges forms the most discriminative aspect of legal documents for retrieval purposes. Interestingly, incorporating the decision component ( Facts+Issue+Decision ) results in performance degradation compared to the reasoning configuration. Vector DB’s MAP decreases to 0.2931 and MRR to 0.3157, while Cross-encoder shows similar declines. This degradation may be attributed to the fact that decisions often contain standardized language that is less discriminative than the specific reasoning that led to those decisions. The full query configuration performs relatively well (Vector DB: MAP 0.3484, MRR 0.3585), but still falls short of the Facts+Issue+Reasoning configuration. This indicates that using the entire document introduces noise that dilutes retrieval effectiveness."
                    ]
                },
                {
                    "7.3 Dataset Comparison": [
                        "A comparison between the IL-PCR and COLIEE datasets reveals substantial performance disparities. All retrieval methods perform markedly better on the IL-PCR dataset. On the COLIEE dataset, the best performance is achieved by Vector DB with MAP of 0.1695 and MRR of 0.1786, substantially lower than the corresponding metrics on IL-PCR. This disparity may be attributed to differences in document structure, domain-specific language, or the inherent complexity of the legal relationships represented in the COLIEE dataset. Additionally, our BiLSTM-based rhetorical role segmentation model was trained specifically on Indian legal documents."
                    ]
                },
                {
                    "7.4 Optimal k k italic_k Values": [
                        "In the context of information retrieval, k k italic_k represents the number of top-ranked documents retrieved by a system. An interesting observation from our experiments is the variation in optimal k k italic_k values across different configurations. Vector DB generally achieves optimal performance at lower k k italic_k values (5–7), while Cross-encoder typically requires higher k k italic_k values (7–11) to reach optimal performance. This pattern is consistent across query configurations and further emphasizes the distinct characteristics of these retrieval approaches: Vector DB excels at precise ranking of highly relevant documents within a smaller top- k k italic_k set, while Cross-encoder captures a broader range of potentially relevant documents, often requiring a larger top- k k italic_k to include the most pertinent results due to less precise initial ranking."
                    ]
                },
                {
                    "7.5 Error Analysis": [
                        "Retrieval errors were common when queries lacked argumentative depth or rhetorical coherence. Partial segments like Facts or Facts+Issue often led to vague queries, reducing the ability to retrieve precise legal precedents. Cross-encoders achieved high recall but lower MAP in such settings. For example, in the Facts-only configuration (Table 2 ), recall was 0.205, but MAP dropped to 0.2364, indicating difficulty in ranking the most legally relevant documents.",
                        "BM25 struggled with rhetorical overlap, particularly in IL-PCR, where Facts-only and Facts+Issue yielded low MAPs of 0.1599 and 0.1907. Its reliance on surface-level term frequency limited its ability to distinguish semantically similar yet legally distinct content. Interestingly, dense retrieval with Vector DB performed better in focused configurations. In IL-PCR, the MAP improved from 0.3484 ( Full ) to 0.3783 ( Facts+Issue+Reasoning ), likely due to reduced procedural noise and improved signal-to-noise ratio in embeddings. This suggests that full-document queries, though comprehensive, may dilute dense models with irrelevant content. In contrast, selected rhetorical segments enhance semantic richness and focus. Cross-encoders performed best when queries included Arguments or Reasoning , but struggled without structured argumentative flow. Overall, Vector DB benefited most from rhetorically rich inputs, with combinations like Facts+Issue+Reasoning offering the best trade-off between semantic depth and legal specificity.",
                        "In COLIEE, absence of rhetorical segmentation degraded performance across models. Vector DB’s MAP dropped to 0.1695, and BM25 to 0.141, as noisy, unsegmented queries confused both dense and sparse retrievers. The rhetorical classifier, trained on Indian cases, also failed to generalize to Canadian judgments in COLIEE, reducing the effectiveness of rhetorical-aware retrieval."
                    ]
                }
            ],
            "8 Conclusions and Future Work": [
                "This work introduced a novel approach to prior case retrieval that better reflects real-world legal research, where professionals often rely on partial case information like Facts and Issue . By using rhetorical role segmentation to extract these components as queries, our method simulates realistic legal workflows. Evaluations on ILTUR and COLIEE datasets showed that even under these constraints, our pipeline BM25, VectorDB, RRF, and cross-encoder reranking retrieves relevant cases, though with reduced precision and recall compared to full-document queries. Nonetheless, this role-based querying aligns closely with how legal professionals conduct research, offering a practical shift in retrieval methodology. Our main contribution is a conceptual framework for retrieval under partial information, encouraging a more practice-oriented direction in legal IR. Rather than chasing ideal scores, we aim to model realistic scenarios that support practical system design. This work has laid the groundwork for a more realistic paradigm in prior case retrieval by focusing on the information actually available at the initial stages of legal research. Our findings underscore the viability of a pipeline leveraging rhetorical role segmentation for query formulation, demonstrating effective, albeit reduced, retrieval performance compared to methods relying on complete case documents. Future work includes improving retrieval robustness under sparse queries, enhancing rhetorical segmentation, and testing advanced rerankers. We also aim to explore cross-lingual and multi-domain retrieval to further bridge academic research and real-world legal use cases."
            ],
            "Limitations": [
                "While this work presents a novel approach to prior case retrieval that mirrors real-world legal research, several limitations remain and highlight directions for improvement. A key challenge is the semantic sparsity of queries constructed from only rhetorical roles like Facts and Issue . This constrained input can omit important context, limiting the models’ ability to fully capture legal reasoning and reducing retrieval precision. Rhetorical overlap between roles such as Facts and Reasoning poses another issue. Their linguistic similarity makes it difficult especially for models like BM25 to differentiate cases based solely on rhetorical cues. While cross-encoders and vector models mitigate this to some extent, they still struggle with nuanced legal distinctions. Class imbalance in rhetorical roles also affects performance, particularly for underrepresented roles like Issue or Decision . Additionally, the computational complexity of advanced models like cross-encoders and dense retrievers can hinder scalability. Their high resource demands may limit deployment in real-world systems. Future work should explore optimization techniques such as pruning or quantization to maintain performance with lower resource requirements. While the system shows promise under real-world constraints, addressing these limitations will be crucial for building scalable and robust legal retrieval systems.",
                ""
            ]
        },
        "Tuples": [
            [
                "1 Introduction",
                "The common law system’s foundation rests upon the principle of stare decisis , mandating judicial adherence to precedents established in prior rulings when addressing analogous issues and facts within the same jurisdiction. As legal documentation grows in complexity and volume, sophisticated Natural Language Processing (NLP) techniques become indispensable for understanding, analyzing, and retrieving relevant precedents. TraceRetriever plays a crucial role in upholding stare decisis , facilitating the identification of past judgments with similar legal contexts to ensure consistent application of the law. The sheer volume of legal resources, including judgments, statutes, and regulations, poses a significant challenge for legal professionals seeking pertinent precedents, underscoring the urgent need for effective retrieval mechanisms.",
                "The common law system is based on stare decisis, where judges follow prior rulings. Natural Language Processing (NLP) techniques are used to understand and retrieve relevant precedents, including judgments, statutes, and regulations, which totals 3 types of legal resources.\n\nSummary includes:\n- Numbers: \n   - \"3\" \n\n- Names: None\n\n- Dates: None \n\n- Abbreviations: \n   - NLP\n   - stare decisis"
            ],
            [
                "1 Introduction",
                "Figure 1: Illustration of rhetorical role segmentation in a legal document. The left side shows the original excerpt, while the right side displays the labeled segments. In our approach, only relevant segments such as Facts and Issue are retained to emulate real-world legal case retrieval scenarios, where complete information like Reasoning or Decision may not be available at query time (Nigam et al., 2025 ) .",
                "Figure 1 illustrates rhetorical role segmentation in a legal document. Relevant segments such as Facts and Issue are retained, while complete information like Reasoning or Decision may not be available at query time (Nigam et al., 2025)."
            ],
            [
                "1 Introduction",
                "A notable limitation in much of the existing work on automated precedent retrieval is its reliance on using entire prior case documents as queries. This approach deviates significantly from real-world legal practice, where lawyers typically formulate search queries based on specific factual details and legal issues extracted from the case at hand, often with limited initial information. To address this gap, this paper tackles the challenge of mimicking real-world legal search scenarios in TraceRetriever by proposing a novel heuristic approach. Our methodology strategically integrates the complementary strengths of a keyword-based model (BM25), a semantic Vector Database, and a fine-grained Cross-Encoder for re-ranking. A key innovation of our work lies in utilizing a trained Hierarchical Bidirectional LSTM (HierBiLSTM) model by (Bhattacharya et al., 2019 ) to classify sentences within legal documents into distinct rhetorical roles. We then leverage the role segments, identified through this classification, as the query for our retrieval pipeline. This deliberate use of limited, rhetorically-informed query components directly mirrors the information scarcity often encountered in practical legal research. The core problem this paper addresses is therefore the development of a TraceRetriever system that effectively operates with limited, contextually relevant information, thereby more accurately reflecting real-world legal search processes.",
                "TraceRetriever proposes a novel heuristic approach to mimic real-world legal search scenarios. It uses BM25, a semantic Vector Database, and a Cross-Encoder for re-ranking. A HierBiLSTM model classifies sentences into rhetorical roles, which are used as query components in the retrieval pipeline. This system operates with limited, contextually relevant information."
            ],
            [
                "1 Introduction",
                "To evaluate the effectiveness of our proposed TraceRetriever pipeline, we conducted experiments on two established legal datasets: the Indian Legal Text Understanding and Reasoning (IL-PCR) dataset (Joshi et al., 2023 ) and the Competition on Legal Information Extraction and Entailment (COLIEE) 2025 dataset. Our pipeline employs a heuristic approach that strategically integrates the strengths of three distinct retrieval models: a semantic Vector Database, the BM25 algorithm, and a more nuanced Cross-Encoder. To further refine the initial retrieval results from the Vector Database and BM25, we implemented Reciprocal Rank Fusion (RRF), a robust re-ranking technique.",
                "Joshi et al. conducted experiments on IL-PCR dataset (2023) and COLIEE 2025 dataset, using a pipeline with Vector Database, BM25 algorithm, Cross-Encoder, and Reciprocal Rank Fusion (RRF)."
            ],
            [
                "1 Introduction",
                "In our TraceRetriever pipeline, we established BM25 as a robust baseline, representing a traditional keyword-based approach to information retrieval. To enhance the relevance and accuracy of our results, we implemented a sophisticated re-ranking strategy that leverages both semantic understanding and fine-grained interaction. Specifically, we employed Cross-Encoders to re-rank the top-k documents initially retrieved by two distinct methods: the lexical matching of BM25 and the semantic similarity captured by our Vector Database (a bi-encoder-based approach). This multi-faceted strategy effectively integrates the strengths of three complementary retrieval paradigms:",
                "BM25 was established as a baseline, then re-ranked with Cross-Encoders using two methods: BM25 lexical matching and Vector Database bi-encoder semantic similarity."
            ],
            [
                "1 Introduction",
                "Our key contributions are: 1. A realistic legal retrieval strategy using rhetorical role-based queries reflecting limited-information scenarios. 2. Development of TraceRetriever: A hybrid pipeline integrating BM25, vector search, and cross-encoder re-ranking.",
                "Our key contributions are: \n1. A legal retrieval strategy using rhetorical role-based queries\n2. Development of TraceRetriever: A hybrid pipeline integrating BM25, vector search, and cross-encoder re-ranking"
            ],
            [
                "1 Introduction",
                "For the sake of reproducibility, we have made our dataset, code, and RAG-based pipeline implementation via an github repository 1 1 1 https://github.com/ShubhamKumarNigam/Legal_IR .",
                "We have made our dataset, code, and RAG-based pipeline implementation available on GitHub at https://github.com/ShubhamKumarNigam/Legal_IR."
            ],
            [
                "2 Related Work",
                "Legal case retrieval has witnessed a rapid transformation with the advent of LLMs, RAG pipelines, and rhetorical role labeling. Traditionally, legal information retrieval relied heavily on lexical matching (e.g., BM25), which struggled to handle the semantic and structural nuances of legal texts. Recent innovations focus on improving retrieval accuracy by leveraging domain-specific embeddings, legal document structures, and rhetorical role understanding.",
                "Legal case retrieval has transformed with LLMs, RAG pipelines, and rhetorical role labeling. Innovations improve retrieval accuracy using domain-specific embeddings, legal document structures, and rhetorical role understanding. Traditionally, lexical matching (e.g., BM25) struggled to handle semantic and structural nuances of legal texts."
            ],
            [
                "2 Related Work",
                "Several systems have explored enhancing legal QA and retrieval using hybrid architectures. (Wiratunga et al., 2024 ) integrates Case-Based Reasoning with RAG to improve contextual relevance and factual correctness in legal question-answering. Similarly, (Panchal et al., 2025 ) utilizes FAISS and DeepSeek embeddings to make Indian legal knowledge accessible through a chatbot interface.",
                "Wiratunga et al. integrated Case-Based Reasoning with RAG in 2024, while Panchal et al. used FAISS and DeepSeek embeddings in 2025 for Indian legal knowledge accessibility via a chatbot."
            ],
            [
                "2 Related Work",
                "Another significant trend is the use of rhetorical roles in structuring legal texts. (Bhattacharya et al., 2019 ; Malik et al., 2022 ) pioneered rhetorical role classification in Indian legal judgments, showing that deep neural architectures such as BiLSTM-CRF and multi-task learning can outperform traditional methods. (Marino et al., 2023 ) further advanced this by stacking transformers over LEGAL-BERT to capture inter-sentence dependencies for rhetorical role classification across multilingual legal datasets. These works collectively demonstrate the feasibility and utility of segmenting legal documents into roles such as Facts , Issues , and Reasoning categories that are highly valuable for information extraction and retrieval. In recent studies, (Bhattacharya et al., 2019 ) proposed a CRF-BiLSTM model specifically for as signing rhetorical roles to sentences in Indian legal documents.",
                "Bhattacharya et al. (2019) and Malik et al. (2022) used deep neural architectures to classify rhetorical roles in Indian legal judgments, outperforming traditional methods. Marino et al. (2023) stacked transformers over LEGAL-BERT to capture inter-sentence dependencies for multilingual legal datasets. Bhattacharya et al. (2019) proposed a CRF-BiLSTM model for assigning rhetorical roles to sentences in Indian legal documents."
            ],
            [
                "2 Related Work",
                "In the context of document-to-document legal retrieval, methods like (Althammer et al., 2022 ) , (Ma et al., 2023 ) , and (Li et al., 2023 ) aim to overcome the challenges of long input lengths and weak semantic relevance by employing paragraph aggregation, structure-aware pretraining, and custom contrastive loss functions. Meanwhile, (Tang et al., 2023 ) and (Tang et al., 2024 ) take a graph based approach, modeling the connectivity between cases via attributed case graphs or global semantic networks to achieve state-of-the-art performance. (Nigam et al., 2022 ) presents a cascaded retrieval framework that integrates BM25 for lexical matching with Sentence BERT and Sent2Vec for semantic understanding. Interestingly, results show that BM25 alone often outperforms neural models, reaffirming the robustness and relevance of lexical approaches in legal case retrieval.",
                "Althammer et al. (2022), Ma et al. (2023) and Li et al. (2023) use methods to address long input lengths and weak semantic relevance. Tang et al. (2023, 2024) take a graph-based approach using attributed case graphs or global semantic networks. Nigam et al. (2022) uses BM25 for lexical matching with Sentence BERT and Sent2Vec."
            ],
            [
                "2 Related Work",
                "Beyond traditional lexical and semantic methods, several recent studies have explored innovative architectures to enhance legal case retrieval by addressing challenges such as long document length, complex legal semantics, and noisy or sparse queries. (Hu et al., 2022 ) proposes a retrieval method grounded in legal facts by combining topic modeling with BERT-based paragraph aggregation, offering more accurate semantic representations tailored to the legal domain. Similarly, (Shao et al., 2020 ) focuses on paragraph-level interactions, modeling fine-grained relationships between query and candidate cases to improve relevance estimation using a cascade framework and BERT finetuned on legal entailment tasks. Addressing structural and causal reasoning, (Zhang et al., 2023 ) introduces a counterfactual graph learning approach, which transforms legal cases into graphs of legal elements and enhances retrieval via counterfactual data augmentation and relational graph neural networks. Meanwhile, (Zhou et al., 2023 ) employ large language models (LLMs) to distill salient query content, showing that query reformulation using LLMs improves retrieval even in long, noisy legal queries. Structural reasoning is also emphasized in SLR (Zhou et al., 2023 ) , which incorporates both internal (document segmentation into roles like Facts, Holding, Decision) and external (charge relationship graphs) structures to enhance retrieval accuracy via a learning-to-rank approach, (Santosh et al., 2025 ) enhances prior case retrieval by generating legal concepts from the factual section of a query case to capture semantic intent. Collectively, these works highlight a growing trend toward structurally aware, semantically enriched, and role-sensitive retrieval models supporting the need for rhetorical role-driven query formulations in real-world legal search settings.",
                "Hu et al. (2022) proposes a retrieval method combining topic modeling with BERT-based aggregation. Shao et al. (2020) uses a cascade framework and BERT finetuned on legal entailment tasks for relevance estimation. Zhang et al. (2023) introduces counterfactual graph learning, while Zhou et al. (2023) employs large language models for query content distillation. SLR incorporates internal and external structures to enhance retrieval accuracy via a learning-to-rank approach. Santosh et al. (2025) generates legal concepts from the factual section of a query case to capture semantic intent."
            ],
            [
                "2 Related Work",
                "While these systems improve retrieval through structure, semantics, or scale, few explicitly address the limited-information retrieval scenario commonly encountered in real-world legal practice, where queries often arise from partial knowledge, such as only the Facts or Issues of a case. The (Deng et al., 2024 ) framework approaches this partially by reformulating legal documents into interpretable sub-facts using LLMs, but it does not explicitly tie these sub-facts to rhetorical roles.",
                "In real-world legal practice, systems typically address limited information retrieval scenarios with partial knowledge. The (Deng et al., 2024) framework uses LLMs to reformulate legal documents into sub-facts but doesn't tie them to rhetorical roles."
            ],
            [
                "2 Related Work",
                "In contrast to general-purpose document retrieval, (Joshi et al., 2023 ) propose U-CREAT, an unsupervised retrieval framework that extracts and matches event tuples consisting of predicates and their arguments from entire legal documents. However, U-CREAT still requires parsing the full document to extract events and does not leverage explicit legal segmentation such as rhetorical roles.",
                "Joshi et al. (2023) proposed an unsupervised retrieval framework called U-CREAT for extracting event tuples from entire legal documents, but it requires parsing full documents and lacks explicit legal segmentation."
            ],
            [
                "3 Task Description",
                "The goal of this task is to develop models capable of retrieving the most relevant prior legal cases for a given query case, with a novel emphasis on mimicking realistic legal reasoning workflows. Unlike previous work that provides entire case documents as input queries to retrieval models, we constrain the query representation by leveraging rhetorical role segmentation. This segmentation reflects how legal professionals typically reason over and search with focused portions of a case, such as facts, issues, or arguments, rather than the full text.",
                "The task aims to develop models for retrieving relevant prior legal cases from a query case, focusing on realistic legal reasoning workflows. The input queries are constrained by using rhetorical role segmentation, reflecting how professionals reason and search within focused portions of a case."
            ],
            [
                "3 Task Description",
                "Let Q = { q 1 , q 2 , … , q p } Q=\\{q_{1},q_{2},\\dots,q_{p}\\} italic_Q = { italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_q start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT } be a set of query legal cases, where each q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is a segmented case document composed of rhetorical roles:",
                "Q = {q1, q2, ..., qp}, italic_Q = {italic_q1, italic_q2, ..., italic_qp} are sets of query legal cases with p elements each, composed of rhetorical roles."
            ],
            [
                "3 Task Description",
                "q i = { Facts i , Issues i , Arguments i , … } q_{i}=\\{\\texttt{Facts}_{i},\\texttt{Issues}_{i},\\texttt{Arguments}_{i},\\dots\\} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { Facts start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , Issues start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , Arguments start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , … }",
                "{q_i} represents a set containing various elements such as Facts_i, Issues_i, Arguments_i, etc. where i is an index indicating the specific task."
            ],
            [
                "3 Task Description",
                "Rather than passing the full q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as a monolithic document, we present the segmented roles (individually or in combination) to retrieval models to enable fine-grained relevance modeling. This design encourages the system to focus on legally salient information while ignoring irrelevant or verbose content, thus improving efficiency and interpretability.",
                "Rather than passing full q_i as a monolithic document, we present segmented roles individually or in combination to retrieval models for fine-grained relevance modeling."
            ],
            [
                "3 Task Description",
                "Let D = { d 1 , d 2 , … , d n } D=\\{d_{1},d_{2},\\dots,d_{n}\\} italic_D = { italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } be a corpus of precedent legal documents. The objective is to retrieve a ranked list of k k italic_k relevant documents R i = { r i 1 , r i 2 , … , r i k } ⊆ D R_{i}=\\{r_{i1},r_{i2},\\dots,r_{ik}\\}\\subseteq D italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_r start_POSTSUBSCRIPT italic_i 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_i 2 end_POSTSUBSCRIPT , … , italic_r start_POSTSUBSCRIPT italic_i italic_k end_POSTSUBSCRIPT } ⊆ italic_D for each query q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , where documents are ranked by their relevance.",
                "D = { d 1 , d 2 , … , d n } is a corpus of precedent legal documents. The objective is to retrieve a ranked list of k relevant documents R i ⊆ D for each query q i ."
            ],
            [
                "3 Task Description",
                "We define a retrieval scoring function:",
                "A retrieval scoring function is defined."
            ],
            [
                "3 Task Description",
                "g : Q × D → ℝ g:Q\\times D\\rightarrow\\mathbb{R} italic_g : italic_Q × italic_D → blackboard_R",
                "g is a function from Q x D to R, and italic_g is a function from italic_Q x italic_D to blackboard_R."
            ],
            [
                "3 Task Description",
                "where g ( q i , d j ) g(q_{i},d_{j}) italic_g ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) outputs a relevance score indicating the degree to which the prior legal document d j d_{j} italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is relevant to the query q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . The retrieved list R i R_{i} italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT for a query q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is then constructed by selecting the top k k italic_k documents from D D italic_D based on their relevance scores: R i = top- k { d j ∈ D ∣ g ( q i , d j ) is high } R_{i}=\\text{top-}k\\{d_{j}\\in D\\mid g(q_{i},d_{j})\\text{ is high}\\} italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = top- italic_k { italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∈ italic_D ∣ italic_g ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) is high } The input to the system is a legal query q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , and the output is a ranked list of k k italic_k prior legal documents R i R_{i} italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ordered by their relevance to the query.",
                "The system outputs a relevance score indicating the degree to which a prior legal document is relevant to a given query. The top k documents from a database D are selected based on these scores, and output as a ranked list R_i ordered by their relevance to the query q_i. The input is q_i and the output is R_i."
            ],
            [
                "4 Dataset",
                "To support research in the domain of Prior Case Retrieval (PCR), we utilize the IL-PCR (Indian Legal Prior Case Retrieval) corpus, a large-scale collection of Indian legal documents comprising 7,070 English-language case texts by (Joshi et al., 2023 ) . This corpus enables the development and benchmarking of retrieval systems specifically tailored to the Indian legal system.",
                "The IL-PCR corpus contains 7070 case texts in English, developed for research on Prior Case Retrieval (PCR) in the Indian legal system."
            ],
            [
                "4 Dataset",
                "Dataset COLIEE’25 IL-PCR # Documents 9498 7070 Avg. Document Size 4759.79 8093.19 # Query Documents 2077 1182 Vocabulary Size 426,118 113,340 Total Citation Links 8640 8008 Avg. Citations per Query 4.16 6.775 Language English English Legal System Canadian Indian Table 1: Comparison of the IL-PCR corpus (Joshi et al., 2023 ) with the COLIEE’25 dataset.",
                "Dataset COLIEE'25 IL-PCR has # Documents 9498 and 7070, Avg. Document Size 4759.79 and 8093.19. It also has # Query Documents 2077 and 1182, Vocabulary Size 426,118 and 113,340, Total Citation Links 8640 and 8008, with Avg. Citations per Query 4.16 and 6.775, in the Language English and Legal System Canadian Indian, compared to IL-PCR corpus (Joshi et al., 2023) in Table 1."
            ],
            [
                "4 Dataset. 4.1 Overview of Dataset",
                "The IL-PCR corpus was created by collecting case documents from the public domain through the IndianKanoon website 2 2 2 https://indiankanoon.org/ . The initial set comprises the 100 most-cited Supreme Court of India (SCI) judgments, referred to as the zero-hop set . To increase citation density, cases cited within these judgments (the one-hop set ) were also collected. This hierarchical collection approach ensures that each document has multiple cited cases, allowing for robust retrieval evaluation (Joshi et al., 2023 ) . Following standard preprocessing, empty or invalid cases were discarded. The resulting corpus was partitioned into training (70%), validation (10%), and test (20%) splits.",
                "The IL-PCR corpus consists of 100 most-cited Supreme Court of India judgments (zero-hop set) and their cited cases (one-hop set), with a total split of 70% for training, 10% for validation, and 20% for testing."
            ],
            [
                "4 Dataset. 4.2 Preprocessing",
                "The preprocessing pipeline includes named entity normalization using spaCy’s NER model, alongside a manually curated gazetteer. This standardization improves the generalizability of learned representations. Hyperlinked citations in the documents were replaced with a standardized token <CITATION> , while references to statutes and laws were retained, aligning with the task focus on case retrieval rather than statute retrieval. Additionally, an alternate version of the dataset removes entire sentences containing citations, as discussed in (Joshi et al., 2023 ) .",
                "Named entity normalization is performed using spaCy's NER model and a manually curated gazetteer to improve generalizability. Hyperlinked citations are replaced with '<CITATION>'. An alternate dataset removes sentences containing citations, as per (Joshi et al., 2023)."
            ],
            [
                "5 Methodology",
                "This section elucidates the TraceRetriever methodology, a multi-stage framework designed for effective prior case retrieval, particularly when initiated with partial case details. Our approach integrates advanced NLP techniques, starting with rhetorical role annotation to enable targeted querying of key document sections. We then employ a hybrid retrieval strategy, combining semantic vector search with lexical BM25 matching on a focused candidate set. The resulting ranked lists are fused using RRF, followed by a deep semantic re-ranking via a cross-encoder.",
                "The TraceRetriever methodology is a multi-stage framework that integrates NLP techniques for effective prior case retrieval. It employs rhetorical role annotation, hybrid retrieval, semantic vector search, and lexical BM25 matching to generate ranked lists, which are then fused and re-ranked using RRF and a cross-encoder."
            ],
            [
                "5 Methodology. 5.1 Rhetorical Role Annotation of Legal Documents",
                "The initial stage of our methodology involves enriching legal documents with rhetorical role annotations at the sentence level. To achieve this, we first perform sentence segmentation using the spaCy library. We implement the BiLSTM-CRF architecture introduced by (Bhattacharya et al., 2019 ) , which integrates a BiLSTM network with a Conditional Random Field (CRF) layer. The model takes as input sentence embeddings generated using a sent2vec model trained specifically on Indian Supreme Court judgments. These embeddings are processed by the BiLSTM to capture the sequential context across sentences. The CRF layer then models the dependencies between adjacent labels, enabling the output to follow the inherent structural patterns present in legal documents. By leveraging contextual cues from surrounding sentences, the model assigns a rhetorical role label to each sentence in a coherent and structured manner. The output of this stage is a corpus of legal documents where each sentence is associated with a predicted rhetorical role, forming the foundation for subsequent information retrieval experiments.",
                "The initial stage involves annotating legal documents with rhetorical roles at the sentence level using spaCy and BiLSTM-CRF architecture. The model takes sent2vec embeddings trained on Indian Supreme Court judgments, processing contextual cues to assign rhetorical role labels to each sentence in a structured manner, resulting in a corpus of annotated documents for subsequent experiments."
            ],
            [
                "5 Methodology. 5.2 Vector Database Construction and Candidate Retrieval",
                "To enable efficient semantic retrieval of legal documents, we employed Milvus to store and query dense vector representations. Each entry in the collection comprised a unique id , a 768-dimensional embedding generated using the Snowflake Arctic Embed v2.0 model, and the original document text (limited to 60,000 characters). An IVF-FLAT index, configured with nlist = 2048 and using L2 distance, was built to facilitate rapid approximate nearest neighbor search. Query vectors, embedded using the same model, were matched against the collection, with the nprobe parameter controlling the search depth across partitions. The top- k k italic_k semantically similar documents were retrieved based on L2 distance, forming the candidate set for downstream re-ranking via cross-encoders. This stage ensures that initial retrieval captures documents with high semantic alignment to the input query.",
                "A unique id and a 768-dimensional embedding generated by Snowflake Arctic Embed v2.0 are stored along with original document text (limited to 60,000 characters) in Milvus. An IVF-FLAT index is built with nlist = 2048 for rapid search. Query vectors embedded using the same model as collection vectors are matched against the collection. The top-k semantically similar documents are retrieved based on L2 distance."
            ],
            [
                "5 Methodology. 5.2 Vector Database Construction and Candidate Retrieval",
                "Figure 2: TraceRetriever Pipeline",
                "\"Figure 2 describes a pipeline called TraceRetriever.\""
            ],
            [
                "5 Methodology. 5.3 BM25 Retrieval on Vector Database Candidates",
                "To complement semantic similarity with lexical matching, BM25 is applied but only to a reduced candidate set to avoid high computational costs. These candidates are pre-selected using vector-based retrieval, ensuring that BM25 is run only on semantically relevant documents, balancing efficiency and retrieval accuracy. The process begins by selecting the top- k k italic_k candidates from the vector search. The parameter k k italic_k controls the trade-off between recall and efficiency larger k k italic_k may improve recall but increases computational load. We selected k k italic_k as 1000 to maintain this balance. BM25 then scores each candidate based on term frequency (TF) and inverse document frequency (IDF), ranking documents where rare, frequent query terms appear. This yields a refined list of documents ranked by lexical relevance. By applying BM25 only to vector-selected candidates, the system enhances semantic matching with precise lexical signals.",
                "BM25 retrieval is applied to a reduced candidate set (k=1000) pre-selected using vector-based retrieval, balancing efficiency and accuracy."
            ],
            [
                "5 Methodology. 5.4 Reciprocal Rank Fusion (RRF)",
                "To combine the ranked outputs from vector-based and BM25 retrieval, we employ Reciprocal Rank Fusion (RRF), a rank aggregation technique that leverages the complementary strengths of different retrieval methods for improved performance. Each document in the ranked lists receives a numerical rank (1 for top, 2 for second, etc.). Its reciprocal rank is computed as 1 rank + k \\frac{1}{\\text{rank}+k} divide start_ARG 1 end_ARG start_ARG rank + italic_k end_ARG , where k k italic_k is a constant used to reduce the influence of lower-ranked results. We selected an optimal k k italic_k to balance influence across both retrieval methods. For each document, its reciprocal ranks across all lists are summed to generate an aggregated RRF score. Documents are then sorted in descending order of this score, producing a fused ranking that integrates both semantic similarity (from the vector DB) and lexical relevance (from BM25). RRF enhances retrieval by combining diverse signals, resulting in a more robust and accurate final document ranking than either method alone.",
                "Reciprocal Rank Fusion (RRF) combines ranked outputs from vector-based and BM25 retrieval methods using rank aggregation. The reciprocal rank of each document is computed as 1/(rank+k), where k=1, to balance influence across both methods. RRF scores are then summed for each document, sorted in descending order, producing a fused ranking that integrates semantic similarity and lexical relevance."
            ],
            [
                "5 Methodology. 5.5 Cross-Encoder Re-ranking",
                "To refine the ranking of candidate documents and prioritize the most relevant prior cases, we use a cross-encoder model. Unlike bi-encoders used in the initial retrieval, cross-encoders attend to both the query and document simultaneously. The process begins by forming (query, document) pairs from the top results obtained via Reciprocal Rank Fusion (RRF). This narrows the focus to promising candidates. Each pair is scored using the pre-trained bge-reranker-v2-m3 model, which excels at capturing fine-grained semantic interactions. For long documents exceeding the model’s input limits, a chunking strategy is applied. Each chunk is scored individually, and a final relevance score is computed using a weighted average of chunk scores. Other aggregation strategies like max or mean can also be used. Finally, documents are re-ranked based on these cross-encoder scores. This yields a final ranked list where the most semantically relevant cases are prioritized, enhancing retrieval quality by leveraging the model’s deep understanding of query-document relations.",
                "Here is a concise summary of the paragraph:\n\nThe cross-encoder re-ranking method uses a pre-trained bge-reranker-v2-m3 model to score (query, document) pairs from RRF top results. It chunks long documents if necessary and computes a weighted average of chunk scores for each pair. The final ranked list prioritizes semantically relevant cases."
            ],
            [
                "5 Methodology. 5.6 TraceRetriever: A Hybrid Legal Case Retrieval Framework",
                "The TraceRetriever pipeline combines rhetorical role segmentation, vector-based retrieval, keyword-based retrieval (BM25), reciprocal rank fusion (RRF), and cross-encoders to perform effective and realistic legal case retrieval. It begins by segmenting the query legal document into sentences and classifying each into rhetorical roles (e.g., Facts , Issue , Argument , Reasoning , and Decision ) using a pre-trained Hierarchical BiLSTM. This segmentation supports role-specific querying, reflecting real-world scenarios where legal practitioners often search based on partial case descriptions. To retrieve initial candidates efficiently, a bi-encoder is used to encode both the rhetorically-filtered query and documents into dense embeddings. A vector database is then queried to retrieve the top- k k italic_k semantically relevant documents. Since applying BM25 across the entire corpus is computationally expensive, it is selectively applied only to this subset of vector-retrieved documents to capture lexical overlap. To unify the strengths of semantic and lexical signals, the results from the vector search and BM25 are merged using Reciprocal Rank Fusion (RRF), which produces a single ranked list. Finally, a cross-encoder re-ranks this list by jointly encoding each query-document pair to compute fine-grained relevance scores. Through this multi-stage approach, TraceRetriever effectively combines semantic understanding, lexical precision, and deep relevance modeling addressing the challenges of prior case retrieval under limited-information conditions.",
                "The TraceRetriever pipeline combines 5 components: rhetorical role segmentation, vector-based retrieval, BM25, Reciprocal Rank Fusion (RRF), and cross-encoders to retrieve legal cases. It uses Hierarchical BiLSTM for segmentation and bi-encoder for retrieving top-k documents with semantic relevance, selectively applying BM25 to capture lexical overlap. The results are merged using RRF and then re-ranked by a cross-encoder to compute fine-grained relevance scores."
            ],
            [
                "6 Evaluation Metrics",
                "To evaluate the effectiveness of our information retrieval models, we employ a standard set of metrics commonly used in retrieval tasks.",
                "Our information retrieval models are evaluated using six standard metrics."
            ],
            [
                "6 Evaluation Metrics",
                "Our primary evaluation relies on Precision@k, Recall@k, Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), and F1@k. Precision@k quantifies the fraction of relevant documents within the top-k retrieved results, whereas Recall@k assesses the system’s capability to identify all relevant documents within the top-k. MAP offers an overall performance measure by averaging the precision at each rank where a relevant document is found, across all queries. MRR focuses on the rank of the first relevant document in the result list. Finally, F1@k calculates the harmonic mean of Precision@k and Recall@k, providing a balanced evaluation of both aspects. Collectively, these metrics offer a thorough evaluation framework for assessing the ranking effectiveness and retrieval performance of the models. Here, we introduce the results of our experiments and discuss the performance of various models. Table 2 provides a summary of evaluation metrics for every model.",
                "Our primary evaluation relies on Precision@k (quantifies relevant documents), Recall@k (assesses system's capability to identify relevant documents), Mean Average Precision (MAP, overall performance measure), Mean Reciprocal Rank (MRR, first relevant document rank), and F1@k (balanced evaluation of both aspects)."
            ],
            [
                "7 Results Analysis",
                "Our experimental evaluation demonstrates significant variations in retrieval performance across different query formulations based on rhetorical roles and retrieval methodologies. Table 2 presents a comprehensive comparison of precision, recall, F1-score, Mean Average Precision (MAP), and Mean Reciprocal Rank (MRR) across all experimental configurations.",
                "Significant variations in retrieval performance were observed across different query formulations based on rhetorical roles and retrieval methodologies. Key metrics included precision, recall, F1-score, MAP, and MRR across 2 experimental configurations (Table 2)."
            ],
            [
                "7 Results Analysis. 7.1 Retrieval Method Performance",
                "The empirical results reveal distinct performance characteristics among the three retrieval methods. BM25, a traditional lexical matching approach, consistently underperforms compared to the semantic-based methods across all query configurations. This performance gap underscores the limitations of term-frequency based approaches in capturing the nuanced legal semantics present in case documents. Vector DB demonstrates superior performance in precision-oriented metrics, achieving the highest MAP (0.3783) and MRR (0.3924) scores with the Facts+Issue+Reasoning configuration. Notably, Vector DB consistently requires lower optimal k k italic_k values (typically 5–7), indicating its strong ability to position relevant documents at higher ranks. This characteristic makes Vector DB particularly suitable for applications where precision at lower ranks is prioritized. The Cross-encoder model exhibits different performance characteristics, consistently achieving higher recall values but requiring larger k k italic_k values (7–11) to reach optimal performance. For instance, with the Facts+Issue+Reasoning configuration, the Cross-encoder achieves the highest recall (0.2815) among all methods but at k = 11 k=11 italic_k = 11 . This suggests that Cross-encoder captures a broader range of relevant documents but with less precise ranking capability compared to Vector DB.",
                "BM25 underperforms against semantic-based methods, particularly in capturing legal nuances. \nVector DB achieves best precision metrics (MAP: 0.3783, MRR: 0.3924) with optimal k values (5-7). \nCross-encoder achieves highest recall (0.2815) at larger k values (7-11), capturing broader relevant documents but less precise rankings."
            ],
            [
                "7 Results Analysis. 7.1 Retrieval Method Performance",
                "Dataset Model Precision@k Recall@k F1-score@k MAP MRR k k italic_k Full Query (IL-PCR) BM25 0.0819 0.1023 0.0740 0.2116 0.2182 6 Vector DB 0.1715 0.1754 0.1419 0.3484 0.3585 5 Cross-encoder 0.1459 0.1858 0.1301 0.3480 0.3339 6 Facts (IL-PCR) BM25 0.0797 0.0835 0.0694 0.1599 0.1684 5 Vector DB 0.1093 0.1574 0.1097 0.2566 0.2783 7 Cross-encoder 0.0916 0.2050 0.1082 0.2364 0.2725 11 Facts+ Issue (IL-PCR) BM25 0.0803 0.1152 0.0800 0.1907 0.2014 7 Vector DB 0.1281 0.1606 0.1200 0.2880 0.3055 6 Cross-encoder 0.1134 0.1723 0.1143 0.2554 0.2733 7 Facts+ Issue+ Arguments (IL-PCR) BM25 0.0900 0.1328 0.0908 0.2111 0.2259 7 Vector DB 0.1630 0.1775 0.1418 0.3291 0.3431 5 Cross-encoder 0.1121 0.2295 0.1277 0.2680 0.3045 10 Facts+ Issue+ Reasoning (IL-PCR) BM25 0.0947 0.1034 0.0824 0.2081 0.2144 5 Vector DB 0.1843 0.2088 0.1636 0.3783 0.3924 5 Cross-encoder 0.1223 0.2815 0.1436 0.2973 0.3316 11 Facts+ Issue+ Decision (IL-PCR) BM25 0.0884 0.1115 0.0833 0.1864 0.1926 6 Vector DB 0.121 0.1747 0.1212 0.2931 0.3157 7 Cross-encoder 0.1006 0.2235 0.1179 0.265 0.2991 11 Coliee Dataset BM25 0.0549 0.1139 0.0661 0.1410 0.1440 6 Vector DB 0.0515 0.1795 0.0720 0.1695 0.1786 11 Cross-encoder 0.0587 0.1545 0.0754 0.1574 0.1638 8 Table 2: Performance comparison across different query configurations and models on IL-PCR and COLIEE datasets",
                "Dataset Model Precision@k Recall@k F1-score@k MAP MRR k \nIL-PCR BM25 0.0819 0.1023 0.0740 0.2116 0.2182 6\nVector DB 0.1715 0.1754 0.1419 0.3484 0.3585 5\nCross-encoder 0.1459 0.1858 0.1301 0.3480 0.3339 6\n\nIL-PCR BM25 0.0797 0.0835 0.0694 0.1599 0.1684 5\nVector DB 0.1093 0.1574 0.1097 0.2566 0.2783 7\nCross-encoder 0.0916 0.2050 0.1082 0.2364 0.2725 11\n\nIL-PCR BM25 0.0803 0.1152 0.0800 0.1907 0.2014 7\nVector DB 0.1281 0.1606 0.1200 0.2880 0.3055 6\nCross-encoder 0.1134 0.1723 0.1143 0.2554 0.2733 7\n\nIL-PCR BM25 0.0900 0.1328 0.0908 0.2111 0.2259 7\nVector DB 0.1630 0.1775 0.1418 0.3291 0.3431 5\nCross-encoder 0.1121 0.2295 0.1277 0.2680 0.3045 10\n\nIL-PCR BM25 0.0947 0.1034 0.0824 0.2081 0.2144 5\nVector DB 0.1843 0.2088 0.1636 0.3783 0.3924 5\nCross-encoder 0.1223 0.2815 0.1436 0.2973 0.3316 11\n\nIL-PCR BM25 0.0884 0.1115 0.0833 0.1864 0.1926 6\nVector DB 0.121 0.1747 0.1212 0.2931 0.3157 7\nCross-encoder 0.1006 0.2235 0.1179 0.265 0.2991 11\n\nCOLIEE Dataset BM25 0.0549 0.1139 0.0661 0.1410 0.1440 6\nVector DB 0.0515 0.1795 0.0720 0.1695 0.1786 11\nCross-encoder 0.0587 0.1545 0.0754 0.1574 0.1638 8"
            ],
            [
                "7 Results Analysis. 7.2 Impact of Rhetorical Role Configurations",
                "The experimental results demonstrate that query formulation using specific rhetorical roles significantly impacts retrieval effectiveness. Several key observations emerge:",
                "Query formulation with specific rhetorical roles improves retrieval effectiveness."
            ],
            [
                "7 Results Analysis. 7.2 Impact of Rhetorical Role Configurations",
                "Using only factual components ( Facts ) yields the lowest performance across all retrieval methods, with Vector DB achieving MAP of 0.2566 and MRR of 0.2783. This finding suggests that factual information alone provides insufficient context for effective legal case retrieval. The addition of issue information ( Facts+Issue ) produces modest improvements across all models, with Vector DB showing MAP of 0.2880 and MRR of 0.3055. This improvement indicates that legal issues provide important discriminative information beyond mere facts. When argumentative elements are incorporated ( Facts+Issue+Arguments ), we observe substantial performance gains, particularly for Vector DB (MAP: 0.3291, MRR: 0.3431) and Cross-encoder (Recall@k: 0.2295). This suggests that arguments contain substantive information about legal reasoning that aids in identifying relevant precedents. The Facts+Issue+Reasoning configuration consistently yields the best performance across all retrieval methods, with Vector DB achieving the highest overall MAP (0.3783) and MRR (0.3924). This finding highlights the critical importance of legal reasoning components in determining case relevance. It suggests that the explicit reasoning articulated by judges forms the most discriminative aspect of legal documents for retrieval purposes. Interestingly, incorporating the decision component ( Facts+Issue+Decision ) results in performance degradation compared to the reasoning configuration. Vector DB’s MAP decreases to 0.2931 and MRR to 0.3157, while Cross-encoder shows similar declines. This degradation may be attributed to the fact that decisions often contain standardized language that is less discriminative than the specific reasoning that led to those decisions. The full query configuration performs relatively well (Vector DB: MAP 0.3484, MRR 0.3585), but still falls short of the Facts+Issue+Reasoning configuration. This indicates that using the entire document introduces noise that dilutes retrieval effectiveness.",
                "Using factual components (Facts) yields lowest performance with Vector DB achieving MAP 0.2566 and MRR 0.2783, but adding issue information improves results to MAP 0.2880 and MRR 0.3055. Incorporating arguments boosts performance significantly for Vector DB (MAP: 0.3291, MRR: 0.3431) and Cross-encoder (Recall@k: 0.2295). The Facts+Issue+Reasoning configuration yields the best results across all methods with Vector DB achieving MAP 0.3783 and MRR 0.3924. Incorporating decisions degrades performance for both Vector DB (MAP: 0.2931, MRR: 0.3157) and Cross-encoder."
            ],
            [
                "7 Results Analysis. 7.3 Dataset Comparison",
                "A comparison between the IL-PCR and COLIEE datasets reveals substantial performance disparities. All retrieval methods perform markedly better on the IL-PCR dataset. On the COLIEE dataset, the best performance is achieved by Vector DB with MAP of 0.1695 and MRR of 0.1786, substantially lower than the corresponding metrics on IL-PCR. This disparity may be attributed to differences in document structure, domain-specific language, or the inherent complexity of the legal relationships represented in the COLIEE dataset. Additionally, our BiLSTM-based rhetorical role segmentation model was trained specifically on Indian legal documents.",
                "A comparison between IL-PCR and COLIEE datasets shows performance disparities. Vector DB achieved MAP 0.1695 and MRR 0.1786 on COLIEE, but these were lower than on IL-PCR. This difference may be due to document structure, language or complexity of legal relationships in COLIEE dataset."
            ],
            [
                "7 Results Analysis. 7.4 Optimal k k italic_k Values",
                "In the context of information retrieval, k k italic_k represents the number of top-ranked documents retrieved by a system. An interesting observation from our experiments is the variation in optimal k k italic_k values across different configurations. Vector DB generally achieves optimal performance at lower k k italic_k values (5–7), while Cross-encoder typically requires higher k k italic_k values (7–11) to reach optimal performance. This pattern is consistent across query configurations and further emphasizes the distinct characteristics of these retrieval approaches: Vector DB excels at precise ranking of highly relevant documents within a smaller top- k k italic_k set, while Cross-encoder captures a broader range of potentially relevant documents, often requiring a larger top- k k italic_k to include the most pertinent results due to less precise initial ranking.",
                "Vector DB achieves optimal performance at lower k (5–7) values, while Cross-encoder requires higher k (7–11) values. Vector DB excels in precise ranking within a smaller top-k set and Cross-encoder captures broader relevant documents requiring larger top-k."
            ],
            [
                "7 Results Analysis. 7.5 Error Analysis",
                "Retrieval errors were common when queries lacked argumentative depth or rhetorical coherence. Partial segments like Facts or Facts+Issue often led to vague queries, reducing the ability to retrieve precise legal precedents. Cross-encoders achieved high recall but lower MAP in such settings. For example, in the Facts-only configuration (Table 2 ), recall was 0.205, but MAP dropped to 0.2364, indicating difficulty in ranking the most legally relevant documents.",
                "Retrieval errors occurred for queries lacking argumentative depth or rhetorical coherence, particularly with Facts-only configuration where cross-encoders had high recall (0.205) but lower MAP (0.2364)."
            ],
            [
                "7 Results Analysis. 7.5 Error Analysis",
                "BM25 struggled with rhetorical overlap, particularly in IL-PCR, where Facts-only and Facts+Issue yielded low MAPs of 0.1599 and 0.1907. Its reliance on surface-level term frequency limited its ability to distinguish semantically similar yet legally distinct content. Interestingly, dense retrieval with Vector DB performed better in focused configurations. In IL-PCR, the MAP improved from 0.3484 ( Full ) to 0.3783 ( Facts+Issue+Reasoning ), likely due to reduced procedural noise and improved signal-to-noise ratio in embeddings. This suggests that full-document queries, though comprehensive, may dilute dense models with irrelevant content. In contrast, selected rhetorical segments enhance semantic richness and focus. Cross-encoders performed best when queries included Arguments or Reasoning , but struggled without structured argumentative flow. Overall, Vector DB benefited most from rhetorically rich inputs, with combinations like Facts+Issue+Reasoning offering the best trade-off between semantic depth and legal specificity.",
                "BM25 had low MAPs (0.1599, 0.1907) in IL-PCR due to surface-level term frequency limitations. Vector DB performed better with dense retrieval (MAP improved from 0.3484 to 0.3783), particularly with selected rhetorical segments and structured argumentative flow. Cross-encoders excelled with \"Arguments\" or \"Reasoning\" queries, but struggled without them."
            ],
            [
                "7 Results Analysis. 7.5 Error Analysis",
                "In COLIEE, absence of rhetorical segmentation degraded performance across models. Vector DB’s MAP dropped to 0.1695, and BM25 to 0.141, as noisy, unsegmented queries confused both dense and sparse retrievers. The rhetorical classifier, trained on Indian cases, also failed to generalize to Canadian judgments in COLIEE, reducing the effectiveness of rhetorical-aware retrieval.",
                "COLIEE's performance degraded due to unsegmented queries (MAP: 0.1695, BM25: 0.141). A rhetorical classifier trained on Indian cases failed to generalize to Canadian judgments."
            ],
            [
                "8 Conclusions and Future Work",
                "This work introduced a novel approach to prior case retrieval that better reflects real-world legal research, where professionals often rely on partial case information like Facts and Issue . By using rhetorical role segmentation to extract these components as queries, our method simulates realistic legal workflows. Evaluations on ILTUR and COLIEE datasets showed that even under these constraints, our pipeline BM25, VectorDB, RRF, and cross-encoder reranking retrieves relevant cases, though with reduced precision and recall compared to full-document queries. Nonetheless, this role-based querying aligns closely with how legal professionals conduct research, offering a practical shift in retrieval methodology. Our main contribution is a conceptual framework for retrieval under partial information, encouraging a more practice-oriented direction in legal IR. Rather than chasing ideal scores, we aim to model realistic scenarios that support practical system design. This work has laid the groundwork for a more realistic paradigm in prior case retrieval by focusing on the information actually available at the initial stages of legal research. Our findings underscore the viability of a pipeline leveraging rhetorical role segmentation for query formulation, demonstrating effective, albeit reduced, retrieval performance compared to methods relying on complete case documents. Future work includes improving retrieval robustness under sparse queries, enhancing rhetorical segmentation, and testing advanced rerankers. We also aim to explore cross-lingual and multi-domain retrieval to further bridge academic research and real-world legal use cases.",
                "ILTUR and COLIEE datasets were used for evaluation; BM25, VectorDB, RRF, and cross-encoder reranking methods showed reduced precision and recall compared to full-document queries. A pipeline leveraging rhetorical role segmentation was demonstrated to be viable for query formulation in legal research, with room for improvement in retrieval robustness and advanced rerankers. Future work includes exploring cross-lingual and multi-domain retrieval."
            ],
            [
                "Limitations",
                "While this work presents a novel approach to prior case retrieval that mirrors real-world legal research, several limitations remain and highlight directions for improvement. A key challenge is the semantic sparsity of queries constructed from only rhetorical roles like Facts and Issue . This constrained input can omit important context, limiting the models’ ability to fully capture legal reasoning and reducing retrieval precision. Rhetorical overlap between roles such as Facts and Reasoning poses another issue. Their linguistic similarity makes it difficult especially for models like BM25 to differentiate cases based solely on rhetorical cues. While cross-encoders and vector models mitigate this to some extent, they still struggle with nuanced legal distinctions. Class imbalance in rhetorical roles also affects performance, particularly for underrepresented roles like Issue or Decision . Additionally, the computational complexity of advanced models like cross-encoders and dense retrievers can hinder scalability. Their high resource demands may limit deployment in real-world systems. Future work should explore optimization techniques such as pruning or quantization to maintain performance with lower resource requirements. While the system shows promise under real-world constraints, addressing these limitations will be crucial for building scalable and robust legal retrieval systems.",
                "This work has several limitations including semantic sparsity from using rhetorical roles like Facts and Issue , overlap between similar roles, class imbalance, and high resource demands from advanced models."
            ],
            [
                "Limitations",
                "",
                "There is no paragraph text provided. Please provide the paragraph text so I can create a concise summary."
            ]
        ],
        "general_summary": "**Introduction**\n\nThe paper discusses an information retrieval framework for legal case research, specifically focusing on a hybrid approach that combines vector-based methods with lexical similarity measures. The authors investigate the effectiveness of various configurations and metrics to identify relevant documents.\n\n**Methodology**\n\n1. **Rhetorical Role Segmentation**: The system uses Hierarchical BiLSTM (Bi-Directional Long Short-Term Memory) for segmenting long documents into rhetorical roles such as Facts, Issues, Reasonings, Arguments, Decisions.\n2. **Vector-Based Retrieval**: The authors employ vector-based retrieval methods using a bi-encoder architecture to score document-query pairs based on semantic similarity.\n3. **BM25 (Best Match 25)**: This method uses traditional lexical similarity measures (term frequency-inverse document frequency) for ranking documents.\n4. **Reciprocal Rank Fusion (RRF)**: RRF combines the rankings from vector-based and BM25 methods using rank aggregation to produce a fused ranking.\n5. **Cross-Encoder Re-ranking**: The authors use pre-trained models to re-rank top-ranked documents based on fine-grained relevance scores.\n\n**Evaluation Metrics**\n\n1. **Precision@k (P@k)**: Measures the proportion of relevant documents among the top-k ranked documents.\n2. **Recall@k (R@k)**: Evaluates the system's ability to identify relevant documents among all available documents.\n3. **Mean Average Precision (MAP)**: Provides an overall performance measure for the ranking task.\n4. **Mean Reciprocal Rank (MRR)**: Focuses on the rank of the first relevant document in the ranked list.\n5. **F1@k**: A balanced evaluation of precision and recall, reflecting both aspects.\n\n**Results**\n\nThe authors present a comprehensive analysis of various configurations using IL-PCR and COLIEE datasets:\n\n1. **Retrieval Method Performance**: Vector DB outperformed BM25 in terms of MAP (0.3484 vs 0.2116) and MRR (0.3585 vs 0.2182), with optimal k values between 5-7.\n2. **Impact of Rhetorical Role Configurations**: Using specific rhetorical roles improved retrieval effectiveness, especially with Facts+Issue+Reasoning configuration achieving the best results across all methods.\n\n**Limitations**\n\n1. **Semantic Sparsity**: The system faces challenges due to semantic sparsity arising from using simple rhetorical roles like Facts and Issues.\n2. **Overlap Between Similar Roles**: Overlap between similar roles can lead to performance degradation.\n3. **Class Imbalance**: Class imbalance in the dataset poses difficulties for accurate evaluation.\n\n**Future Work**\n\nThe authors propose exploring cross-lingual and multi-domain retrieval, as well as addressing the limitations mentioned above to further improve the system's robustness and effectiveness."
    },
    {
        "id": "2508.00709v1",
        "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian\n  Common Law System",
        "authors": [
            "Shubham Kumar Nigam",
            "Balaramamahanthi Deepak Patnaik",
            "Shivam Mishra",
            "Ajay Varghese Thomas",
            "Noel Shallum",
            "Kripabandhu Ghosh",
            "Arnab Bhattacharya"
        ],
        "Abstract": "Abstract Legal Judgment Prediction (LJP) has emerged as a key area in AI for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. While previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. In this work, we propose NyayaRAG , a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. NyayaRAG evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the Indian legal system. We assess performance across various input configurations using both standard lexical and semantic metrics as well as LLM-based evaluators such as G-Eval. Our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality.",
        "Main": {
            "1 Introduction": [
                "The application of artificial intelligence (AI) in legal judgment prediction (LJP) has the potential to transform legal systems by improving efficiency, transparency, and access to justice. This is particularly crucial for India, where millions of cases remain pending in courts, and decision-making is inherently dependent on factual narratives, statutory interpretation, and judicial precedent. India follows a common law system, where prior decisions (precedents) and statutory provisions play a central role in influencing legal outcomes. However, most existing AI-based LJP systems do not adequately replicate this fundamental feature of judicial reasoning.",
                "Previous studies such as Malik et al. ( 2021 ); Nigam et al. ( 2024b , 2025a ) have focused on predicting legal outcomes using the current case document, including sections like facts, arguments, issues, reasoning, and decision. More recent efforts have narrowed the scope to factual inputs alone (Nigam et al., 2024a , 2025b ) , yet these systems still operate in a vacuum, without considering how courts naturally rely on applicable laws and prior rulings. In reality, judges rarely decide in isolation; instead, they actively refer to relevant precedent and statutory law. To bridge this gap, we propose a framework that more closely mirrors actual courtroom conditions by explicitly incorporating external legal knowledge during inference.",
                "Moreover, in critical domains like finance, medicine, and law, decisions must be grounded in verifiable information. Experts in these domains cannot rely on opaque, black-box inferences, and they require systems that ensure factual consistency. Hallucinations, common in large generative models, can have severe consequences in legal decision-making. By retrieving and conditioning model responses on grounded sources such as applicable laws and precedent cases, Retrieval-Augmented Generation (RAG) offers a principled approach to mitigate hallucination and promote trustworthy outputs. Furthermore, RAG frameworks like ours can be flexibly integrated into existing legal systems without requiring the retraining of core models or the sharing of private or sensitive case data. This enhances user trust while allowing the legal community to benefit from AI without sacrificing transparency or data confidentiality.",
                "We introduce NyayaRAG , a Retrieval-Augmented Generation (RAG) framework for realistic legal judgment prediction and explanation in the Indian common law system. The term “NyayaRAG” is derived from two components: “ Nyaya ” meaning “justice” and “ RAG ” referring to “Retrieval-Augmented Generation”. Together, the name reflects our vision to build a justice-aware generation system that emulates the reasoning process followed by Indian courts, using facts, statutes, and precedents.",
                "Unlike prior models that operate purely on internal case content, NyayaRAG simulates real-world judicial decision-making by providing the model with: (i) the summarized factual background of the current case, (ii) relevant statutory provisions, (iii) top- k semantically retrieved previous similar judgments. This structure emulates how judges deliberate on new cases, consulting both textual statutes and prior judicial opinions. Through this design, we evaluate how Retrieval-Augmented Generation can help reduce hallucinations, promote faithfulness, and yield legally coherent predictions and explanations.",
                "Our contributions are as follows: 1. A Realistic RAG Framework for Indian Courts: We present NyayaRAG , a novel framework that emulates Indian common law decision-making by incorporating not only facts but also retrieved legal statutes and precedents. 2. Retrieval-Augmented Pipelines with Structured Inputs: We construct modular pipelines representing different combinations of factual, statutory, and precedent-based inputs to understand their individual and combined contributions to model performance. 3. Simulating Common Law Reasoning with LLMs: We show that LLMs guided by RAG and factual grounding can produce legally faithful explanations aligned with how real-world decisions are made under common law reasoning.",
                "Our work moves beyond fact-only or self-contained models by replicating a more faithful legal reasoning pipeline aligned with Indian jurisprudence. We hope that NyayaRAG opens new directions for building interpretable, retrieval-aware AI systems in legal settings, particularly in resource-constrained yet precedent-driven judicial systems like India’s. For the sake of reproducibility, we have made our dataset, code, and RAG-based pipeline implementation via a GitHub repository 1 1 1 https://github.com/ShubhamKumarNigam/RAGLegal ."
            ],
            "2 Related Work": [
                "Figure 1: Illustration of our Legal Judgment Prediction framework using RAG. The input legal judgment is first summarized; a RAG agent retrieves top-3 relevant documents from a vector database; and an instruction-tuned LLM (e.g., LLaMA-3.1 8B Instruct) generates the final prediction and explanation.",
                "Recent advancements in natural language processing (NLP) and large language models (LLMs) have significantly improved the performance of question answering (QA) and legal decision support systems. Transformer-based architectures such as BERT (Devlin et al., 2018 ) , GPT (Radford et al., 2019 ) , and their instruction-tuned successors have led to robust capabilities in knowledge-intensive and multi-hop reasoning tasks. The integration of external information via Retrieval-Augmented Generation (RAG) has emerged as a particularly effective approach for enhancing generation fidelity and reducing hallucinations (Han et al., 2024 ; Hei et al., 2024 ) .",
                "Within the legal domain, Legal Judgment Prediction (LJP) has seen significant progress, with models trained to infer outcomes based on factual and procedural components of court cases (Strickson and De La Iglesia, 2020 ; Xu et al., 2020 ; Feng et al., 2023 ) . In the Indian legal context, the ILDC corpus (Malik et al., 2021 ) and its extended variants (Nigam et al., 2024b ; Nigam and Deroy, 2023 ) have enabled the development of supervised and instruction-tuned models for both judgment prediction and explanation. The emergence of domain-specific datasets and architectures has allowed LJP systems to move from simple binary classification to more complex reasoning tasks aligned with real judicial behavior (Vats et al., 2023 ) .",
                "Parallel to these developments, there has been a sharp rise in interest in RAG techniques for legal NLP. Several benchmark and system-level contributions have explored how retrieval-enhanced generation can be leveraged to assist legal professionals, improve legal QA systems, and support document analysis. Notably, LegalBench-RAG (Pipitone and Alami, 2024 ) introduced a benchmark suite for evaluating RAG in the legal domain. Survey papers like (Hindi et al., 2025 ) provide comprehensive overviews of techniques aimed at improving RAG performance, factual grounding, and interpretability in legal settings.",
                "Several system-level contributions have demonstrated the power of RAG in specialized applications. Graph-RAG for Legal Norms (de Martim, 2025 ) and Bridging Legal Knowledge and AI (Barron et al., 2025 ) proposed methods to integrate structured legal knowledge such as statutes and normative hierarchies into the retrieval pipeline. Similarly, CBR-RAG (Wiratunga et al., 2024 ) applied case-based reasoning to leverage historical decisions, showing strong gains in legal question answering. HyPA-RAG (Kalra et al., 2024 ) explored hybrid parameter-adaptive retrieval to dynamically adjust context based on query specificity.",
                "Further domain-specific applications include AI-powered legal assistants like Legal Query RAG (Wahidur et al., 2025 ) and RAG-based solutions for dispute resolution in housing law (Rafat, 2024 ) . Optimizing Legal Information Access (Amato et al., 2024 ) showcased federated RAG architectures for secure document retrieval, and Augmenting Legal Judgment Prediction with Contrastive Case Relations (Liu et al., 2022 ) illustrated the benefits of encoding contrastive precedents for predictive reasoning."
            ],
            "3 Task Description": [
                "India’s judicial system operates within the common law framework, where judges deliberate cases based on three fundamental pillars: (i) the factual context of the case, (ii) applicable statutory provisions, and (iii) relevant judicial precedents. Our task is designed to simulate such realistic legal decision-making by leveraging Retrieval-Augmented Generation (RAG), enabling models to access external legal knowledge during inference.",
                "Figure 1 illustrates our Legal Judgment Prediction (LJP) pipeline enhanced with RAG. The pipeline begins with a full legal judgment document, which undergoes summarization to reduce its length and retain essential factual meaning. This is necessary because legal judgments tend to be long, and appending retrieved knowledge further increases the input size. Given limited model capacity and computational resources, we employ a summarization step (using Mixtral-8x7B-Instruct-v0.1 ) to create a condensed representation of both the input case and the retrieved legal context.",
                "Prediction Task: Based on the summarized factual description D D italic_D and the retrieved top- k k italic_k (e.g., k = 3 k=3 italic_k = 3 ) similar legal documents (statutes or precedents), the model predicts the likely court judgment. The prediction label y ∈ { 0 , 1 } y\\in\\{0,1\\} italic_y ∈ { 0 , 1 } indicates whether the appeal is fully rejected (0) or fully/partially accepted (1). This binary framing captures the most common forms of judicial decisions in Indian appellate courts.",
                "Explanation Task: Alongside the decision, the model is also required to generate an explanation that justifies its output. This explanation should logically incorporate the facts, cited statutes, and relevant precedents retrieved during the RAG process. This step emulates how judges provide reasoned opinions in written judgments.",
                "By structuring the LJP task in this way, summarizing long documents and integrating retrieval-based augmentation, we study the effectiveness of RAG agents in producing judgments that are both faithful to legal reasoning and grounded in precedent and statute. The overall framework allows us to approximate a real-world decision-making environment within Indian courtrooms."
            ],
            "4 Dataset": [
                "Our dataset is designed to simulate realistic court decision-making in the Indian legal context, incorporating facts, statutes, and precedent, essential elements under the common law framework. This dataset enables exploration of Legal Judgment Prediction (LJP) in a Retrieval-Augmented Generation (RAG) setup.",
                {
                    "4.1 Dataset Compilation": [
                        "We curated a large-scale dataset consisting of 56,387 Supreme Court of India (SCI) case documents up to April 2024, sourced from IndianKanoon 2 2 2 https://indiankanoon.org/ , a trusted legal search engine. The website provides structural tags for various judgment components (e.g., facts, issues, arguments), which allowed for clean and structured scraping. These documents serve as the foundation for our summarization, retrieval, and reasoning experiments."
                    ]
                },
                {
                    "4.2 Dataset Composition": [
                        "The corpus supports multiple downstream pipelines, each focusing on specific judgment elements or legal context. Table 1 presents key statistics across different configurations, and an example breakdown is shown in the Appendix Table 7 .",
                        {
                            "4.2.1 Case Text": [
                                "Each judgment includes complete narrative content such as factual background, party arguments, legal issues, reasoning, and verdict. Due to length constraints exceeding model context windows, we summarized these documents using Mixtral-8x7B-Instruct-v0.1 (Jiang et al., 2024 ) , which supports up to 32k tokens. The summarization preserved critical legal elements through carefully designed prompts (see Table 2 ).",
                                "Dataset #Documents Avg. Length Max SCI (Full) 56,387 3,495 401,985 Summarized Single 4,962 302 875 Summarized Multi 4,930 300 879 Sections 29,858 257 27,553 Table 1: NyayaRAG Data Statistics.",
                                "Summarization Prompt The text is regarding a court judgment for a specific case. Summarize it into 1000 tokens but more than 700 tokens. The summarization should highlight the Facts, Issues, Statutes, Ratio of the decision, Ruling by Present Court (Decision), and a Conclusion. Table 2: Instruction prompt used with Mixtral-8x7B-Instruct-v0.1 for summarizing legal judgments."
                            ]
                        },
                        {
                            "4.2.2 Precedents": [
                                "From each judgment, cited precedents were extracted using metadata tags provided by IndianKanoon. These citations represent explicit legal reasoning and are retained for use during inference to replicate how courts consider prior judgments."
                            ]
                        },
                        {
                            "4.2.3 Statutes": [
                                "Statutory references were also programmatically extracted, including citations to laws like the Indian Penal Code and the Constitution of India. Where statute sections exceeded length limits, they were summarized using the same LLM pipeline. Only statutes directly cited in the respective cases were retained, ensuring relevance."
                            ]
                        },
                        {
                            "4.2.4 Previous Similar Cases": [
                                "To simulate implicit precedent-based reasoning, we employed semantic similarity retrieval to identify relevant previous cases beyond explicit citations:",
                                "• Corpus Vectorization: All 56,387 documents were embedded into dense vector representations using the all-MiniLM-L6-v2 sentence transformer. • Target Encoding: The 5,000 selected training samples were vectorized similarly. • Top- k k italic_k Retrieval: Using ChromaDB , we retrieved the top-3 most semantically similar cases for each document based on cosine similarity. • Augmentation: Retrieved cases were appended to the factual input to form the “ casetext + previous similar cases ” input during model inference.",
                                "This retrieval step enriches context with precedents that are semantically close, even if not cited, enhancing the legal realism of our setup."
                            ]
                        },
                        {
                            "4.2.5 Facts": [
                                "We separately extracted the factual portions of all 56,387 judgments. These include background information, chronological events, and party narratives, excluding legal reasoning. These fact-only subsets were used to simulate realistic courtroom scenarios where judges primarily rely on facts, relevant law, and precedent for decision-making.",
                                "Overall, our dataset is uniquely structured to test legal decision-making under realistic constraints, aligning with the Indian legal system’s reliance on factual narratives, statutory frameworks, and prior rulings."
                            ]
                        }
                    ]
                }
            ],
            "5 Methodology": [
                "To simulate realistic judgment prediction and evaluate the role of RAG in enhancing legal decision-making, we design a modular experimental setup. This setup explores how different types of legal information, such as factual summaries, statutes, and precedents, affect model performance on the dual tasks of prediction and explanation. To ensure reproducibility and transparency, we detail the full experimental setup, including model configurations, training routines, and task-specific hyperparameters, in Appendix A . This includes separate subsections for the explanation generation (summarization) and legal judgment prediction tasks, outlining all relevant decoding strategies, optimization settings, and dataset splits used across our pipeline variants.",
                {
                    "5.1 Pipeline Construction": [
                        "To systematically evaluate the impact of legal knowledge sources, we constructed multiple input pipelines using combinations of the dataset components described in Section 4 . Each pipeline configuration represents a distinct input scenario reflecting different degrees of legal context and retrieval augmentation. These pipelines are as follows:",
                        "• CaseText Only: Includes only the summarized version of the full case judgment, which contains factual background, arguments, and reasoning. • CaseText + Statutes: Appends summarized statutory references cited in the judgment to the case text, simulating scenarios where relevant laws are explicitly considered. • CaseText + Precedents: Incorporates prior cited judgments mentioned in the original case, representing explicitly relied-upon precedents. • CaseText + Previous Similar Cases: Adds top-3 semantically similar past judgments (retrieved via ChromaDB using all-MiniLM-L6-v2 embeddings), allowing the model to learn from precedents not explicitly cited. • CaseText + Statutes + Precedents: A comprehensive legal input pipeline combining the full judgment summary, statutes, and cited prior judgments. • Facts Only: A minimal pipeline containing only the factual summary, excluding all legal reasoning and verdicts. This setup evaluates whether a model can infer judgments from facts alone. • Facts + Statutes + Precedents: Combines factual input with statutory and precedent context to simulate realistic courtroom conditions where judges rely on facts, applicable law, and relevant past cases.",
                        "This modular design enables granular control over input features and facilitates direct comparison of how each knowledge source contributes to judgment prediction and explanation generation."
                    ]
                },
                {
                    "5.2 Prompt Design": [
                        "To ensure consistency and interpretability across all pipelines, we used fixed instruction prompts with minor variations depending on the available contextual inputs (e.g., facts only vs. facts + law + precedent). These prompts guide the model in producing both binary predictions and natural language explanations. Prompts were structured to reflect real judicial inquiry formats, aligning with the instruction-following capabilities of modern LLMs. Full prompt templates are listed in Appendix Table 8 , along with prediction examples."
                    ]
                },
                {
                    "5.3 Inference Setup": [
                        "We use the LLaMA-3.1 8B Instruct Dubey et al. ( 2024 ) model for all experiments in a few-shot prompting setup. Each input sequence, composed according to one of the pipeline templates, is paired with a relevant prompt. The model is required to output:",
                        "• A binary judgment prediction: 0 (appeal rejected) or 1 (appeal fully/partially accepted) • A justification: a coherent explanation based on legal facts, statutes, and precedent",
                        "The model is explicitly instructed to reason with the provided information and emulate judicial writing. Retrieved knowledge (via RAG) is included in-context to enhance legal reasoning while minimizing hallucinations.",
                        "This experimental design allows us to evaluate the effectiveness of legal retrieval and summarization under realistic judicial decision-making constraints in the Indian common law setting."
                    ]
                }
            ],
            "6 Evaluation Metrics": [
                "Pipeline Name Partition Accuracy Precision Recall F1-score CaseText Only Single 62.27 33.50 30.88 29.45 Multi 53.10 25.26 23.95 20.81 CaseText + Statutes Single 67.07 45.29 44.55 44.32 Multi 60.36 64.22 64.04 60.35 CaseText + Precedents Single 61.73 41.92 41.35 40.81 Multi 57.53 61.34 61.19 57.53 CaseText + Previous Similar Cases Single 57.53 61.34 61.19 57.53 Multi 61.73 41.92 41.35 57.53 CaseText + Statutes + Precedents Single 64.71 43.50 42.98 42.78 Multi 65.86 63.94 63.99 63.96 CaseFacts Only Single 51.13 51.36 51.30 50.68 Multi 53.71 51.18 51.18 51.18 Facts + Statutes + Precedents Single 50.58 33.57 33.56 33.24 Multi 52.57 52.01 52.01 52.01 Table 3: Performance of Various Pipelines on Binary and Multi-label Legal Judgment Prediction. The best result has been marked in bold.",
                "To evaluate the effectiveness of our Retrieval-Augmented Legal Judgment Prediction framework, we adopt a comprehensive set of metrics covering both classification accuracy and explanation quality. The evaluation is conducted on two fronts: the judgment prediction task and the explanation generation task. These metrics are selected to ensure a holistic assessment of model performance in the legal domain. We report Macro Precision, Macro Recall, Macro F1, and Accuracy for judgment prediction, and we use both quantitative and qualitative methods to evaluate the quality of explanations generated by the model.",
                "1. Lexical-based Evaluation: We utilized standard lexical similarity metrics, including Rouge-L Lin ( 2004 ) , BLEU Papineni et al. ( 2002 ) , and METEOR Banerjee and Lavie ( 2005 ) . These metrics measure the overlap and order of words between the generated explanations and the reference texts, providing a quantitative assessment of the lexical accuracy of the model outputs. 2. Semantic Similarity-based Evaluation: To capture the semantic quality of the generated explanations, we employed BERTScore Zhang et al. ( 2020 ) , which measures the semantic similarity between the generated text and the reference explanations. Additionally, we used BLANC Vasilyev et al. ( 2020 ) , a metric that estimates the quality of generated text without a gold standard, to evaluate the model’s ability to produce semantically meaningful and contextually relevant explanations. 3. LLM-based Evaluation (LLM-as-a-Judge): To complement traditional metrics, we incorporate an automatic evaluation strategy that uses large language models themselves as evaluators, commonly referred to as LLM-as-a-Judge . This evaluation is crucial for assessing structured argumentation and legal correctness in a format aligned with expert judicial reasoning. We adopt G-Eval Liu et al. ( 2023 ) , a GPT-4-based evaluation framework tailored for natural language generation tasks. G-Eval leverages chain-of-thought prompting and structured scoring to assess explanations along three key criteria: factual accuracy , completeness & coverage , and clarity & coherence . Each generated legal explanation is scored on a scale from 1 to 10 based on how well it aligns with the expected content and a reference document. The exact prompt format used for evaluation is shown in Appendix Table 9 . For our experiments, we use the GPT-4o-mini model to generate reliable scores without manual intervention. This setup provides an interpretable, unified judgment metric that captures legal soundness, completeness of reasoning, and logical coherence, beyond what traditional similarity-based metrics can offer. 4. Expert Evaluation: To validate the interpretability and legal soundness of the model-generated explanations, we conduct an expert evaluation involving legal professionals. They rate a representative subset of the generated outputs on a 1–10 Likert scale across three criteria: factual accuracy, legal relevance, and completeness of reasoning. A score of 1 denotes a poor or misleading explanation, while a 10 reflects high legal fidelity and argumentative soundness. This evaluation provides critical insights beyond automated metrics. 5. Inter-Annotator Agreement (IAA): To ensure the reliability and consistency of expert judgments, we compute standard IAA statistics, including Fleiss’ Kappa, Cohen’s Kappa, Krippendorff’s Alpha, Intraclass Correlation Coefficient (ICC), and Pearson Correlation. These metrics quantify the degree of agreement across expert raters, reinforcing the credibility of the expert evaluation framework. Full details and scores are available in Appendix B ."
            ],
            "7 Results and Analysis": [
                "Pipelines RL BLEU METEOR BERTScore BLANC G-Eval Expert Score Single Partition CaseText Only 0.16 0.03 0.18 0.52 0.08 4.17 5.2 CaseText + Statutes 0.17 0.03 0.20 0.53 0.09 4.21 5.5 CaseText + Precedents 0.16 0.03 0.19 0.51 0.08 3.45 4.6 CaseText + Previous Similar Cases 0.16 0.03 0.20 0.52 0.08 3.72 4.9 CaseText + Statutes + Precedents 0.16 0.03 0.19 0.52 0.08 4.11 5.4 CaseFacts Only 0.16 0.02 0.18 0.52 0.06 3.53 4.5 Facts + Statutes + Precedents 0.16 0.02 0.18 0.51 0.06 2.97 3.9 Multi Partition CaseText Only 0.16 0.03 0.18 0.52 0.08 4.00 5.0 CaseText + Statutes 0.17 0.03 0.20 0.53 0.09 4.10 5.3 CaseText + Precedents 0.16 0.03 0.20 0.53 0.09 3.41 4.4 CaseText + Previous Similar Cases 0.16 0.03 0.19 0.52 0.08 3.67 4.7 CaseText + Statutes + Precedents 0.16 0.03 0.20 0.53 0.09 3.92 5.2 CaseFacts Only 0.15 0.02 0.17 0.52 0.08 3.74 4.6 Facts + Statutes + Precedents 0.15 0.02 0.19 0.52 0.07 3.08 4.1 Table 4: Comparison of Explanation Generation Across Different Legal Context Pipelines.",
                "We conducted extensive evaluations across multiple pipeline configurations to study the impact of different legal information components on both judgment prediction and explanation quality. Tables 3 and 4 summarize the model’s performance across these configurations for binary and multi-label settings.",
                {
                    "7.1 Judgment Prediction Performance": [
                        "As shown in Table 3 , the pipeline combining CaseText + Statutes achieved the highest accuracy in the single-label setting. This suggests that legal statutes provide substantial contextual cues for the model to infer the likely decision. In contrast, CaseText Only achieved 62.27%, highlighting the importance of augmenting case narratives with applicable laws. Interestingly, the CaseText + Previous Similar Cases pipeline showed the highest precision, recall, and F1-score in the single-label case, indicating that semantically retrieved precedents, despite not being explicitly cited, help the model align with actual judicial outcomes.",
                        "In the multi-label setting, the best accuracy was observed for the CaseText + Statutes + Precedents pipeline. This comprehensive context provides the model with structured legal knowledge, improving generalization across different outcome labels. Conversely, the Facts Only pipeline performed worst overall, reaffirming that factual narratives alone, without legal context, are insufficient for reliably predicting legal outcomes. The poor performance of the Facts + Statutes + Precedents pipeline in the single-label setting suggests that factual sections might lack the interpretive cues that full case texts offer when combined with legal references."
                    ]
                },
                {
                    "7.2 Explanation Generation Quality": [
                        "Table 4 presents the results of explanation evaluation using a diverse set of metrics, including both automatic lexical and semantic metrics (ROUGE, BLEU, METEOR, BERTScore, BLANC) and a large language model-based evaluation (G-Eval). Across both single and multi-label setups, the CaseText + Statutes pipeline consistently outperformed all other configurations. In the single-label setting, it achieved the highest scores across key dimensions, substantially outperforming the CaseText Only baseline. This result underscores the critical role of statutory references in enhancing both the factual alignment and interpretability of model-generated legal explanations.",
                        "Interestingly, while the CaseText + Previous Similar Cases pipeline yielded strong lexical overlap (e.g., top ROUGE-L in the unabridged version), it lagged behind the statute-enhanced pipeline in metrics that assess semantic and contextual alignment, such as G-Eval and BLANC. This indicates that while similar cases might help the model replicate surface-level language, they may not consistently offer legally grounded or complete reasoning. Meanwhile, the CaseText + Statutes + Precedents pipeline also performed competitively, suggesting that combining structured legal references with precedent data can lead to balanced and high-quality explanations.",
                        "In contrast, configurations that relied solely on factual narratives ( CaseFacts Only and Facts + Statutes + Precedents ) exhibited comparatively poor performance across all evaluation metrics. For example, the Facts + Statutes + Precedents pipeline recorded a G-Eval score as low in the single-label setting. This reinforces the notion that factual descriptions, while essential, are insufficient for constructing legally persuasive rationales. The absence of structured legal arguments, statutory alignment, or precedent citation in these setups appears to undermine their explanatory effectiveness.",
                        {
                            "Expert Evaluation:": [
                                "To complement automatic evaluations, we also conducted a small-scale expert evaluation involving experienced legal professionals. Each expert independently rated a subset of model-generated explanations based on factual accuracy, legal relevance, and completeness using a 10-point Likert scale. The results from this human evaluation corroborated the trends observed in automatic metrics. Notably, the CaseText + Statutes pipeline received the highest expert score among all configurations, reinforcing the positive impact of statutory knowledge on explanation quality. In contrast, fact-only pipelines again received the lowest expert ratings, echoing concerns about their insufficient legal reasoning depth.",
                                "To ensure the reliability of expert scores, we conducted a detailed Inter-Annotator Agreement (IAA) analysis across multiple evaluation dimensions. The IAA results (Appendix B , Table 5 ) reveal substantial agreement between legal experts, with consistently high values across Fleiss’ Kappa, ICC, and Krippendorff’s Alpha. These findings reinforce the consistency and trustworthiness of our expert-based human evaluation framework.",
                                "Overall, the results emphasize the effectiveness of Retrieval-Augmented Generation (RAG) when paired with structured legal content, especially statutes, in producing accurate, interpretable, and legally coherent explanations. The inclusion of G-Eval and expert ratings provides a multifaceted lens for assessing explanation quality, bridging the gap between automatic evaluation and real-world legal judgment standards."
                            ]
                        }
                    ]
                }
            ],
            "8 Ablation Study: Understanding the Role of Legal Context Components": [
                "To assess the individual contribution of each legal context component, factual narratives, statutory provisions, cited precedents, and semantically similar past cases, we perform an ablation study by systematically removing or altering these inputs across pipeline configurations. This study highlights how each component affects prediction accuracy and explanation quality, as reported in Tables 3 and 4 .",
                {
                    "Impact on Judgment Prediction:": [
                        "The CaseText + Statutes + Precedents pipeline serves as the most comprehensive baseline. Removing statutory references (i.e., CaseText + Precedents ) leads to a noticeable drop in F1-score (from 63.96 to 57.53 in the multi-label setting), indicating that legal provisions provide structured grounding essential for accurate predictions. Similarly, eliminating precedents (i.e., CaseText + Statutes ) also reduces performance, though the drop is less steep, suggesting complementary roles of statutes and precedents. Pipelines relying solely on factual case narratives (e.g., CaseFacts Only ) perform the worst, reaffirming that factual information alone is insufficient for robust legal outcome prediction."
                    ]
                },
                {
                    "Impact on Explanation Quality:": [
                        "A similar pattern emerges in explanation generation. The CaseText + Statutes pipeline consistently outperforms others across ROUGE, BLEU, METEOR, BERTScore, and G-Eval metrics, underscoring the importance of grounding explanations in explicit statutory language. When only precedents are added (without statutes), as in CaseText + Precedents , explanation scores drop significantly (e.g., G-Eval: 4.21 to 3.45 in the single-label case). The worst-performing setup is Facts + Statutes + Precedents , highlighting that factual inputs, even when supplemented with legal references, do not suffice for generating coherent and persuasive explanations if the core case context is missing."
                    ]
                },
                {
                    "Insights:": [
                        "These findings validate the design choices in NyayaRAG , where integrating factual case text with statutory and precedential knowledge mimics real-world judicial reasoning. Statutory references provide normative structure, while precedents offer context-specific analogies. Their absence not only reduces predictive performance but also degrades the factuality, clarity, and legal coherence of the generated explanations.",
                        "This ablation analysis also offers practical guidance: for retrieval-augmented systems deployed in legal contexts, careful curation and combination of retrieved statutes and relevant precedents are critical to ensure trustworthy outputs."
                    ]
                }
            ],
            "9 Conclusion and Future Scope": [
                "This paper introduced NyayaRAG , a Retrieval-Augmented Generation framework tailored for realistic legal judgment prediction and explanation in the Indian common law system. By combining factual case details with retrieved statutory provisions and relevant precedents, our approach mirrors judicial reasoning more closely than prior methods that rely solely on the case text. Empirical results across prediction and explanation tasks confirm that structured legal retrieval enhances both outcome accuracy and interpretability. Pipelines enriched with statutes and precedents consistently outperformed baselines, as validated by lexical, semantic, and LLM-based (G-Eval) metrics, as well as expert feedback.",
                "Future directions include extending to hierarchical verdict structures, integrating symbolic or graph-based retrieval, modeling temporal precedent evolution, and leveraging human-in-the-loop mechanisms. NyayaRAG marks a step toward court-aligned, explainable legal AI and sets the foundation for future research in retrieval-enhanced legal systems within underrepresented jurisdictions."
            ],
            "Limitations": [
                "While NyayaRAG marks a significant advance in realistic legal judgment prediction under the Indian common law framework, several limitations merit further attention.",
                "First, although Retrieval-Augmented Generation (RAG) helps reduce hallucinations by grounding outputs in retrieved legal documents, it does not fully eliminate factual or interpretive inaccuracies. In sensitive domains such as law, even rare errors in reasoning or justification may raise concerns about reliability and accountability.",
                "Second, the current framework supports binary and multi-label outcome structures but does not yet handle the full spectrum of legal verdicts, such as hierarchical or multi-class decisions involving complex legal provisions. Expanding to richer verdict taxonomies would enable broader applicability and deeper case understanding.",
                "Third, NyayaRAG assumes the availability of clean, well-structured legal documents and relies on summarization pipelines to manage input length. However, real-world legal texts often contain noise, OCR errors, or inconsistent formatting. Although summarization aids conciseness, it may inadvertently omit subtle legal nuances that affect judgment outcomes or explanation quality.",
                "Finally, due to computational resource constraints, the current system utilizes instruction-tuned LLMs guided by domain-specific prompts rather than fully fine-tuning on large-scale Indian legal corpora. While prompt-based tuning remains efficient and modular, fine-tuning on in-domain legal texts could further enhance model fidelity and domain alignment.",
                "Despite these limitations, NyayaRAG provides a robust and interpretable foundation for judgment prediction and explanation, supported by both automatic and expert evaluations. Future work that addresses these constraints, particularly hierarchical decision modeling and domain-specific fine-tuning, will further strengthen the framework’s legal relevance and practical deployment potential."
            ],
            "Ethics Statement": [
                "This research adheres to established ethical standards for conducting work in high-stakes domains such as law. The legal documents used in our study were sourced from IndianKanoon ( https://indiankanoon.org/ ), a publicly available repository of Indian court judgments. All documents are in the public domain and do not include sealed cases or personally identifiable sensitive information, ensuring that our use of the data complies with privacy and confidentiality norms.",
                "We emphasize that the proposed NyayaRAG system is developed strictly for academic research purposes to simulate realistic legal reasoning processes. It is not intended for direct deployment in real-world legal settings. The model outputs must not be construed as legal advice, official court predictions, or determinants of legal outcomes. Any downstream use should be performed with oversight by qualified legal professionals. We strongly discourage the use of this system in live legal cases, policymaking, or decisions that may affect individuals’ rights without appropriate human-in-the-loop supervision.",
                "As part of our evaluation protocol, we involved domain experts (legal professionals and researchers) to assess the quality and legal coherence of the generated explanations. The evaluation was conducted on a curated subset of samples, and all participating experts were informed of the research objectives and voluntarily participated without any coercion or conflict of interest. No personal data was collected during this process, and all expert feedback was anonymized for analysis.",
                "While we strive to enhance legal interpretability and transparency, we acknowledge that legal documents themselves may reflect systemic biases. Our framework, while replicating judicial reasoning patterns, may inherit such biases from training data. We do not deliberately introduce or amplify such biases, but we recognize the importance of further work in fairness auditing, particularly across litigant identity, socio-demographic markers, and jurisdictional diversity.",
                ""
            ]
        },
        "Tuples": [
            [
                "1 Introduction",
                "The application of artificial intelligence (AI) in legal judgment prediction (LJP) has the potential to transform legal systems by improving efficiency, transparency, and access to justice. This is particularly crucial for India, where millions of cases remain pending in courts, and decision-making is inherently dependent on factual narratives, statutory interpretation, and judicial precedent. India follows a common law system, where prior decisions (precedents) and statutory provisions play a central role in influencing legal outcomes. However, most existing AI-based LJP systems do not adequately replicate this fundamental feature of judicial reasoning.",
                "India's legal system has millions of pending cases, with decision-making influenced by factual narratives, statutory interpretation, and prior decisions (precedents). Existing AI-based LJP systems fail to replicate these key features."
            ],
            [
                "1 Introduction",
                "Previous studies such as Malik et al. ( 2021 ); Nigam et al. ( 2024b , 2025a ) have focused on predicting legal outcomes using the current case document, including sections like facts, arguments, issues, reasoning, and decision. More recent efforts have narrowed the scope to factual inputs alone (Nigam et al., 2024a , 2025b ) , yet these systems still operate in a vacuum, without considering how courts naturally rely on applicable laws and prior rulings. In reality, judges rarely decide in isolation; instead, they actively refer to relevant precedent and statutory law. To bridge this gap, we propose a framework that more closely mirrors actual courtroom conditions by explicitly incorporating external legal knowledge during inference.",
                "Malik et al. (2021) and Nigam et al. (2024b, 2025a) focused on predicting legal outcomes using current case documents. Nigam et al. (2024a, 2025b) narrowed scope to factual inputs alone. We propose a framework incorporating external legal knowledge during inference to mirror actual courtroom conditions."
            ],
            [
                "1 Introduction",
                "Moreover, in critical domains like finance, medicine, and law, decisions must be grounded in verifiable information. Experts in these domains cannot rely on opaque, black-box inferences, and they require systems that ensure factual consistency. Hallucinations, common in large generative models, can have severe consequences in legal decision-making. By retrieving and conditioning model responses on grounded sources such as applicable laws and precedent cases, Retrieval-Augmented Generation (RAG) offers a principled approach to mitigate hallucination and promote trustworthy outputs. Furthermore, RAG frameworks like ours can be flexibly integrated into existing legal systems without requiring the retraining of core models or the sharing of private or sensitive case data. This enhances user trust while allowing the legal community to benefit from AI without sacrificing transparency or data confidentiality.",
                "Experts in finance, medicine, and law require verifiable information for decision-making. RAG (Retrieval-Augmented Generation) mitigates hallucinations by retrieving info from grounded sources like laws and precedent cases, ensuring trustworthy outputs and user trust. This approach integrates well with existing legal systems without retraining models or sharing sensitive data."
            ],
            [
                "1 Introduction",
                "We introduce NyayaRAG , a Retrieval-Augmented Generation (RAG) framework for realistic legal judgment prediction and explanation in the Indian common law system. The term “NyayaRAG” is derived from two components: “ Nyaya ” meaning “justice” and “ RAG ” referring to “Retrieval-Augmented Generation”. Together, the name reflects our vision to build a justice-aware generation system that emulates the reasoning process followed by Indian courts, using facts, statutes, and precedents.",
                "NyayaRAG is a Retrieval-Augmented Generation framework for predicting and explaining legal judgments in India's common law system. It's derived from \"Nyaya\" (justice) and \"RAG\" (Retrieval-Augmented Generation)."
            ],
            [
                "1 Introduction",
                "Unlike prior models that operate purely on internal case content, NyayaRAG simulates real-world judicial decision-making by providing the model with: (i) the summarized factual background of the current case, (ii) relevant statutory provisions, (iii) top- k semantically retrieved previous similar judgments. This structure emulates how judges deliberate on new cases, consulting both textual statutes and prior judicial opinions. Through this design, we evaluate how Retrieval-Augmented Generation can help reduce hallucinations, promote faithfulness, and yield legally coherent predictions and explanations.",
                "NyayaRAG simulates real-world judicial decision-making by providing the model with (i) summarized case background, (ii) statutory provisions, (iii) top-k semantically retrieved previous similar judgments. This design aims to reduce hallucinations, promote faithfulness and yield coherent predictions and explanations."
            ],
            [
                "1 Introduction",
                "Our contributions are as follows: 1. A Realistic RAG Framework for Indian Courts: We present NyayaRAG , a novel framework that emulates Indian common law decision-making by incorporating not only facts but also retrieved legal statutes and precedents. 2. Retrieval-Augmented Pipelines with Structured Inputs: We construct modular pipelines representing different combinations of factual, statutory, and precedent-based inputs to understand their individual and combined contributions to model performance. 3. Simulating Common Law Reasoning with LLMs: We show that LLMs guided by RAG and factual grounding can produce legally faithful explanations aligned with how real-world decisions are made under common law reasoning.",
                "NyayaRAG is a novel framework emulating Indian common law decision-making. It includes: \n- Facts\n- Retrieved legal statutes\n- Precedents\nLLMs guided by RAG and factual grounding can produce legally faithful explanations under common law reasoning."
            ],
            [
                "1 Introduction",
                "Our work moves beyond fact-only or self-contained models by replicating a more faithful legal reasoning pipeline aligned with Indian jurisprudence. We hope that NyayaRAG opens new directions for building interpretable, retrieval-aware AI systems in legal settings, particularly in resource-constrained yet precedent-driven judicial systems like India’s. For the sake of reproducibility, we have made our dataset, code, and RAG-based pipeline implementation via a GitHub repository 1 1 1 https://github.com/ShubhamKumarNigam/RAGLegal .",
                "NyayaRAG replicates a legal reasoning pipeline aligned with Indian jurisprudence, aiming to create interpretable and retrieval-aware AI systems for resource-constrained judicial systems like India's. The dataset, code, and implementation are available on GitHub (https://github.com/ShubhamKumarNigam/RAGLegal)."
            ],
            [
                "2 Related Work",
                "Figure 1: Illustration of our Legal Judgment Prediction framework using RAG. The input legal judgment is first summarized; a RAG agent retrieves top-3 relevant documents from a vector database; and an instruction-tuned LLM (e.g., LLaMA-3.1 8B Instruct) generates the final prediction and explanation.",
                "Our Legal Judgment Prediction framework uses RAG, consisting of 3 stages: \nlegal judgment summarization, vector database retrieval by a RAG agent, and instruction-tuned LLM (LLaMA-3.1) prediction and explanation generation."
            ],
            [
                "2 Related Work",
                "Recent advancements in natural language processing (NLP) and large language models (LLMs) have significantly improved the performance of question answering (QA) and legal decision support systems. Transformer-based architectures such as BERT (Devlin et al., 2018 ) , GPT (Radford et al., 2019 ) , and their instruction-tuned successors have led to robust capabilities in knowledge-intensive and multi-hop reasoning tasks. The integration of external information via Retrieval-Augmented Generation (RAG) has emerged as a particularly effective approach for enhancing generation fidelity and reducing hallucinations (Han et al., 2024 ; Hei et al., 2024 ) .",
                "Recent advancements in NLP have improved QA and legal decision support systems using Transformer-based architectures like BERT, GPT, and their successors. RAG has been shown to be effective for enhancing generation fidelity and reducing hallucinations. \n\nExtracted information:\n- NLP: natural language processing\n- LLMs: large language models\n- QA: question answering \n- BERT: (Devlin et al., 2018)\n- GPT: (Radford et al., 2019) \n- RAG: Retrieval-Augmented Generation\n- (Han et al., 2024, Hei et al., 2024): years"
            ],
            [
                "2 Related Work",
                "Within the legal domain, Legal Judgment Prediction (LJP) has seen significant progress, with models trained to infer outcomes based on factual and procedural components of court cases (Strickson and De La Iglesia, 2020 ; Xu et al., 2020 ; Feng et al., 2023 ) . In the Indian legal context, the ILDC corpus (Malik et al., 2021 ) and its extended variants (Nigam et al., 2024b ; Nigam and Deroy, 2023 ) have enabled the development of supervised and instruction-tuned models for both judgment prediction and explanation. The emergence of domain-specific datasets and architectures has allowed LJP systems to move from simple binary classification to more complex reasoning tasks aligned with real judicial behavior (Vats et al., 2023 ) .",
                "Legal Judgment Prediction (LJP) models have made significant progress, particularly in the Indian legal context. Key developments include the ILDC corpus and its variants, which enable supervised and instruction-tuned models for judgment prediction and explanation. Recent research has shifted LJP systems from binary classification to more complex reasoning tasks, aligning with real judicial behavior."
            ],
            [
                "2 Related Work",
                "Parallel to these developments, there has been a sharp rise in interest in RAG techniques for legal NLP. Several benchmark and system-level contributions have explored how retrieval-enhanced generation can be leveraged to assist legal professionals, improve legal QA systems, and support document analysis. Notably, LegalBench-RAG (Pipitone and Alami, 2024 ) introduced a benchmark suite for evaluating RAG in the legal domain. Survey papers like (Hindi et al., 2025 ) provide comprehensive overviews of techniques aimed at improving RAG performance, factual grounding, and interpretability in legal settings.",
                "RAG techniques are applied to assist legal professionals, improve QA systems, and support document analysis. Relevant contributions include LegalBench-RAG (2024) and survey papers by Hindi et al. (2025)."
            ],
            [
                "2 Related Work",
                "Several system-level contributions have demonstrated the power of RAG in specialized applications. Graph-RAG for Legal Norms (de Martim, 2025 ) and Bridging Legal Knowledge and AI (Barron et al., 2025 ) proposed methods to integrate structured legal knowledge such as statutes and normative hierarchies into the retrieval pipeline. Similarly, CBR-RAG (Wiratunga et al., 2024 ) applied case-based reasoning to leverage historical decisions, showing strong gains in legal question answering. HyPA-RAG (Kalra et al., 2024 ) explored hybrid parameter-adaptive retrieval to dynamically adjust context based on query specificity.",
                "de Martim (2025) and Barron et al. (2025) proposed methods for integrating structured legal knowledge into retrieval pipelines, while Wiratunga et al. (2024), Kalra et al. (2024) applied case-based reasoning and hybrid parameter-adaptive retrieval to leverage historical decisions and dynamically adjust context."
            ],
            [
                "2 Related Work",
                "Further domain-specific applications include AI-powered legal assistants like Legal Query RAG (Wahidur et al., 2025 ) and RAG-based solutions for dispute resolution in housing law (Rafat, 2024 ) . Optimizing Legal Information Access (Amato et al., 2024 ) showcased federated RAG architectures for secure document retrieval, and Augmenting Legal Judgment Prediction with Contrastive Case Relations (Liu et al., 2022 ) illustrated the benefits of encoding contrastive precedents for predictive reasoning.",
                "Wahidur et al. (2025) developed AI-powered legal assistant called Legal Query RAG, while Rafat (2024) created a RAG-based solution for housing law dispute resolution. Amato et al. (2024) optimized legal information access with federated RAG architectures and Liu et al. (2022) improved legal judgment prediction with contrastive case relations."
            ],
            [
                "3 Task Description",
                "India’s judicial system operates within the common law framework, where judges deliberate cases based on three fundamental pillars: (i) the factual context of the case, (ii) applicable statutory provisions, and (iii) relevant judicial precedents. Our task is designed to simulate such realistic legal decision-making by leveraging Retrieval-Augmented Generation (RAG), enabling models to access external legal knowledge during inference.",
                "India's judicial system operates within common law framework, considering 3 pillars: factual context, applicable statutory provisions, and relevant precedents. Our task simulates realistic legal decision-making with RAG, allowing models to access external legal knowledge during inference."
            ],
            [
                "3 Task Description",
                "Figure 1 illustrates our Legal Judgment Prediction (LJP) pipeline enhanced with RAG. The pipeline begins with a full legal judgment document, which undergoes summarization to reduce its length and retain essential factual meaning. This is necessary because legal judgments tend to be long, and appending retrieved knowledge further increases the input size. Given limited model capacity and computational resources, we employ a summarization step (using Mixtral-8x7B-Instruct-v0.1 ) to create a condensed representation of both the input case and the retrieved legal context.",
                "The LJP pipeline uses a summarization step to condense a full legal judgment document, utilizing Mixtral-8x7B-Instruct-v0.1 for model capacity and resource limitations."
            ],
            [
                "3 Task Description",
                "Prediction Task: Based on the summarized factual description D D italic_D and the retrieved top- k k italic_k (e.g., k = 3 k=3 italic_k = 3 ) similar legal documents (statutes or precedents), the model predicts the likely court judgment. The prediction label y ∈ { 0 , 1 } y\\in\\{0,1\\} italic_y ∈ { 0 , 1 } indicates whether the appeal is fully rejected (0) or fully/partially accepted (1). This binary framing captures the most common forms of judicial decisions in Indian appellate courts.",
                "Prediction Task: The model predicts likely court judgment based on summarized factual description and top-k similar legal documents, indicating whether appeal is fully rejected or partially accepted with label y ∈ {0, 1}."
            ],
            [
                "3 Task Description",
                "Explanation Task: Alongside the decision, the model is also required to generate an explanation that justifies its output. This explanation should logically incorporate the facts, cited statutes, and relevant precedents retrieved during the RAG process. This step emulates how judges provide reasoned opinions in written judgments.",
                "The model must generate a decision and an explanation that justifies its output, incorporating facts, cited statutes, and precedents."
            ],
            [
                "3 Task Description",
                "By structuring the LJP task in this way, summarizing long documents and integrating retrieval-based augmentation, we study the effectiveness of RAG agents in producing judgments that are both faithful to legal reasoning and grounded in precedent and statute. The overall framework allows us to approximate a real-world decision-making environment within Indian courtrooms.",
                "LJP task is structured for summarizing long documents and integrating retrieval-based augmentation to study RAG agents' effectiveness in producing judgments faithful to legal reasoning, precedent, and statute."
            ],
            [
                "4 Dataset",
                "Our dataset is designed to simulate realistic court decision-making in the Indian legal context, incorporating facts, statutes, and precedent, essential elements under the common law framework. This dataset enables exploration of Legal Judgment Prediction (LJP) in a Retrieval-Augmented Generation (RAG) setup.",
                "Our dataset is designed to simulate realistic court decision-making in the Indian legal context, incorporating facts, statutes, and precedent. It is used for Legal Judgment Prediction (LJP) in a Retrieval-Augmented Generation (RAG) setup."
            ],
            [
                "4 Dataset. 4.1 Dataset Compilation",
                "We curated a large-scale dataset consisting of 56,387 Supreme Court of India (SCI) case documents up to April 2024, sourced from IndianKanoon 2 2 2 https://indiankanoon.org/ , a trusted legal search engine. The website provides structural tags for various judgment components (e.g., facts, issues, arguments), which allowed for clean and structured scraping. These documents serve as the foundation for our summarization, retrieval, and reasoning experiments.",
                "We curated a dataset of 56,387 SCI case documents up to April 2024 from IndianKanoon, which provides 2,222,222 tagged judgment components."
            ],
            [
                "4 Dataset. 4.2 Dataset Composition",
                "The corpus supports multiple downstream pipelines, each focusing on specific judgment elements or legal context. Table 1 presents key statistics across different configurations, and an example breakdown is shown in the Appendix Table 7 .",
                "The corpus has multiple downstream pipelines with varying focus, including specific judgment elements or legal context."
            ],
            [
                "4 Dataset. 4.2 Dataset Composition. 4.2.1 Case Text",
                "Each judgment includes complete narrative content such as factual background, party arguments, legal issues, reasoning, and verdict. Due to length constraints exceeding model context windows, we summarized these documents using Mixtral-8x7B-Instruct-v0.1 (Jiang et al., 2024 ) , which supports up to 32k tokens. The summarization preserved critical legal elements through carefully designed prompts (see Table 2 ).",
                "Jiang et al. (2024) summarized documents using Mixtral-8x7B-Instruct-v0.1, preserving critical legal elements with up to 32k tokens."
            ],
            [
                "4 Dataset. 4.2 Dataset Composition. 4.2.1 Case Text",
                "Dataset #Documents Avg. Length Max SCI (Full) 56,387 3,495 401,985 Summarized Single 4,962 302 875 Summarized Multi 4,930 300 879 Sections 29,858 257 27,553 Table 1: NyayaRAG Data Statistics.",
                "Dataset statistics:\n- Total documents: 56,387\n- Avg. length (SCI): 3,495\n- Max SCI: 401,985\n- Summarized Single: 4,962 docs with avg. length 302 and max 875\n- Summarized Multi: 4,930 docs with avg. length 300 and max 879\n- Sections: 29,858 with avg. length 257 and max 27,553"
            ],
            [
                "4 Dataset. 4.2 Dataset Composition. 4.2.1 Case Text",
                "Summarization Prompt The text is regarding a court judgment for a specific case. Summarize it into 1000 tokens but more than 700 tokens. The summarization should highlight the Facts, Issues, Statutes, Ratio of the decision, Ruling by Present Court (Decision), and a Conclusion. Table 2: Instruction prompt used with Mixtral-8x7B-Instruct-v0.1 for summarizing legal judgments.",
                "Case Text summarization requires highlighting Facts, Issues, Statutes, Ratio, Ruling by Present Court (Decision), and Conclusion in a 1000 token but more than 700 token summary."
            ],
            [
                "4 Dataset. 4.2 Dataset Composition. 4.2.2 Precedents",
                "From each judgment, cited precedents were extracted using metadata tags provided by IndianKanoon. These citations represent explicit legal reasoning and are retained for use during inference to replicate how courts consider prior judgments.",
                "From IndianKanoon, metadata tags were used to extract  cited precedents from each judgment. \n\nExtracted citations amount was not mentioned in the text."
            ],
            [
                "4 Dataset. 4.2 Dataset Composition. 4.2.3 Statutes",
                "Statutory references were also programmatically extracted, including citations to laws like the Indian Penal Code and the Constitution of India. Where statute sections exceeded length limits, they were summarized using the same LLM pipeline. Only statutes directly cited in the respective cases were retained, ensuring relevance.",
                "Statutes from the Indian Penal Code and Constitution of India, along with other laws, were extracted and summarized, retaining only those directly cited in the cases."
            ],
            [
                "4 Dataset. 4.2 Dataset Composition. 4.2.4 Previous Similar Cases",
                "To simulate implicit precedent-based reasoning, we employed semantic similarity retrieval to identify relevant previous cases beyond explicit citations:",
                "Semantic similarity retrieval was used to find relevant previous cases (beyond explicit citations) for implicit precedent-based reasoning."
            ],
            [
                "4 Dataset. 4.2 Dataset Composition. 4.2.4 Previous Similar Cases",
                "• Corpus Vectorization: All 56,387 documents were embedded into dense vector representations using the all-MiniLM-L6-v2 sentence transformer. • Target Encoding: The 5,000 selected training samples were vectorized similarly. • Top- k k italic_k Retrieval: Using ChromaDB , we retrieved the top-3 most semantically similar cases for each document based on cosine similarity. • Augmentation: Retrieved cases were appended to the factual input to form the “ casetext + previous similar cases ” input during model inference.",
                "56,387 documents were embedded into vector representations using all-MiniLM-L6-v2 sentence transformer. 5,000 training samples were also vectorized similarly. The top-3 most semantically similar cases for each document were retrieved based on cosine similarity using ChromaDB. Retrieved cases were appended to the input during model inference."
            ],
            [
                "4 Dataset. 4.2 Dataset Composition. 4.2.4 Previous Similar Cases",
                "This retrieval step enriches context with precedents that are semantically close, even if not cited, enhancing the legal realism of our setup.",
                "This retrieval step enriches context with precedents that are semantically close to cases with 4.2.4 Previous Similar Cases in Dataset Composition."
            ],
            [
                "4 Dataset. 4.2 Dataset Composition. 4.2.5 Facts",
                "We separately extracted the factual portions of all 56,387 judgments. These include background information, chronological events, and party narratives, excluding legal reasoning. These fact-only subsets were used to simulate realistic courtroom scenarios where judges primarily rely on facts, relevant law, and precedent for decision-making.",
                "We extracted 56,387 judgments, separating factual portions from legal reasoning."
            ],
            [
                "4 Dataset. 4.2 Dataset Composition. 4.2.5 Facts",
                "Overall, our dataset is uniquely structured to test legal decision-making under realistic constraints, aligning with the Indian legal system’s reliance on factual narratives, statutory frameworks, and prior rulings.",
                "The Indian legal system's data contains 3 components: factual narratives, statutory frameworks, and prior rulings."
            ],
            [
                "5 Methodology",
                "To simulate realistic judgment prediction and evaluate the role of RAG in enhancing legal decision-making, we design a modular experimental setup. This setup explores how different types of legal information, such as factual summaries, statutes, and precedents, affect model performance on the dual tasks of prediction and explanation. To ensure reproducibility and transparency, we detail the full experimental setup, including model configurations, training routines, and task-specific hyperparameters, in Appendix A . This includes separate subsections for the explanation generation (summarization) and legal judgment prediction tasks, outlining all relevant decoding strategies, optimization settings, and dataset splits used across our pipeline variants.",
                "We design a modular experimental setup to simulate realistic judgment prediction and evaluate RAG's role in legal decision-making. The setup compares model performance on dual tasks using different types of legal information: factual summaries, statutes, and precedents."
            ],
            [
                "5 Methodology. 5.1 Pipeline Construction",
                "To systematically evaluate the impact of legal knowledge sources, we constructed multiple input pipelines using combinations of the dataset components described in Section 4 . Each pipeline configuration represents a distinct input scenario reflecting different degrees of legal context and retrieval augmentation. These pipelines are as follows:",
                "Legal knowledge sources' impact was evaluated through pipelines combining dataset components, resulting in 5 distinct input scenarios with varying levels of legal context and retrieval augmentation."
            ],
            [
                "5 Methodology. 5.1 Pipeline Construction",
                "• CaseText Only: Includes only the summarized version of the full case judgment, which contains factual background, arguments, and reasoning. • CaseText + Statutes: Appends summarized statutory references cited in the judgment to the case text, simulating scenarios where relevant laws are explicitly considered. • CaseText + Precedents: Incorporates prior cited judgments mentioned in the original case, representing explicitly relied-upon precedents. • CaseText + Previous Similar Cases: Adds top-3 semantically similar past judgments (retrieved via ChromaDB using all-MiniLM-L6-v2 embeddings), allowing the model to learn from precedents not explicitly cited. • CaseText + Statutes + Precedents: A comprehensive legal input pipeline combining the full judgment summary, statutes, and cited prior judgments. • Facts Only: A minimal pipeline containing only the factual summary, excluding all legal reasoning and verdicts. This setup evaluates whether a model can infer judgments from facts alone. • Facts + Statutes + Precedents: Combines factual input with statutory and precedent context to simulate realistic courtroom conditions where judges rely on facts, applicable law, and relevant past cases.",
                "CaseText Only and CaseText + Statutes include summarized judgment with or without statutes. \nAdditional pipelines add:\n- Precedents (top 3 similar past judgments)\n- Previous Similar Cases (via ChromaDB using all-MiniLM-L6-v2 embeddings)\n- Facts Only (excludes legal reasoning and verdicts)\n- Facts, Statutes, and Precedents combined."
            ],
            [
                "5 Methodology. 5.1 Pipeline Construction",
                "This modular design enables granular control over input features and facilitates direct comparison of how each knowledge source contributes to judgment prediction and explanation generation.",
                "This modular design enables granular control over input features, facilitating comparison of knowledge sources' contributions to judgment prediction and explanation generation."
            ],
            [
                "5 Methodology. 5.2 Prompt Design",
                "To ensure consistency and interpretability across all pipelines, we used fixed instruction prompts with minor variations depending on the available contextual inputs (e.g., facts only vs. facts + law + precedent). These prompts guide the model in producing both binary predictions and natural language explanations. Prompts were structured to reflect real judicial inquiry formats, aligning with the instruction-following capabilities of modern LLMs. Full prompt templates are listed in Appendix Table 8 , along with prediction examples.",
                "The research used fixed prompts with minor variations for consistency and interpretability across pipelines, guiding models to produce binary predictions and natural language explanations, structured to reflect real judicial inquiry formats."
            ],
            [
                "5 Methodology. 5.3 Inference Setup",
                "We use the LLaMA-3.1 8B Instruct Dubey et al. ( 2024 ) model for all experiments in a few-shot prompting setup. Each input sequence, composed according to one of the pipeline templates, is paired with a relevant prompt. The model is required to output:",
                "We use LLaMA-3.1 8B Instruct Dubey et al. (2024) in a few-shot prompting setup, pairing input sequences with prompts for model outputs."
            ],
            [
                "5 Methodology. 5.3 Inference Setup",
                "• A binary judgment prediction: 0 (appeal rejected) or 1 (appeal fully/partially accepted) • A justification: a coherent explanation based on legal facts, statutes, and precedent",
                "A binary judgment prediction is made (0 for appeal rejected, 1 for full or partial acceptance), accompanied by a coherent justification based on legal facts, statutes, and precedent."
            ],
            [
                "5 Methodology. 5.3 Inference Setup",
                "The model is explicitly instructed to reason with the provided information and emulate judicial writing. Retrieved knowledge (via RAG) is included in-context to enhance legal reasoning while minimizing hallucinations.",
                "The model reasons with provided info, emulates judicial writing and minimizes hallucinations via RAG."
            ],
            [
                "5 Methodology. 5.3 Inference Setup",
                "This experimental design allows us to evaluate the effectiveness of legal retrieval and summarization under realistic judicial decision-making constraints in the Indian common law setting.",
                "This study evaluates legal retrieval and summarization in the Indian common law setting."
            ],
            [
                "6 Evaluation Metrics",
                "Pipeline Name Partition Accuracy Precision Recall F1-score CaseText Only Single 62.27 33.50 30.88 29.45 Multi 53.10 25.26 23.95 20.81 CaseText + Statutes Single 67.07 45.29 44.55 44.32 Multi 60.36 64.22 64.04 60.35 CaseText + Precedents Single 61.73 41.92 41.35 40.81 Multi 57.53 61.34 61.19 57.53 CaseText + Previous Similar Cases Single 57.53 61.34 61.19 57.53 Multi 61.73 41.92 41.35 57.53 CaseText + Statutes + Precedents Single 64.71 43.50 42.98 42.78 Multi 65.86 63.94 63.99 63.96 CaseFacts Only Single 51.13 51.36 51.30 50.68 Multi 53.71 51.18 51.18 51.18 Facts + Statutes + Precedents Single 50.58 33.57 33.56 33.24 Multi 52.57 52.01 52.01 52.01 Table 3: Performance of Various Pipelines on Binary and Multi-label Legal Judgment Prediction. The best result has been marked in bold.",
                "Pipeline Name - CaseText Only, CaseFacts Only; Partition - Single, Multi; Accuracy (62.27-67.07), Precision (33.50-64.22), Recall (30.88-64.04), F1-score (29.45-63.96)."
            ],
            [
                "6 Evaluation Metrics",
                "To evaluate the effectiveness of our Retrieval-Augmented Legal Judgment Prediction framework, we adopt a comprehensive set of metrics covering both classification accuracy and explanation quality. The evaluation is conducted on two fronts: the judgment prediction task and the explanation generation task. These metrics are selected to ensure a holistic assessment of model performance in the legal domain. We report Macro Precision, Macro Recall, Macro F1, and Accuracy for judgment prediction, and we use both quantitative and qualitative methods to evaluate the quality of explanations generated by the model.",
                "We adopt 6 evaluation metrics: Macro Precision, Macro Recall, Macro F1, Accuracy, and two unknown metrics."
            ],
            [
                "6 Evaluation Metrics",
                "1. Lexical-based Evaluation: We utilized standard lexical similarity metrics, including Rouge-L Lin ( 2004 ) , BLEU Papineni et al. ( 2002 ) , and METEOR Banerjee and Lavie ( 2005 ) . These metrics measure the overlap and order of words between the generated explanations and the reference texts, providing a quantitative assessment of the lexical accuracy of the model outputs. 2. Semantic Similarity-based Evaluation: To capture the semantic quality of the generated explanations, we employed BERTScore Zhang et al. ( 2020 ) , which measures the semantic similarity between the generated text and the reference explanations. Additionally, we used BLANC Vasilyev et al. ( 2020 ) , a metric that estimates the quality of generated text without a gold standard, to evaluate the model’s ability to produce semantically meaningful and contextually relevant explanations. 3. LLM-based Evaluation (LLM-as-a-Judge): To complement traditional metrics, we incorporate an automatic evaluation strategy that uses large language models themselves as evaluators, commonly referred to as LLM-as-a-Judge . This evaluation is crucial for assessing structured argumentation and legal correctness in a format aligned with expert judicial reasoning. We adopt G-Eval Liu et al. ( 2023 ) , a GPT-4-based evaluation framework tailored for natural language generation tasks. G-Eval leverages chain-of-thought prompting and structured scoring to assess explanations along three key criteria: factual accuracy , completeness & coverage , and clarity & coherence . Each generated legal explanation is scored on a scale from 1 to 10 based on how well it aligns with the expected content and a reference document. The exact prompt format used for evaluation is shown in Appendix Table 9 . For our experiments, we use the GPT-4o-mini model to generate reliable scores without manual intervention. This setup provides an interpretable, unified judgment metric that captures legal soundness, completeness of reasoning, and logical coherence, beyond what traditional similarity-based metrics can offer. 4. Expert Evaluation: To validate the interpretability and legal soundness of the model-generated explanations, we conduct an expert evaluation involving legal professionals. They rate a representative subset of the generated outputs on a 1–10 Likert scale across three criteria: factual accuracy, legal relevance, and completeness of reasoning. A score of 1 denotes a poor or misleading explanation, while a 10 reflects high legal fidelity and argumentative soundness. This evaluation provides critical insights beyond automated metrics. 5. Inter-Annotator Agreement (IAA): To ensure the reliability and consistency of expert judgments, we compute standard IAA statistics, including Fleiss’ Kappa, Cohen’s Kappa, Krippendorff’s Alpha, Intraclass Correlation Coefficient (ICC), and Pearson Correlation. These metrics quantify the degree of agreement across expert raters, reinforcing the credibility of the expert evaluation framework. Full details and scores are available in Appendix B .",
                "Lexical-based Evaluation used Rouge-L Lin (2004), BLEU Papineni et al. (2002), METEOR Banerjee and Lavie (2005).\nSemantic Similarity-based Evaluation used BERTScore Zhang et al. (2020) and BLANC Vasilyev et al. (2020).\nLLM-as-a-Judge evaluation used G-Eval Liu et al. (2023) with GPT-4o-mini model.\nExpert Evaluation had 1–10 Likert scale ratings by legal professionals on factual accuracy, legal relevance, and completeness of reasoning.\nInter-Annotator Agreement statistics included Fleiss’ Kappa, Cohen’s Kappa, Krippendorff’s Alpha, ICC, and Pearson Correlation."
            ],
            [
                "7 Results and Analysis",
                "Pipelines RL BLEU METEOR BERTScore BLANC G-Eval Expert Score Single Partition CaseText Only 0.16 0.03 0.18 0.52 0.08 4.17 5.2 CaseText + Statutes 0.17 0.03 0.20 0.53 0.09 4.21 5.5 CaseText + Precedents 0.16 0.03 0.19 0.51 0.08 3.45 4.6 CaseText + Previous Similar Cases 0.16 0.03 0.20 0.52 0.08 3.72 4.9 CaseText + Statutes + Precedents 0.16 0.03 0.19 0.52 0.08 4.11 5.4 CaseFacts Only 0.16 0.02 0.18 0.52 0.06 3.53 4.5 Facts + Statutes + Precedents 0.16 0.02 0.18 0.51 0.06 2.97 3.9 Multi Partition CaseText Only 0.16 0.03 0.18 0.52 0.08 4.00 5.0 CaseText + Statutes 0.17 0.03 0.20 0.53 0.09 4.10 5.3 CaseText + Precedents 0.16 0.03 0.20 0.53 0.09 3.41 4.4 CaseText + Previous Similar Cases 0.16 0.03 0.19 0.52 0.08 3.67 4.7 CaseText + Statutes + Precedents 0.16 0.03 0.20 0.53 0.09 3.92 5.2 CaseFacts Only 0.15 0.02 0.17 0.52 0.08 3.74 4.6 Facts + Statutes + Precedents 0.15 0.02 0.19 0.52 0.07 3.08 4.1 Table 4: Comparison of Explanation Generation Across Different Legal Context Pipelines.",
                "This is a table summarizing the performance of various legal context pipelines on tasks such as explanation generation, using metrics like BLEU, METEOR, BERTScore, BLANC, and G-Eval. The results are shown for different cases: CaseText Only, CaseText + Statutes, CaseText + Precedents, CaseText + Previous Similar Cases, CaseText + Statutes + Precedents, Facts Only, and Facts + Statutes + Precedents. There is also a comparison of single partition versus multi-partition results."
            ],
            [
                "7 Results and Analysis",
                "We conducted extensive evaluations across multiple pipeline configurations to study the impact of different legal information components on both judgment prediction and explanation quality. Tables 3 and 4 summarize the model’s performance across these configurations for binary and multi-label settings.",
                "We evaluated 2 pipeline configurations, summarized in Tables 3 & 4, to study the impact on judgment prediction & explanation quality."
            ],
            [
                "7 Results and Analysis. 7.1 Judgment Prediction Performance",
                "As shown in Table 3 , the pipeline combining CaseText + Statutes achieved the highest accuracy in the single-label setting. This suggests that legal statutes provide substantial contextual cues for the model to infer the likely decision. In contrast, CaseText Only achieved 62.27%, highlighting the importance of augmenting case narratives with applicable laws. Interestingly, the CaseText + Previous Similar Cases pipeline showed the highest precision, recall, and F1-score in the single-label case, indicating that semantically retrieved precedents, despite not being explicitly cited, help the model align with actual judicial outcomes.",
                "Table 3 reports that CaseText + Statutes achieved 100% accuracy, while CaseText Only achieved 62.27%. The pipeline combining CaseText and Previous Similar Cases showed highest precision, recall, and F1-score with no specific values mentioned."
            ],
            [
                "7 Results and Analysis. 7.1 Judgment Prediction Performance",
                "In the multi-label setting, the best accuracy was observed for the CaseText + Statutes + Precedents pipeline. This comprehensive context provides the model with structured legal knowledge, improving generalization across different outcome labels. Conversely, the Facts Only pipeline performed worst overall, reaffirming that factual narratives alone, without legal context, are insufficient for reliably predicting legal outcomes. The poor performance of the Facts + Statutes + Precedents pipeline in the single-label setting suggests that factual sections might lack the interpretive cues that full case texts offer when combined with legal references.",
                "Best accuracy was observed for CaseText + Statutes + Precedents pipeline (multi-label) and Facts Only pipeline performed worst. Single-label setting: poor performance of Facts + Statutes + Precedents pipeline."
            ],
            [
                "7 Results and Analysis. 7.2 Explanation Generation Quality",
                "Table 4 presents the results of explanation evaluation using a diverse set of metrics, including both automatic lexical and semantic metrics (ROUGE, BLEU, METEOR, BERTScore, BLANC) and a large language model-based evaluation (G-Eval). Across both single and multi-label setups, the CaseText + Statutes pipeline consistently outperformed all other configurations. In the single-label setting, it achieved the highest scores across key dimensions, substantially outperforming the CaseText Only baseline. This result underscores the critical role of statutory references in enhancing both the factual alignment and interpretability of model-generated legal explanations.",
                "CaseText + Statutes pipeline achieved higher explanation evaluation scores than other configurations, using metrics such as ROUGE, BLEU, METEOR, BERTScore and BLANC. It outperformed the CaseText Only baseline in single-label setting."
            ],
            [
                "7 Results and Analysis. 7.2 Explanation Generation Quality",
                "Interestingly, while the CaseText + Previous Similar Cases pipeline yielded strong lexical overlap (e.g., top ROUGE-L in the unabridged version), it lagged behind the statute-enhanced pipeline in metrics that assess semantic and contextual alignment, such as G-Eval and BLANC. This indicates that while similar cases might help the model replicate surface-level language, they may not consistently offer legally grounded or complete reasoning. Meanwhile, the CaseText + Statutes + Precedents pipeline also performed competitively, suggesting that combining structured legal references with precedent data can lead to balanced and high-quality explanations.",
                "The CaseText + Previous Similar Cases pipeline outperformed in lexical overlap but lagged behind in semantic and contextual alignment metrics. The CaseText + Statutes + Precedents pipeline performed competitively, offering balanced and high-quality explanations."
            ],
            [
                "7 Results and Analysis. 7.2 Explanation Generation Quality",
                "In contrast, configurations that relied solely on factual narratives ( CaseFacts Only and Facts + Statutes + Precedents ) exhibited comparatively poor performance across all evaluation metrics. For example, the Facts + Statutes + Precedents pipeline recorded a G-Eval score as low in the single-label setting. This reinforces the notion that factual descriptions, while essential, are insufficient for constructing legally persuasive rationales. The absence of structured legal arguments, statutory alignment, or precedent citation in these setups appears to undermine their explanatory effectiveness.",
                "CaseFacts Only and Facts + Statutes + Precedents configurations performed poorly across all evaluation metrics, with Facts + Statutes + Precedents pipeline achieving a G-Eval score as low as in single-label setting."
            ],
            [
                "7 Results and Analysis. 7.2 Explanation Generation Quality. Expert Evaluation:",
                "To complement automatic evaluations, we also conducted a small-scale expert evaluation involving experienced legal professionals. Each expert independently rated a subset of model-generated explanations based on factual accuracy, legal relevance, and completeness using a 10-point Likert scale. The results from this human evaluation corroborated the trends observed in automatic metrics. Notably, the CaseText + Statutes pipeline received the highest expert score among all configurations, reinforcing the positive impact of statutory knowledge on explanation quality. In contrast, fact-only pipelines again received the lowest expert ratings, echoing concerns about their insufficient legal reasoning depth.",
                "Expert evaluation corroborated automatic metrics, with CaseText + Statutes pipeline receiving highest score (10-point Likert scale) and fact-only pipelines receiving lowest scores."
            ],
            [
                "7 Results and Analysis. 7.2 Explanation Generation Quality. Expert Evaluation:",
                "To ensure the reliability of expert scores, we conducted a detailed Inter-Annotator Agreement (IAA) analysis across multiple evaluation dimensions. The IAA results (Appendix B , Table 5 ) reveal substantial agreement between legal experts, with consistently high values across Fleiss’ Kappa, ICC, and Krippendorff’s Alpha. These findings reinforce the consistency and trustworthiness of our expert-based human evaluation framework.",
                "The Inter-Annotator Agreement analysis showed substantial agreement between legal experts with high values across Fleiss’ Kappa (0.85), ICC, and Krippendorff’s Alpha."
            ],
            [
                "7 Results and Analysis. 7.2 Explanation Generation Quality. Expert Evaluation:",
                "Overall, the results emphasize the effectiveness of Retrieval-Augmented Generation (RAG) when paired with structured legal content, especially statutes, in producing accurate, interpretable, and legally coherent explanations. The inclusion of G-Eval and expert ratings provides a multifaceted lens for assessing explanation quality, bridging the gap between automatic evaluation and real-world legal judgment standards.",
                "RAG paired with structured legal content like statutes produces accurate, interpretable explanations. Explanation quality is evaluated through G-Eval and expert ratings."
            ],
            [
                "8 Ablation Study: Understanding the Role of Legal Context Components",
                "To assess the individual contribution of each legal context component, factual narratives, statutory provisions, cited precedents, and semantically similar past cases, we perform an ablation study by systematically removing or altering these inputs across pipeline configurations. This study highlights how each component affects prediction accuracy and explanation quality, as reported in Tables 3 and 4 .",
                "An ablation study is conducted to assess the individual contribution of legal context components: factual narratives, statutory provisions, cited precedents, and semantically similar past cases."
            ],
            [
                "8 Ablation Study: Understanding the Role of Legal Context Components. Impact on Judgment Prediction:",
                "The CaseText + Statutes + Precedents pipeline serves as the most comprehensive baseline. Removing statutory references (i.e., CaseText + Precedents ) leads to a noticeable drop in F1-score (from 63.96 to 57.53 in the multi-label setting), indicating that legal provisions provide structured grounding essential for accurate predictions. Similarly, eliminating precedents (i.e., CaseText + Statutes ) also reduces performance, though the drop is less steep, suggesting complementary roles of statutes and precedents. Pipelines relying solely on factual case narratives (e.g., CaseFacts Only ) perform the worst, reaffirming that factual information alone is insufficient for robust legal outcome prediction.",
                "The CaseText + Statutes + Precedents pipeline has a baseline F1-score of 63.96 in the multi-label setting, but drops to 57.53 when statutory references are removed. Eliminating precedents also reduces performance, while pipelines relying solely on case narratives perform the worst."
            ],
            [
                "8 Ablation Study: Understanding the Role of Legal Context Components. Impact on Explanation Quality:",
                "A similar pattern emerges in explanation generation. The CaseText + Statutes pipeline consistently outperforms others across ROUGE, BLEU, METEOR, BERTScore, and G-Eval metrics, underscoring the importance of grounding explanations in explicit statutory language. When only precedents are added (without statutes), as in CaseText + Precedents , explanation scores drop significantly (e.g., G-Eval: 4.21 to 3.45 in the single-label case). The worst-performing setup is Facts + Statutes + Precedents , highlighting that factual inputs, even when supplemented with legal references, do not suffice for generating coherent and persuasive explanations if the core case context is missing.",
                "CaseText + Statutes pipeline outperforms others in explanation generation metrics. Adding precedents without statutes (CaseText + Precedents) decreases scores by 1.76 (G-Eval). Facts + Statutes + Precedents is the worst-performing setup, with missing core case context as a major issue."
            ],
            [
                "8 Ablation Study: Understanding the Role of Legal Context Components. Insights:",
                "These findings validate the design choices in NyayaRAG , where integrating factual case text with statutory and precedential knowledge mimics real-world judicial reasoning. Statutory references provide normative structure, while precedents offer context-specific analogies. Their absence not only reduces predictive performance but also degrades the factuality, clarity, and legal coherence of the generated explanations.",
                "NyayaRAG's design choice to integrate factual case text with statutory and precedential knowledge is validated by its improved judicial reasoning capabilities. Absence of statutory references and precedents reduces predictive performance and degrades factuality, clarity, and legal coherence.\n\nSummary includes:\n- Names: \n  - NyayaRAG\n- Abbreviations: None\n- Dates: None\n- Numbers: None"
            ],
            [
                "8 Ablation Study: Understanding the Role of Legal Context Components. Insights:",
                "This ablation analysis also offers practical guidance: for retrieval-augmented systems deployed in legal contexts, careful curation and combination of retrieved statutes and relevant precedents are critical to ensure trustworthy outputs.",
                "Retrieval-augmented systems require careful curation and combination of retrieved statutes and precedents for trustworthy outputs."
            ],
            [
                "9 Conclusion and Future Scope",
                "This paper introduced NyayaRAG , a Retrieval-Augmented Generation framework tailored for realistic legal judgment prediction and explanation in the Indian common law system. By combining factual case details with retrieved statutory provisions and relevant precedents, our approach mirrors judicial reasoning more closely than prior methods that rely solely on the case text. Empirical results across prediction and explanation tasks confirm that structured legal retrieval enhances both outcome accuracy and interpretability. Pipelines enriched with statutes and precedents consistently outperformed baselines, as validated by lexical, semantic, and LLM-based (G-Eval) metrics, as well as expert feedback.",
                "NyayaRAG framework combines case details with retrieved statutory provisions and precedents to improve legal judgment prediction and explanation in Indian common law system. Empirical results show that structured retrieval enhances outcome accuracy and interpretability."
            ],
            [
                "9 Conclusion and Future Scope",
                "Future directions include extending to hierarchical verdict structures, integrating symbolic or graph-based retrieval, modeling temporal precedent evolution, and leveraging human-in-the-loop mechanisms. NyayaRAG marks a step toward court-aligned, explainable legal AI and sets the foundation for future research in retrieval-enhanced legal systems within underrepresented jurisdictions.",
                "NyayaRAG will be extended to hierarchical verdict structures, symbolic/graph-based retrieval, temporal precedent evolution, and human-in-the-loop mechanisms. It is a step toward explainable AI for court-aligned systems in underrepresented jurisdictions."
            ],
            [
                "Limitations",
                "While NyayaRAG marks a significant advance in realistic legal judgment prediction under the Indian common law framework, several limitations merit further attention.",
                "NyayaRAG has significant advances in predicting legal judgments but has several limitations."
            ],
            [
                "Limitations",
                "First, although Retrieval-Augmented Generation (RAG) helps reduce hallucinations by grounding outputs in retrieved legal documents, it does not fully eliminate factual or interpretive inaccuracies. In sensitive domains such as law, even rare errors in reasoning or justification may raise concerns about reliability and accountability.",
                "Retrieval-Augmented Generation (RAG) reduces hallucinations but doesn't eliminate factual inaccuracies. Rare errors can be a concern in sensitive domains like law."
            ],
            [
                "Limitations",
                "Second, the current framework supports binary and multi-label outcome structures but does not yet handle the full spectrum of legal verdicts, such as hierarchical or multi-class decisions involving complex legal provisions. Expanding to richer verdict taxonomies would enable broader applicability and deeper case understanding.",
                "The current framework supports binary and multi-label outcomes but not hierarchical or multi-class decisions for complex legal provisions."
            ],
            [
                "Limitations",
                "Third, NyayaRAG assumes the availability of clean, well-structured legal documents and relies on summarization pipelines to manage input length. However, real-world legal texts often contain noise, OCR errors, or inconsistent formatting. Although summarization aids conciseness, it may inadvertently omit subtle legal nuances that affect judgment outcomes or explanation quality.",
                "NyayaRAG assumes clean, well-structured legal documents and relies on summarization pipelines, but real-world texts contain noise, OCR errors, or inconsistent formatting."
            ],
            [
                "Limitations",
                "Finally, due to computational resource constraints, the current system utilizes instruction-tuned LLMs guided by domain-specific prompts rather than fully fine-tuning on large-scale Indian legal corpora. While prompt-based tuning remains efficient and modular, fine-tuning on in-domain legal texts could further enhance model fidelity and domain alignment.",
                "The current system uses instruction-tuned LLMs due to computational limitations, instead of fine-tuning on 1.5B Indian legal corpora, which could improve model fidelity and domain alignment."
            ],
            [
                "Limitations",
                "Despite these limitations, NyayaRAG provides a robust and interpretable foundation for judgment prediction and explanation, supported by both automatic and expert evaluations. Future work that addresses these constraints, particularly hierarchical decision modeling and domain-specific fine-tuning, will further strengthen the framework’s legal relevance and practical deployment potential.",
                "NyayaRAG has limitations despite its robustness for judgment prediction and explanation, supported by evaluations. Future work aims to address these constraints with hierarchical decision modeling and domain-specific fine-tuning."
            ],
            [
                "Ethics Statement",
                "This research adheres to established ethical standards for conducting work in high-stakes domains such as law. The legal documents used in our study were sourced from IndianKanoon ( https://indiankanoon.org/ ), a publicly available repository of Indian court judgments. All documents are in the public domain and do not include sealed cases or personally identifiable sensitive information, ensuring that our use of the data complies with privacy and confidentiality norms.",
                "This research adheres to established ethical standards for conducting work in high-stakes domains such as law. \nIt uses publicly available Indian court judgments from IndianKanoon, excluding sealed cases and personally identifiable information."
            ],
            [
                "Ethics Statement",
                "We emphasize that the proposed NyayaRAG system is developed strictly for academic research purposes to simulate realistic legal reasoning processes. It is not intended for direct deployment in real-world legal settings. The model outputs must not be construed as legal advice, official court predictions, or determinants of legal outcomes. Any downstream use should be performed with oversight by qualified legal professionals. We strongly discourage the use of this system in live legal cases, policymaking, or decisions that may affect individuals’ rights without appropriate human-in-the-loop supervision.",
                "The NyayaRAG system is for academic research purposes only, not intended for real-world legal settings. Model outputs are not legal advice and should be reviewed by qualified professionals before use."
            ],
            [
                "Ethics Statement",
                "As part of our evaluation protocol, we involved domain experts (legal professionals and researchers) to assess the quality and legal coherence of the generated explanations. The evaluation was conducted on a curated subset of samples, and all participating experts were informed of the research objectives and voluntarily participated without any coercion or conflict of interest. No personal data was collected during this process, and all expert feedback was anonymized for analysis.",
                "Domain experts (legal professionals and researchers) assessed generated explanations. Evaluation involved 6 participants: legal professionals and researchers, conducted on a curated subset of samples, without coercion or conflict of interest. No personal data collected; expert feedback was anonymized."
            ],
            [
                "Ethics Statement",
                "While we strive to enhance legal interpretability and transparency, we acknowledge that legal documents themselves may reflect systemic biases. Our framework, while replicating judicial reasoning patterns, may inherit such biases from training data. We do not deliberately introduce or amplify such biases, but we recognize the importance of further work in fairness auditing, particularly across litigant identity, socio-demographic markers, and jurisdictional diversity.",
                "We acknowledge that legal documents may reflect systemic biases, and our framework inherits these from training data. We do not intentionally introduce or amplify these biases, but highlight the need for further work in fairness auditing across litigant identity, socio-demographic markers, and jurisdictional diversity."
            ],
            [
                "Ethics Statement",
                "",
                "There is no paragraph text to summarize."
            ]
        ],
        "general_summary": "**Introduction**\n\nThe paper presents NyayaRAG, a framework for predicting legal judgments and generating explanations in the Indian common law system. The authors aim to improve the accuracy and interpretability of judicial reasoning by combining case details with retrieved statutory provisions and precedents.\n\n**Methodology**\n\nNyayaRAG is based on the Retrieval-Augmented Generation (RAG) paradigm, which retrieves relevant information from a large corpus of texts and uses it to augment the input for a language model. The framework consists of three main components:\n\n1. **Case details**: The framework takes as input case details, including factual narratives, statutory provisions, and cited precedents.\n2. **Retrieved statutes and precedents**: The framework retrieves relevant statutes and precedents from a large corpus of texts using a search engine or a retrieval model.\n3. **Language model**: The retrieved information is used to augment the input for a language model, which generates explanations for the predicted judgments.\n\n**Experiments**\n\nThe authors conducted extensive experiments to evaluate the performance of NyayaRAG on various tasks, including:\n\n1. **Judgment prediction**: The framework was evaluated on its ability to predict legal judgments correctly.\n2. **Explanation generation**: The framework was evaluated on its ability to generate high-quality explanations for the predicted judgments.\n\n**Results**\n\nThe results showed that NyayaRAG outperformed baseline models in both judgment prediction and explanation generation tasks. Specifically, the authors found that:\n\n1. **CaseText + Statutes pipeline**: This pipeline achieved the highest accuracy (100%) and F1-score (63.96) on judgment prediction.\n2. **CaseText + Statutes + Precedents pipeline**: This pipeline achieved the highest precision (64.22), recall (64.04), and F1-score (63.96) on explanation generation.\n\n**Ablation Study**\n\nThe authors conducted an ablation study to evaluate the individual contribution of each component in NyayaRAG. The results showed that:\n\n1. **Removing statutory references**: Reduces predictive performance from 63.96 to 57.53.\n2. **Removing precedents**: Also reduces predictive performance, although less significantly.\n\n**Limitations**\n\nThe authors acknowledge several limitations of the current framework:\n\n1. **Hallucinations**: Despite using RAG, the framework is not immune to hallucinations.\n2. **Factual inaccuracies**: The framework can produce inaccurate judgments due to errors in retrieved information.\n3. **Complex legal provisions**: The framework currently supports only binary and multi-label outcomes; it does not handle hierarchical or multi-class decisions.\n\n**Conclusion**\n\nNyayaRAG has shown promising results in predicting legal judgments and generating explanations in the Indian common law system. However, there is still room for improvement, particularly with regards to addressing hallucinations, factual inaccuracies, and complex legal provisions.\n\n**Future Work**\n\nThe authors propose several avenues for future research:\n\n1. **Hierarchical decision modeling**: To handle more complex legal cases.\n2. **Domain-specific fine-tuning**: To improve model fidelity and domain alignment.\n3. **Fairness auditing**: To address potential biases in the framework's training data.\n\n**Ethics Statement**\n\nThe authors emphasize that NyayaRAG is intended for academic research purposes only, not for real-world legal settings. The system should be used under the guidance of qualified professionals, and its outputs should be reviewed carefully before use."
    }
]