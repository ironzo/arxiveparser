[
    {
        "id": "2508.00709v1",
        "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian\n  Common Law System",
        "authors": [
            "Shubham Kumar Nigam",
            "Balaramamahanthi Deepak Patnaik",
            "Shivam Mishra",
            "Ajay Varghese Thomas",
            "Noel Shallum",
            "Kripabandhu Ghosh",
            "Arnab Bhattacharya"
        ],
        "Abstract": "Abstract Legal Judgment Prediction (LJP) has emerged as a key area in AI for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. While previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. In this work, we propose NyayaRAG , a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. NyayaRAG evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the Indian legal system. We assess performance across various input configurations using both standard lexical and semantic metrics as well as LLM-based evaluators such as G-Eval. Our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality.",
        "1 Introduction": [
            "The application of artificial intelligence (AI) in legal judgment prediction (LJP) has the potential to transform legal systems by improving efficiency, transparency, and access to justice. This is particularly crucial for India, where millions of cases remain pending in courts, and decision-making is inherently dependent on factual narratives, statutory interpretation, and judicial precedent. India follows a common law system, where prior decisions (precedents) and statutory provisions play a central role in influencing legal outcomes. However, most existing AI-based LJP systems do not adequately replicate this fundamental feature of judicial reasoning.",
            "Previous studies such as Malik et al. ( 2021 ); Nigam et al. ( 2024b , 2025a ) have focused on predicting legal outcomes using the current case document, including sections like facts, arguments, issues, reasoning, and decision. More recent efforts have narrowed the scope to factual inputs alone (Nigam et al., 2024a , 2025b ) , yet these systems still operate in a vacuum, without considering how courts naturally rely on applicable laws and prior rulings. In reality, judges rarely decide in isolation; instead, they actively refer to relevant precedent and statutory law. To bridge this gap, we propose a framework that more closely mirrors actual courtroom conditions by explicitly incorporating external legal knowledge during inference.",
            "Moreover, in critical domains like finance, medicine, and law, decisions must be grounded in verifiable information. Experts in these domains cannot rely on opaque, black-box inferences, and they require systems that ensure factual consistency. Hallucinations, common in large generative models, can have severe consequences in legal decision-making. By retrieving and conditioning model responses on grounded sources such as applicable laws and precedent cases, Retrieval-Augmented Generation (RAG) offers a principled approach to mitigate hallucination and promote trustworthy outputs. Furthermore, RAG frameworks like ours can be flexibly integrated into existing legal systems without requiring the retraining of core models or the sharing of private or sensitive case data. This enhances user trust while allowing the legal community to benefit from AI without sacrificing transparency or data confidentiality.",
            "We introduce NyayaRAG , a Retrieval-Augmented Generation (RAG) framework for realistic legal judgment prediction and explanation in the Indian common law system. The term “NyayaRAG” is derived from two components: “ Nyaya ” meaning “justice” and “ RAG ” referring to “Retrieval-Augmented Generation”. Together, the name reflects our vision to build a justice-aware generation system that emulates the reasoning process followed by Indian courts, using facts, statutes, and precedents.",
            "Unlike prior models that operate purely on internal case content, NyayaRAG simulates real-world judicial decision-making by providing the model with: (i) the summarized factual background of the current case, (ii) relevant statutory provisions, (iii) top- k semantically retrieved previous similar judgments. This structure emulates how judges deliberate on new cases, consulting both textual statutes and prior judicial opinions. Through this design, we evaluate how Retrieval-Augmented Generation can help reduce hallucinations, promote faithfulness, and yield legally coherent predictions and explanations.",
            "Our contributions are as follows: 1. A Realistic RAG Framework for Indian Courts: We present NyayaRAG , a novel framework that emulates Indian common law decision-making by incorporating not only facts but also retrieved legal statutes and precedents. 2. Retrieval-Augmented Pipelines with Structured Inputs: We construct modular pipelines representing different combinations of factual, statutory, and precedent-based inputs to understand their individual and combined contributions to model performance. 3. Simulating Common Law Reasoning with LLMs: We show that LLMs guided by RAG and factual grounding can produce legally faithful explanations aligned with how real-world decisions are made under common law reasoning.",
            "Our work moves beyond fact-only or self-contained models by replicating a more faithful legal reasoning pipeline aligned with Indian jurisprudence. We hope that NyayaRAG opens new directions for building interpretable, retrieval-aware AI systems in legal settings, particularly in resource-constrained yet precedent-driven judicial systems like India’s. For the sake of reproducibility, we have made our dataset, code, and RAG-based pipeline implementation via a GitHub repository 1 1 1 https://github.com/ShubhamKumarNigam/RAGLegal ."
        ],
        "2 Related Work": [
            "Figure 1: Illustration of our Legal Judgment Prediction framework using RAG. The input legal judgment is first summarized; a RAG agent retrieves top-3 relevant documents from a vector database; and an instruction-tuned LLM (e.g., LLaMA-3.1 8B Instruct) generates the final prediction and explanation.",
            "Recent advancements in natural language processing (NLP) and large language models (LLMs) have significantly improved the performance of question answering (QA) and legal decision support systems. Transformer-based architectures such as BERT (Devlin et al., 2018 ) , GPT (Radford et al., 2019 ) , and their instruction-tuned successors have led to robust capabilities in knowledge-intensive and multi-hop reasoning tasks. The integration of external information via Retrieval-Augmented Generation (RAG) has emerged as a particularly effective approach for enhancing generation fidelity and reducing hallucinations (Han et al., 2024 ; Hei et al., 2024 ) .",
            "Within the legal domain, Legal Judgment Prediction (LJP) has seen significant progress, with models trained to infer outcomes based on factual and procedural components of court cases (Strickson and De La Iglesia, 2020 ; Xu et al., 2020 ; Feng et al., 2023 ) . In the Indian legal context, the ILDC corpus (Malik et al., 2021 ) and its extended variants (Nigam et al., 2024b ; Nigam and Deroy, 2023 ) have enabled the development of supervised and instruction-tuned models for both judgment prediction and explanation. The emergence of domain-specific datasets and architectures has allowed LJP systems to move from simple binary classification to more complex reasoning tasks aligned with real judicial behavior (Vats et al., 2023 ) .",
            "Parallel to these developments, there has been a sharp rise in interest in RAG techniques for legal NLP. Several benchmark and system-level contributions have explored how retrieval-enhanced generation can be leveraged to assist legal professionals, improve legal QA systems, and support document analysis. Notably, LegalBench-RAG (Pipitone and Alami, 2024 ) introduced a benchmark suite for evaluating RAG in the legal domain. Survey papers like (Hindi et al., 2025 ) provide comprehensive overviews of techniques aimed at improving RAG performance, factual grounding, and interpretability in legal settings.",
            "Several system-level contributions have demonstrated the power of RAG in specialized applications. Graph-RAG for Legal Norms (de Martim, 2025 ) and Bridging Legal Knowledge and AI (Barron et al., 2025 ) proposed methods to integrate structured legal knowledge such as statutes and normative hierarchies into the retrieval pipeline. Similarly, CBR-RAG (Wiratunga et al., 2024 ) applied case-based reasoning to leverage historical decisions, showing strong gains in legal question answering. HyPA-RAG (Kalra et al., 2024 ) explored hybrid parameter-adaptive retrieval to dynamically adjust context based on query specificity.",
            "Further domain-specific applications include AI-powered legal assistants like Legal Query RAG (Wahidur et al., 2025 ) and RAG-based solutions for dispute resolution in housing law (Rafat, 2024 ) . Optimizing Legal Information Access (Amato et al., 2024 ) showcased federated RAG architectures for secure document retrieval, and Augmenting Legal Judgment Prediction with Contrastive Case Relations (Liu et al., 2022 ) illustrated the benefits of encoding contrastive precedents for predictive reasoning."
        ],
        "3 Task Description": [
            "India’s judicial system operates within the common law framework, where judges deliberate cases based on three fundamental pillars: (i) the factual context of the case, (ii) applicable statutory provisions, and (iii) relevant judicial precedents. Our task is designed to simulate such realistic legal decision-making by leveraging Retrieval-Augmented Generation (RAG), enabling models to access external legal knowledge during inference.",
            "Figure 1 illustrates our Legal Judgment Prediction (LJP) pipeline enhanced with RAG. The pipeline begins with a full legal judgment document, which undergoes summarization to reduce its length and retain essential factual meaning. This is necessary because legal judgments tend to be long, and appending retrieved knowledge further increases the input size. Given limited model capacity and computational resources, we employ a summarization step (using Mixtral-8x7B-Instruct-v0.1 ) to create a condensed representation of both the input case and the retrieved legal context.",
            "Prediction Task: Based on the summarized factual description D D italic_D and the retrieved top- k k italic_k (e.g., k = 3 k=3 italic_k = 3 ) similar legal documents (statutes or precedents), the model predicts the likely court judgment. The prediction label y ∈ { 0 , 1 } y\\in\\{0,1\\} italic_y ∈ { 0 , 1 } indicates whether the appeal is fully rejected (0) or fully/partially accepted (1). This binary framing captures the most common forms of judicial decisions in Indian appellate courts.",
            "Explanation Task: Alongside the decision, the model is also required to generate an explanation that justifies its output. This explanation should logically incorporate the facts, cited statutes, and relevant precedents retrieved during the RAG process. This step emulates how judges provide reasoned opinions in written judgments.",
            "By structuring the LJP task in this way, summarizing long documents and integrating retrieval-based augmentation, we study the effectiveness of RAG agents in producing judgments that are both faithful to legal reasoning and grounded in precedent and statute. The overall framework allows us to approximate a real-world decision-making environment within Indian courtrooms."
        ],
        "4 Dataset": [
            "Our dataset is designed to simulate realistic court decision-making in the Indian legal context, incorporating facts, statutes, and precedent, essential elements under the common law framework. This dataset enables exploration of Legal Judgment Prediction (LJP) in a Retrieval-Augmented Generation (RAG) setup.",
            {
                "4.1 Dataset Compilation": [
                    "We curated a large-scale dataset consisting of 56,387 Supreme Court of India (SCI) case documents up to April 2024, sourced from IndianKanoon 2 2 2 https://indiankanoon.org/ , a trusted legal search engine. The website provides structural tags for various judgment components (e.g., facts, issues, arguments), which allowed for clean and structured scraping. These documents serve as the foundation for our summarization, retrieval, and reasoning experiments."
                ]
            },
            {
                "4.2 Dataset Composition": [
                    "The corpus supports multiple downstream pipelines, each focusing on specific judgment elements or legal context. Table 1 presents key statistics across different configurations, and an example breakdown is shown in the Appendix Table 7 .",
                    {
                        "4.2.1 Case Text": [
                            "Each judgment includes complete narrative content such as factual background, party arguments, legal issues, reasoning, and verdict. Due to length constraints exceeding model context windows, we summarized these documents using Mixtral-8x7B-Instruct-v0.1 (Jiang et al., 2024 ) , which supports up to 32k tokens. The summarization preserved critical legal elements through carefully designed prompts (see Table 2 ).",
                            "Dataset #Documents Avg. Length Max SCI (Full) 56,387 3,495 401,985 Summarized Single 4,962 302 875 Summarized Multi 4,930 300 879 Sections 29,858 257 27,553 Table 1: NyayaRAG Data Statistics.",
                            "Summarization Prompt The text is regarding a court judgment for a specific case. Summarize it into 1000 tokens but more than 700 tokens. The summarization should highlight the Facts, Issues, Statutes, Ratio of the decision, Ruling by Present Court (Decision), and a Conclusion. Table 2: Instruction prompt used with Mixtral-8x7B-Instruct-v0.1 for summarizing legal judgments."
                        ]
                    },
                    {
                        "4.2.2 Precedents": [
                            "From each judgment, cited precedents were extracted using metadata tags provided by IndianKanoon. These citations represent explicit legal reasoning and are retained for use during inference to replicate how courts consider prior judgments."
                        ]
                    },
                    {
                        "4.2.3 Statutes": [
                            "Statutory references were also programmatically extracted, including citations to laws like the Indian Penal Code and the Constitution of India. Where statute sections exceeded length limits, they were summarized using the same LLM pipeline. Only statutes directly cited in the respective cases were retained, ensuring relevance."
                        ]
                    },
                    {
                        "4.2.4 Previous Similar Cases": [
                            "To simulate implicit precedent-based reasoning, we employed semantic similarity retrieval to identify relevant previous cases beyond explicit citations:",
                            "• Corpus Vectorization: All 56,387 documents were embedded into dense vector representations using the all-MiniLM-L6-v2 sentence transformer. • Target Encoding: The 5,000 selected training samples were vectorized similarly. • Top- k k italic_k Retrieval: Using ChromaDB , we retrieved the top-3 most semantically similar cases for each document based on cosine similarity. • Augmentation: Retrieved cases were appended to the factual input to form the “ casetext + previous similar cases ” input during model inference.",
                            "This retrieval step enriches context with precedents that are semantically close, even if not cited, enhancing the legal realism of our setup."
                        ]
                    },
                    {
                        "4.2.5 Facts": [
                            "We separately extracted the factual portions of all 56,387 judgments. These include background information, chronological events, and party narratives, excluding legal reasoning. These fact-only subsets were used to simulate realistic courtroom scenarios where judges primarily rely on facts, relevant law, and precedent for decision-making.",
                            "Overall, our dataset is uniquely structured to test legal decision-making under realistic constraints, aligning with the Indian legal system’s reliance on factual narratives, statutory frameworks, and prior rulings."
                        ]
                    }
                ]
            }
        ],
        "5 Methodology": [
            "To simulate realistic judgment prediction and evaluate the role of RAG in enhancing legal decision-making, we design a modular experimental setup. This setup explores how different types of legal information, such as factual summaries, statutes, and precedents, affect model performance on the dual tasks of prediction and explanation. To ensure reproducibility and transparency, we detail the full experimental setup, including model configurations, training routines, and task-specific hyperparameters, in Appendix A . This includes separate subsections for the explanation generation (summarization) and legal judgment prediction tasks, outlining all relevant decoding strategies, optimization settings, and dataset splits used across our pipeline variants.",
            {
                "5.1 Pipeline Construction": [
                    "To systematically evaluate the impact of legal knowledge sources, we constructed multiple input pipelines using combinations of the dataset components described in Section 4 . Each pipeline configuration represents a distinct input scenario reflecting different degrees of legal context and retrieval augmentation. These pipelines are as follows:",
                    "• CaseText Only: Includes only the summarized version of the full case judgment, which contains factual background, arguments, and reasoning. • CaseText + Statutes: Appends summarized statutory references cited in the judgment to the case text, simulating scenarios where relevant laws are explicitly considered. • CaseText + Precedents: Incorporates prior cited judgments mentioned in the original case, representing explicitly relied-upon precedents. • CaseText + Previous Similar Cases: Adds top-3 semantically similar past judgments (retrieved via ChromaDB using all-MiniLM-L6-v2 embeddings), allowing the model to learn from precedents not explicitly cited. • CaseText + Statutes + Precedents: A comprehensive legal input pipeline combining the full judgment summary, statutes, and cited prior judgments. • Facts Only: A minimal pipeline containing only the factual summary, excluding all legal reasoning and verdicts. This setup evaluates whether a model can infer judgments from facts alone. • Facts + Statutes + Precedents: Combines factual input with statutory and precedent context to simulate realistic courtroom conditions where judges rely on facts, applicable law, and relevant past cases.",
                    "This modular design enables granular control over input features and facilitates direct comparison of how each knowledge source contributes to judgment prediction and explanation generation."
                ]
            },
            {
                "5.2 Prompt Design": [
                    "To ensure consistency and interpretability across all pipelines, we used fixed instruction prompts with minor variations depending on the available contextual inputs (e.g., facts only vs. facts + law + precedent). These prompts guide the model in producing both binary predictions and natural language explanations. Prompts were structured to reflect real judicial inquiry formats, aligning with the instruction-following capabilities of modern LLMs. Full prompt templates are listed in Appendix Table 8 , along with prediction examples."
                ]
            },
            {
                "5.3 Inference Setup": [
                    "We use the LLaMA-3.1 8B Instruct Dubey et al. ( 2024 ) model for all experiments in a few-shot prompting setup. Each input sequence, composed according to one of the pipeline templates, is paired with a relevant prompt. The model is required to output:",
                    "• A binary judgment prediction: 0 (appeal rejected) or 1 (appeal fully/partially accepted) • A justification: a coherent explanation based on legal facts, statutes, and precedent",
                    "The model is explicitly instructed to reason with the provided information and emulate judicial writing. Retrieved knowledge (via RAG) is included in-context to enhance legal reasoning while minimizing hallucinations.",
                    "This experimental design allows us to evaluate the effectiveness of legal retrieval and summarization under realistic judicial decision-making constraints in the Indian common law setting."
                ]
            }
        ],
        "6 Evaluation Metrics": [
            "Pipeline Name Partition Accuracy Precision Recall F1-score CaseText Only Single 62.27 33.50 30.88 29.45 Multi 53.10 25.26 23.95 20.81 CaseText + Statutes Single 67.07 45.29 44.55 44.32 Multi 60.36 64.22 64.04 60.35 CaseText + Precedents Single 61.73 41.92 41.35 40.81 Multi 57.53 61.34 61.19 57.53 CaseText + Previous Similar Cases Single 57.53 61.34 61.19 57.53 Multi 61.73 41.92 41.35 57.53 CaseText + Statutes + Precedents Single 64.71 43.50 42.98 42.78 Multi 65.86 63.94 63.99 63.96 CaseFacts Only Single 51.13 51.36 51.30 50.68 Multi 53.71 51.18 51.18 51.18 Facts + Statutes + Precedents Single 50.58 33.57 33.56 33.24 Multi 52.57 52.01 52.01 52.01 Table 3: Performance of Various Pipelines on Binary and Multi-label Legal Judgment Prediction. The best result has been marked in bold.",
            "To evaluate the effectiveness of our Retrieval-Augmented Legal Judgment Prediction framework, we adopt a comprehensive set of metrics covering both classification accuracy and explanation quality. The evaluation is conducted on two fronts: the judgment prediction task and the explanation generation task. These metrics are selected to ensure a holistic assessment of model performance in the legal domain. We report Macro Precision, Macro Recall, Macro F1, and Accuracy for judgment prediction, and we use both quantitative and qualitative methods to evaluate the quality of explanations generated by the model.",
            "1. Lexical-based Evaluation: We utilized standard lexical similarity metrics, including Rouge-L Lin ( 2004 ) , BLEU Papineni et al. ( 2002 ) , and METEOR Banerjee and Lavie ( 2005 ) . These metrics measure the overlap and order of words between the generated explanations and the reference texts, providing a quantitative assessment of the lexical accuracy of the model outputs. 2. Semantic Similarity-based Evaluation: To capture the semantic quality of the generated explanations, we employed BERTScore Zhang et al. ( 2020 ) , which measures the semantic similarity between the generated text and the reference explanations. Additionally, we used BLANC Vasilyev et al. ( 2020 ) , a metric that estimates the quality of generated text without a gold standard, to evaluate the model’s ability to produce semantically meaningful and contextually relevant explanations. 3. LLM-based Evaluation (LLM-as-a-Judge): To complement traditional metrics, we incorporate an automatic evaluation strategy that uses large language models themselves as evaluators, commonly referred to as LLM-as-a-Judge . This evaluation is crucial for assessing structured argumentation and legal correctness in a format aligned with expert judicial reasoning. We adopt G-Eval Liu et al. ( 2023 ) , a GPT-4-based evaluation framework tailored for natural language generation tasks. G-Eval leverages chain-of-thought prompting and structured scoring to assess explanations along three key criteria: factual accuracy , completeness & coverage , and clarity & coherence . Each generated legal explanation is scored on a scale from 1 to 10 based on how well it aligns with the expected content and a reference document. The exact prompt format used for evaluation is shown in Appendix Table 9 . For our experiments, we use the GPT-4o-mini model to generate reliable scores without manual intervention. This setup provides an interpretable, unified judgment metric that captures legal soundness, completeness of reasoning, and logical coherence, beyond what traditional similarity-based metrics can offer. 4. Expert Evaluation: To validate the interpretability and legal soundness of the model-generated explanations, we conduct an expert evaluation involving legal professionals. They rate a representative subset of the generated outputs on a 1–10 Likert scale across three criteria: factual accuracy, legal relevance, and completeness of reasoning. A score of 1 denotes a poor or misleading explanation, while a 10 reflects high legal fidelity and argumentative soundness. This evaluation provides critical insights beyond automated metrics. 5. Inter-Annotator Agreement (IAA): To ensure the reliability and consistency of expert judgments, we compute standard IAA statistics, including Fleiss’ Kappa, Cohen’s Kappa, Krippendorff’s Alpha, Intraclass Correlation Coefficient (ICC), and Pearson Correlation. These metrics quantify the degree of agreement across expert raters, reinforcing the credibility of the expert evaluation framework. Full details and scores are available in Appendix B ."
        ],
        "7 Results and Analysis": [
            "Pipelines RL BLEU METEOR BERTScore BLANC G-Eval Expert Score Single Partition CaseText Only 0.16 0.03 0.18 0.52 0.08 4.17 5.2 CaseText + Statutes 0.17 0.03 0.20 0.53 0.09 4.21 5.5 CaseText + Precedents 0.16 0.03 0.19 0.51 0.08 3.45 4.6 CaseText + Previous Similar Cases 0.16 0.03 0.20 0.52 0.08 3.72 4.9 CaseText + Statutes + Precedents 0.16 0.03 0.19 0.52 0.08 4.11 5.4 CaseFacts Only 0.16 0.02 0.18 0.52 0.06 3.53 4.5 Facts + Statutes + Precedents 0.16 0.02 0.18 0.51 0.06 2.97 3.9 Multi Partition CaseText Only 0.16 0.03 0.18 0.52 0.08 4.00 5.0 CaseText + Statutes 0.17 0.03 0.20 0.53 0.09 4.10 5.3 CaseText + Precedents 0.16 0.03 0.20 0.53 0.09 3.41 4.4 CaseText + Previous Similar Cases 0.16 0.03 0.19 0.52 0.08 3.67 4.7 CaseText + Statutes + Precedents 0.16 0.03 0.20 0.53 0.09 3.92 5.2 CaseFacts Only 0.15 0.02 0.17 0.52 0.08 3.74 4.6 Facts + Statutes + Precedents 0.15 0.02 0.19 0.52 0.07 3.08 4.1 Table 4: Comparison of Explanation Generation Across Different Legal Context Pipelines.",
            "We conducted extensive evaluations across multiple pipeline configurations to study the impact of different legal information components on both judgment prediction and explanation quality. Tables 3 and 4 summarize the model’s performance across these configurations for binary and multi-label settings.",
            {
                "7.1 Judgment Prediction Performance": [
                    "As shown in Table 3 , the pipeline combining CaseText + Statutes achieved the highest accuracy in the single-label setting. This suggests that legal statutes provide substantial contextual cues for the model to infer the likely decision. In contrast, CaseText Only achieved 62.27%, highlighting the importance of augmenting case narratives with applicable laws. Interestingly, the CaseText + Previous Similar Cases pipeline showed the highest precision, recall, and F1-score in the single-label case, indicating that semantically retrieved precedents, despite not being explicitly cited, help the model align with actual judicial outcomes.",
                    "In the multi-label setting, the best accuracy was observed for the CaseText + Statutes + Precedents pipeline. This comprehensive context provides the model with structured legal knowledge, improving generalization across different outcome labels. Conversely, the Facts Only pipeline performed worst overall, reaffirming that factual narratives alone, without legal context, are insufficient for reliably predicting legal outcomes. The poor performance of the Facts + Statutes + Precedents pipeline in the single-label setting suggests that factual sections might lack the interpretive cues that full case texts offer when combined with legal references."
                ]
            },
            {
                "7.2 Explanation Generation Quality": [
                    "Table 4 presents the results of explanation evaluation using a diverse set of metrics, including both automatic lexical and semantic metrics (ROUGE, BLEU, METEOR, BERTScore, BLANC) and a large language model-based evaluation (G-Eval). Across both single and multi-label setups, the CaseText + Statutes pipeline consistently outperformed all other configurations. In the single-label setting, it achieved the highest scores across key dimensions, substantially outperforming the CaseText Only baseline. This result underscores the critical role of statutory references in enhancing both the factual alignment and interpretability of model-generated legal explanations.",
                    "Interestingly, while the CaseText + Previous Similar Cases pipeline yielded strong lexical overlap (e.g., top ROUGE-L in the unabridged version), it lagged behind the statute-enhanced pipeline in metrics that assess semantic and contextual alignment, such as G-Eval and BLANC. This indicates that while similar cases might help the model replicate surface-level language, they may not consistently offer legally grounded or complete reasoning. Meanwhile, the CaseText + Statutes + Precedents pipeline also performed competitively, suggesting that combining structured legal references with precedent data can lead to balanced and high-quality explanations.",
                    "In contrast, configurations that relied solely on factual narratives ( CaseFacts Only and Facts + Statutes + Precedents ) exhibited comparatively poor performance across all evaluation metrics. For example, the Facts + Statutes + Precedents pipeline recorded a G-Eval score as low in the single-label setting. This reinforces the notion that factual descriptions, while essential, are insufficient for constructing legally persuasive rationales. The absence of structured legal arguments, statutory alignment, or precedent citation in these setups appears to undermine their explanatory effectiveness.",
                    {
                        "Expert Evaluation:": [
                            "To complement automatic evaluations, we also conducted a small-scale expert evaluation involving experienced legal professionals. Each expert independently rated a subset of model-generated explanations based on factual accuracy, legal relevance, and completeness using a 10-point Likert scale. The results from this human evaluation corroborated the trends observed in automatic metrics. Notably, the CaseText + Statutes pipeline received the highest expert score among all configurations, reinforcing the positive impact of statutory knowledge on explanation quality. In contrast, fact-only pipelines again received the lowest expert ratings, echoing concerns about their insufficient legal reasoning depth.",
                            "To ensure the reliability of expert scores, we conducted a detailed Inter-Annotator Agreement (IAA) analysis across multiple evaluation dimensions. The IAA results (Appendix B , Table 5 ) reveal substantial agreement between legal experts, with consistently high values across Fleiss’ Kappa, ICC, and Krippendorff’s Alpha. These findings reinforce the consistency and trustworthiness of our expert-based human evaluation framework.",
                            "Overall, the results emphasize the effectiveness of Retrieval-Augmented Generation (RAG) when paired with structured legal content, especially statutes, in producing accurate, interpretable, and legally coherent explanations. The inclusion of G-Eval and expert ratings provides a multifaceted lens for assessing explanation quality, bridging the gap between automatic evaluation and real-world legal judgment standards."
                        ]
                    }
                ]
            }
        ],
        "8 Ablation Study: Understanding the Role of Legal Context Components": [
            "To assess the individual contribution of each legal context component, factual narratives, statutory provisions, cited precedents, and semantically similar past cases, we perform an ablation study by systematically removing or altering these inputs across pipeline configurations. This study highlights how each component affects prediction accuracy and explanation quality, as reported in Tables 3 and 4 .",
            {
                "Impact on Judgment Prediction:": [
                    "The CaseText + Statutes + Precedents pipeline serves as the most comprehensive baseline. Removing statutory references (i.e., CaseText + Precedents ) leads to a noticeable drop in F1-score (from 63.96 to 57.53 in the multi-label setting), indicating that legal provisions provide structured grounding essential for accurate predictions. Similarly, eliminating precedents (i.e., CaseText + Statutes ) also reduces performance, though the drop is less steep, suggesting complementary roles of statutes and precedents. Pipelines relying solely on factual case narratives (e.g., CaseFacts Only ) perform the worst, reaffirming that factual information alone is insufficient for robust legal outcome prediction."
                ]
            },
            {
                "Impact on Explanation Quality:": [
                    "A similar pattern emerges in explanation generation. The CaseText + Statutes pipeline consistently outperforms others across ROUGE, BLEU, METEOR, BERTScore, and G-Eval metrics, underscoring the importance of grounding explanations in explicit statutory language. When only precedents are added (without statutes), as in CaseText + Precedents , explanation scores drop significantly (e.g., G-Eval: 4.21 to 3.45 in the single-label case). The worst-performing setup is Facts + Statutes + Precedents , highlighting that factual inputs, even when supplemented with legal references, do not suffice for generating coherent and persuasive explanations if the core case context is missing."
                ]
            },
            {
                "Insights:": [
                    "These findings validate the design choices in NyayaRAG , where integrating factual case text with statutory and precedential knowledge mimics real-world judicial reasoning. Statutory references provide normative structure, while precedents offer context-specific analogies. Their absence not only reduces predictive performance but also degrades the factuality, clarity, and legal coherence of the generated explanations.",
                    "This ablation analysis also offers practical guidance: for retrieval-augmented systems deployed in legal contexts, careful curation and combination of retrieved statutes and relevant precedents are critical to ensure trustworthy outputs."
                ]
            }
        ],
        "9 Conclusion and Future Scope": [
            "This paper introduced NyayaRAG , a Retrieval-Augmented Generation framework tailored for realistic legal judgment prediction and explanation in the Indian common law system. By combining factual case details with retrieved statutory provisions and relevant precedents, our approach mirrors judicial reasoning more closely than prior methods that rely solely on the case text. Empirical results across prediction and explanation tasks confirm that structured legal retrieval enhances both outcome accuracy and interpretability. Pipelines enriched with statutes and precedents consistently outperformed baselines, as validated by lexical, semantic, and LLM-based (G-Eval) metrics, as well as expert feedback.",
            "Future directions include extending to hierarchical verdict structures, integrating symbolic or graph-based retrieval, modeling temporal precedent evolution, and leveraging human-in-the-loop mechanisms. NyayaRAG marks a step toward court-aligned, explainable legal AI and sets the foundation for future research in retrieval-enhanced legal systems within underrepresented jurisdictions."
        ],
        "Limitations": [
            "While NyayaRAG marks a significant advance in realistic legal judgment prediction under the Indian common law framework, several limitations merit further attention.",
            "First, although Retrieval-Augmented Generation (RAG) helps reduce hallucinations by grounding outputs in retrieved legal documents, it does not fully eliminate factual or interpretive inaccuracies. In sensitive domains such as law, even rare errors in reasoning or justification may raise concerns about reliability and accountability.",
            "Second, the current framework supports binary and multi-label outcome structures but does not yet handle the full spectrum of legal verdicts, such as hierarchical or multi-class decisions involving complex legal provisions. Expanding to richer verdict taxonomies would enable broader applicability and deeper case understanding.",
            "Third, NyayaRAG assumes the availability of clean, well-structured legal documents and relies on summarization pipelines to manage input length. However, real-world legal texts often contain noise, OCR errors, or inconsistent formatting. Although summarization aids conciseness, it may inadvertently omit subtle legal nuances that affect judgment outcomes or explanation quality.",
            "Finally, due to computational resource constraints, the current system utilizes instruction-tuned LLMs guided by domain-specific prompts rather than fully fine-tuning on large-scale Indian legal corpora. While prompt-based tuning remains efficient and modular, fine-tuning on in-domain legal texts could further enhance model fidelity and domain alignment.",
            "Despite these limitations, NyayaRAG provides a robust and interpretable foundation for judgment prediction and explanation, supported by both automatic and expert evaluations. Future work that addresses these constraints, particularly hierarchical decision modeling and domain-specific fine-tuning, will further strengthen the framework’s legal relevance and practical deployment potential."
        ],
        "Ethics Statement": [
            "This research adheres to established ethical standards for conducting work in high-stakes domains such as law. The legal documents used in our study were sourced from IndianKanoon ( https://indiankanoon.org/ ), a publicly available repository of Indian court judgments. All documents are in the public domain and do not include sealed cases or personally identifiable sensitive information, ensuring that our use of the data complies with privacy and confidentiality norms.",
            "We emphasize that the proposed NyayaRAG system is developed strictly for academic research purposes to simulate realistic legal reasoning processes. It is not intended for direct deployment in real-world legal settings. The model outputs must not be construed as legal advice, official court predictions, or determinants of legal outcomes. Any downstream use should be performed with oversight by qualified legal professionals. We strongly discourage the use of this system in live legal cases, policymaking, or decisions that may affect individuals’ rights without appropriate human-in-the-loop supervision.",
            "As part of our evaluation protocol, we involved domain experts (legal professionals and researchers) to assess the quality and legal coherence of the generated explanations. The evaluation was conducted on a curated subset of samples, and all participating experts were informed of the research objectives and voluntarily participated without any coercion or conflict of interest. No personal data was collected during this process, and all expert feedback was anonymized for analysis.",
            "While we strive to enhance legal interpretability and transparency, we acknowledge that legal documents themselves may reflect systemic biases. Our framework, while replicating judicial reasoning patterns, may inherit such biases from training data. We do not deliberately introduce or amplify such biases, but we recognize the importance of further work in fairness auditing, particularly across litigant identity, socio-demographic markers, and jurisdictional diversity.",
            ""
        ]
    },
    {
        "id": "2508.05666v1",
        "title": "HySemRAG: A Hybrid Semantic Retrieval-Augmented Generation Framework for\n  Automated Literature Synthesis and Methodological Gap Analysis",
        "authors": [
            "Alejandro Godinez"
        ],
        "Abstract": ""
    },
    {
        "id": "2508.05664v1",
        "title": "Enhancing Retrieval-Augmented Generation for Electric Power Industry\n  Customer Support",
        "authors": [
            "Hei Yu Chan",
            "Kuok Tou Ho",
            "Chenglong Ma",
            "Yujing Si",
            "Hok Lai Lin",
            "Sa Lei Lam"
        ],
        "Abstract": ""
    },
    {
        "id": "2508.01005v1",
        "title": "MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented\n  Generation",
        "authors": [
            "Yiqun Chen",
            "Erhan Zhang",
            "Lingyong Yan",
            "Shuaiqiang Wang",
            "Jizhou Huang",
            "Dawei Yin",
            "Jiaxin Mao"
        ],
        "Abstract": ""
    },
    {
        "id": "2508.00743v1",
        "title": "Agentic large language models improve retrieval-based radiology question\n  answering",
        "authors": [
            "Sebastian Wind",
            "Jeta Sopa",
            "Daniel Truhn",
            "Mahshad Lotfinia",
            "Tri-Thien Nguyen",
            "Keno Bressem",
            "Lisa Adams",
            "Mirabela Rusu",
            "Harald Köstler",
            "Gerhard Wellein",
            "Andreas Maier",
            "Soroosh Tayebi Arasteh"
        ],
        "Abstract": ""
    },
    {
        "id": "2508.00965v1",
        "title": "VAULT: Vigilant Adversarial Updates via LLM-Driven Retrieval-Augmented\n  Generation for NLI",
        "authors": [
            "Roie Kazoom",
            "Ofir Cohen",
            "Rami Puzis",
            "Asaf Shabtai",
            "Ofer Hadar"
        ],
        "Abstract": "Abstract We introduce VAULT , a fully automated adversarial RAG pipeline that systematically uncovers and remedies weaknesses in NLI models through three stages: retrieval , adversarial generation , and iterative retraining . First, we perform balanced few‑shot retrieval by embedding premises with both semantic (BGE) and lexical (BM25) similarity. Next, we assemble these contexts into LLM prompts to generate adversarial hypotheses , which are then validated by an LLM ensemble for label fidelity. Finally, the validated adversarial examples are injected back into the training set at increasing mixing ratios, progressively fortifying a zero‑shot RoBERTa‑base model.On standard benchmarks, VAULT elevates RoBERTa‑base accuracy from 88.48% to 92.60% on SNLI +4.12%, from 75.04% to 80.95% on ANLI +5.91%, and from 54.67% to 71.99% on MultiNLI +17.32%. It also consistently outperforms prior in‑context adversarial methods by up to 2.0% across datasets. By automating high‑quality adversarial data curation at scale, VAULT enables rapid, human‑independent robustness improvements in NLI inference tasks.",
        "Introduction": [
            "Natural language inference (NLI)-the task of determining whether a hypothesis is entailed by, contradicted by, or neutral with respect to a given premise-is fundamental to many downstream NLP applications such as question answering, summarization, and dialogue systems. Despite rapid progress, even state-of-the-art models remain brittle when faced with adversarial or out-of-domain examples, often exploiting spurious lexical cues or failing on simple syntactic variations (Glockner, Shwartz, and Goldberg 2018 ; Carmona, Mitchell, and Riedel 2018 ) . Benchmarks like ANLI (Nie et al. 2019 ) and manually curated corpora such as SNLI (Bowman et al. 2015 ) and MultiNLI (Williams, Nangia, and Bowman 2018 ) have driven robustness improvements but incur high annotation costs and still leave many failure modes uncovered. More recently, large synthetic datasets like GNLI (Hosseini et al. 2024 ) have been generated at scale, but their untargeted nature often dilutes the most critical adversarial patterns.",
            "Inspired by these limitations, we introduce VAULT, a fully automated adversarial Retrieval-Augmented Generation (RAG) pipeline that systematically mines and repairs the weak spots of NLI models without any manual labeling. VAULT begins by retrieving balanced few-shot contexts from SNLI using both semantic embeddings (BGE M3 (Chen et al. 2024 ) ) and lexical matching (BM25 (Robertson and Zaragoza 2009 ) ), then prompts a LLM to generate challenging hypotheses tailored to the model’s current weaknesses. Each candidate pair of premise and hypothesis is vetted by an ensemble of three LLMs and only unanimously agreed-upon examples are used for future training of the target model. By iterating this retrieve-generate-validate loop for multiple rounds, VAULT progressively hardens the target NLI model against its own blind spots, focusing data where it matters most.",
            "In a strict zero-shot evaluation on SNLI, ANLI, and MultiNLI test sets, VAULT achieves substantial gains over the original RoBERTa-base accuracy: from 88.48% to 92.13% on SNLI (+3.65%), from 75.04% to 80.27% on ANLI (+5.23%), and from 54.67% to 71.12% on MultiNLI (+16.45%). These improvements exceed those of prior in-context adversarial approaches by at least 2% on each benchmark, demonstrating that fully automated adversarial augmentation can match or surpass human-curated data with only a fraction of the examples.",
            "The key stages of VAULT are: • Retrieval: Embed all SNLI data with a semantic text embedder and retrieve balanced few-shot examples via both semantic and lexical similarity; • Generation: Using the retrieved exmaples, assemble a context to serve as prompt for the LLM and generate challenging hypotheses; • Adversarial Filtering: Pass the generated examples through the target model, and keep only the examples that failed it. • Validation & training: Further filter the generated examples for unanimous agreement among the LLM judges to ensure data correctness. Then use high-confidence examples for training; • Iterative Retraining: repeat all previous steps for multiple rounds to continually strengthen the model.",
            "Our contributions are: 1. An end-to-end automated adversarial RAG pipeline that requires no human annotation and adapts dynamically to a model’s weaknesses. 2. A demonstration that targeted synthesis and validation solely via LLMs can yield significant zero-shot and few-shot accuracy gains on multiple NLI benchmarks using an order of magnitude less data than prior synthetic corpora. 3. Empirical evidence that VAULT not only outperforms existing adversarial augmentation methods in effectiveness but also offers superior data efficiency, highlighting a new direction for high-impact robustness improvements."
        ],
        "Background and Related Work": [
            "Improving the robustness and performance of NLI models remains a significant challenge in natural language understanding (Glockner, Shwartz, and Goldberg 2018 ; Carmona, Mitchell, and Riedel 2018 ) . While traditional approaches heavily relied on manually created datasets, such as the Stanford NLI (SNLI) corpus (Bowman et al. 2015 ) , this labor-intensive process highlighted the need for more efficient alternatives. The Multi-Genre NLI (MultiNLI) dataset (Williams, Nangia, and Bowman 2018 ) expanded coverage to diverse text genres, and ANLI (Nie et al. 2019 ) introduced a human-and-model-in-the-loop protocol to collect hard cases, yet all still require extensive annotation effort. More recently, Kazoom et al . (Kazoom et al. 2025 ) proposed a training-free adversarial detection framework that leverages retrieval-augmented generation to automatically generate and filter challenging examples without manual labeling. Recent advances in large language models have enabled automated dataset creation at scale. In our VAULT pipeline, we synthesize adversarial hypotheses with Llama-4-Scout-17B-16E-Instruct and then employ an ensemble of Gemma-3-27B-IT, Phi-4, and Qwen3-32B to unanimously validate each candidate before fine-tuning RoBERTa-base (Liu et al. 2019 ) . This approach builds on synthetic data methods such as GNLI (Hosseini et al. 2024 ) , which demonstrated that purely LLM-generated corpora can yield strong zero-shot transfer on ANLI and MNLI, and on counterfactual and paraphrase generation techniques that enrich training distributions (Li et al. 2023 ; Klemen and Robnik-Šikonja 2021 ) .",
            "Retrieval for Few-Shot Prompting. Quality context examples are critical for reliable generation. Hybrid retrieval-combining semantically rich embeddings (BGE M3) with robust lexical scoring (BM25)-has been shown to select more diverse and relevant few-shot demonstrations, leading to higher-fidelity outputs and fewer label errors downstream (Chen et al. 2024 ; Robertson and Zaragoza 2009 ) .",
            "Automated Adversarial Example Generation. Automated adversarial pipelines seek to stress-test and fortify NLI models without manual curation. Minervini et al .generate logical-constraint-violating instances via LLM prompting, improving SNLI and MultiNLI robustness by 2-3% (Minervini and Riedel 2018 ) . Nie et al.’s ANLI leverages a model-in-the-loop to surface challenging examples, boosting out-of-domain transfer by roughly 5% (Nie et al. 2020 ) . Iyyer et al.’s SCPNs apply controlled syntactic transformations to create paraphrase-based attacks, yielding a 4% robustness gain (Iyyer et al. 2018 ) . More recent work on large-scale synthetic NLI data (e.g. GNLI) has shown that such corpora can rival or surpass real training sets on zero-shot benchmarks (Hosseini et al. 2024 ) . Unlike these prior methods, VAULT fully automates retrieval, adversarial generation, multi-LLM validation, and iterative retraining, providing a scalable, end-to-end solution for enhancing NLI models’ resilience."
        ],
        "Methodology": [
            "Figure 2: VAULT’s stages: on the left , direct LLM hypothesis generation (no retrieval or validation); in the middle , context retrieval and adversarial hypothesis generation (no validation); and on the right , the full pipeline with retrieval, generation, and automated validation before reinjection.",
            "Let 𝒟 = { ( p i , y i ) } i = 1 N \\mathcal{D}=\\{(p_{i},y_{i})\\}_{i=1}^{N} caligraphic_D = { ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT be the NLI training set on which the target model was trained, where each premise p i p_{i} italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is paired with a label y i ∈ { entail , neutral , contradict } y_{i}\\in\\{\\text{entail},\\text{neutral},\\text{contradict}\\} italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ { entail , neutral , contradict } . We denote by M ( t ) M^{(t)} italic_M start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT the target NLI model after t t italic_t rounds of adversarial retraining, with M ( 0 ) M^{(0)} italic_M start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT pre-trained on 𝒟 \\mathcal{D} caligraphic_D . The VAULT pipeline (see Fig. 2 ) enhances M M italic_M through five stages-Retrieval, Hypothesis Generation, Adversarial filtering, Automated Validation, and Iterative Retraining-applied to each ( p , y ) ∈ 𝒟 (p,y)\\in\\mathcal{D} ( italic_p , italic_y ) ∈ caligraphic_D . Figure 1 provides an overview.",
            {
                "1. Retrieval": [
                    "For each premise p p italic_p , we assemble a label-balanced few-shot context 𝒞 p = ⋃ y ′ ∈ { entail, neutral, contradict } 𝒞 p r ( y ′ ) \\mathcal{C}_{p}=\\bigcup_{y^{\\prime}\\in\\{\\text{entail, neutral, contradict}\\}}\\mathcal{C}_{p}^{r}(y^{\\prime}) caligraphic_C start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = ⋃ start_POSTSUBSCRIPT italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ { entail, neutral, contradict } end_POSTSUBSCRIPT caligraphic_C start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ( italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) , with | 𝒞 p | = 3 k |\\mathcal{C}_{p}|=3k | caligraphic_C start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT | = 3 italic_k by retrieving k k italic_k examples per label under mode r ∈ { sem , lex } r\\in\\{\\mathrm{sem},\\mathrm{lex}\\} italic_r ∈ { roman_sem , roman_lex } . Let 𝒟 y ′ \\mathcal{D}_{y^{\\prime}} caligraphic_D start_POSTSUBSCRIPT italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT be all premises with label y ′ y^{\\prime} italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT .",
                    "Semantic Retrieval. We denote the text embedder as e m b emb italic_e italic_m italic_b . Embed each premise x x italic_x as e x = E emb ( x ) ∈ ℝ d e_{x}=E_{\\mathrm{emb}}(x)\\in\\mathbb{R}^{d} italic_e start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = italic_E start_POSTSUBSCRIPT roman_emb end_POSTSUBSCRIPT ( italic_x ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT . For query p p italic_p , e p = E emb ( p ) e_{p}=E_{\\mathrm{emb}}(p) italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = italic_E start_POSTSUBSCRIPT roman_emb end_POSTSUBSCRIPT ( italic_p ) , and for each y ′ y^{\\prime} italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT we select 𝒞 p sem ( y ′ ) = arg ⁡ max S ⊆ 𝒟 y ′ | S | = k ∑ x ∈ S cos ⁡ ( e p , e x ) , \\mathcal{C}_{p}^{\\mathrm{sem}}(y^{\\prime})=\\arg\\!\\max_{\\begin{subarray}{c}S\\subseteq\\mathcal{D}_{y^{\\prime}}\\\\ |S|=k\\end{subarray}}\\sum_{x\\in S}\\cos(e_{p},e_{x}), caligraphic_C start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_sem end_POSTSUPERSCRIPT ( italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) = roman_arg roman_max start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_S ⊆ caligraphic_D start_POSTSUBSCRIPT italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL | italic_S | = italic_k end_CELL end_ROW end_ARG end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_x ∈ italic_S end_POSTSUBSCRIPT roman_cos ( italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ) , i.e. the k k italic_k nearest neighbors by cosine similarity in embedding space.",
                    "Lexical (BM25) Retrieval. Index premises with BM25 ( k 1 = 1.5 , b = 0.75 k_{1}=1.5,b=0.75 italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 1.5 , italic_b = 0.75 ) and define s BM25 ( p , x ) = ∑ t ∈ p IDF ( t ) ⋅ tf ( t , x ) ( k 1 + 1 ) tf ( t , x ) + k 1 ( 1 − b + b | x | avgdl ) . s_{\\mathrm{BM25}}(p,x)=\\sum_{t\\in p}\\mathrm{IDF}(t)\\cdot\\frac{\\mathrm{tf}(t,x)(k_{1}+1)}{\\mathrm{tf}(t,x)+k_{1}(1-b+b\\frac{|x|}{\\mathrm{avgdl}})}. italic_s start_POSTSUBSCRIPT BM25 end_POSTSUBSCRIPT ( italic_p , italic_x ) = ∑ start_POSTSUBSCRIPT italic_t ∈ italic_p end_POSTSUBSCRIPT roman_IDF ( italic_t ) ⋅ divide start_ARG roman_tf ( italic_t , italic_x ) ( italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + 1 ) end_ARG start_ARG roman_tf ( italic_t , italic_x ) + italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( 1 - italic_b + italic_b divide start_ARG | italic_x | end_ARG start_ARG roman_avgdl end_ARG ) end_ARG . Then for each y ′ y^{\\prime} italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , retrieve 𝒞 p lex ( y ′ ) = arg ⁡ max S ⊆ 𝒟 y ′ | S | = k ∑ x ∈ S s BM25 ( p , x ) . \\mathcal{C}_{p}^{\\mathrm{lex}}(y^{\\prime})=\\arg\\!\\max_{\\begin{subarray}{c}S\\subseteq\\mathcal{D}_{y^{\\prime}}\\\\ |S|=k\\end{subarray}}\\sum_{x\\in S}s_{\\mathrm{BM25}}(p,x). caligraphic_C start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_lex end_POSTSUPERSCRIPT ( italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) = roman_arg roman_max start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_S ⊆ caligraphic_D start_POSTSUBSCRIPT italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL | italic_S | = italic_k end_CELL end_ROW end_ARG end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_x ∈ italic_S end_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT BM25 end_POSTSUBSCRIPT ( italic_p , italic_x ) .",
                    "Combining both modes yields | 𝒞 p | = 3 k |\\mathcal{C}_{p}|=3k | caligraphic_C start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT | = 3 italic_k with equal representation of each NLI class, blending semantic depth and lexical relevance.",
                    "Combined (semantic + BM25) Retrieval. To leverage both semantic and lexical signals, we first compute for each candidate x x italic_x and query p p italic_p : s ~ sem ( p , x ) \\displaystyle\\tilde{s}_{\\mathrm{sem}}(p,x) over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT roman_sem end_POSTSUBSCRIPT ( italic_p , italic_x ) = cos ⁡ ( E emb ( p ) , E emb ( x ) ) − μ sem σ sem , \\displaystyle=\\frac{\\cos\\bigl{(}E_{\\mathrm{emb}}(p),\\,E_{\\mathrm{emb}}(x)\\bigr{)}-\\mu_{\\mathrm{sem}}}{\\sigma_{\\mathrm{sem}}}, = divide start_ARG roman_cos ( italic_E start_POSTSUBSCRIPT roman_emb end_POSTSUBSCRIPT ( italic_p ) , italic_E start_POSTSUBSCRIPT roman_emb end_POSTSUBSCRIPT ( italic_x ) ) - italic_μ start_POSTSUBSCRIPT roman_sem end_POSTSUBSCRIPT end_ARG start_ARG italic_σ start_POSTSUBSCRIPT roman_sem end_POSTSUBSCRIPT end_ARG , (1) s ~ lex ( p , x ) \\displaystyle\\tilde{s}_{\\mathrm{lex}}(p,x) over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT roman_lex end_POSTSUBSCRIPT ( italic_p , italic_x ) = s BM25 ( p , x ) − μ lex σ lex , \\displaystyle=\\frac{s_{\\mathrm{BM25}}(p,x)-\\mu_{\\mathrm{lex}}}{\\sigma_{\\mathrm{lex}}}, = divide start_ARG italic_s start_POSTSUBSCRIPT BM25 end_POSTSUBSCRIPT ( italic_p , italic_x ) - italic_μ start_POSTSUBSCRIPT roman_lex end_POSTSUBSCRIPT end_ARG start_ARG italic_σ start_POSTSUBSCRIPT roman_lex end_POSTSUBSCRIPT end_ARG , (2) s comb ( p , x ) \\displaystyle s_{\\mathrm{comb}}(p,x) italic_s start_POSTSUBSCRIPT roman_comb end_POSTSUBSCRIPT ( italic_p , italic_x ) = α s ~ sem ( p , x ) + ( 1 − α ) s ~ lex ( p , x ) . \\displaystyle=\\alpha\\,\\tilde{s}_{\\mathrm{sem}}(p,x)+(1-\\alpha)\\,\\tilde{s}_{\\mathrm{lex}}(p,x). = italic_α over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT roman_sem end_POSTSUBSCRIPT ( italic_p , italic_x ) + ( 1 - italic_α ) over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT roman_lex end_POSTSUBSCRIPT ( italic_p , italic_x ) . (3)",
                    "where μ \\mu italic_μ and σ \\sigma italic_σ are the corpus mean and standard deviation of each score. We then form a weighted sum s comb ( p , x ) = α s ~ sem ( p , x ) + ( 1 − α ) s ~ lex ( p , x ) , s_{\\mathrm{comb}}(p,x)=\\alpha\\,\\tilde{s}_{\\mathrm{sem}}(p,x)\\;+\\;(1-\\alpha)\\,\\tilde{s}_{\\mathrm{lex}}(p,x), italic_s start_POSTSUBSCRIPT roman_comb end_POSTSUBSCRIPT ( italic_p , italic_x ) = italic_α over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT roman_sem end_POSTSUBSCRIPT ( italic_p , italic_x ) + ( 1 - italic_α ) over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT roman_lex end_POSTSUBSCRIPT ( italic_p , italic_x ) , with α ∈ [ 0 , 1 ] \\alpha\\in[0,1] italic_α ∈ [ 0 , 1 ] controlling the interpolation between semantic and lexical retrieval. Finally, for each label y ′ y^{\\prime} italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , we retrieve 𝒞 p comb ( y ′ ) = arg ⁡ max S ⊆ 𝒟 y ′ | S | = k ∑ x ∈ S s comb ( p , x ) , \\mathcal{C}_{p}^{\\mathrm{comb}}(y^{\\prime})=\\arg\\!\\max_{\\begin{subarray}{c}S\\subseteq\\mathcal{D}_{y^{\\prime}}\\\\ |S|=k\\end{subarray}}\\sum_{x\\in S}s_{\\mathrm{comb}}(p,x), caligraphic_C start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_comb end_POSTSUPERSCRIPT ( italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) = roman_arg roman_max start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_S ⊆ caligraphic_D start_POSTSUBSCRIPT italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL | italic_S | = italic_k end_CELL end_ROW end_ARG end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_x ∈ italic_S end_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT roman_comb end_POSTSUBSCRIPT ( italic_p , italic_x ) , selecting the top- k k italic_k premises by combined score. This yields | 𝒞 p | = 3 k \\lvert\\mathcal{C}_{p}\\rvert=3k | caligraphic_C start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT | = 3 italic_k with examples that capture both deep contextual similarity and surface-level overlap.",
                    "In each run, we set 𝒞 p = ⋃ y ′ ∈ { entail , neutral , contradict } 𝒞 p r ( y ′ ) , \\mathcal{C}_{p}=\\bigcup_{y^{\\prime}\\in\\{\\mathrm{entail},\\,\\mathrm{neutral},\\,\\mathrm{contradict}\\}}\\mathcal{C}_{p}^{r}(y^{\\prime}), caligraphic_C start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = ⋃ start_POSTSUBSCRIPT italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ { roman_entail , roman_neutral , roman_contradict } end_POSTSUBSCRIPT caligraphic_C start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ( italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) , yielding a balanced prompt context of size 3 k 3k 3 italic_k .",
                    "As described in Algorithm 1 , we retrieve a label-balanced few-shot context for each premise by selecting the top- k k italic_k examples per NLI class under semantic, lexical, or combined scoring.",
                    "Algorithm 1 Balanced Few‐Shot Context Retrieval (with combined mode) Input : Premise p p italic_p , dataset 𝒟 \\mathcal{D} caligraphic_D partitioned by label { 𝒟 y } \\{\\mathcal{D}_{y}\\} { caligraphic_D start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT } Parameter : examples per label k k italic_k , mode r ∈ { sem , lex , comb } r\\in\\{\\mathrm{sem},\\mathrm{lex},\\mathrm{comb}\\} italic_r ∈ { roman_sem , roman_lex , roman_comb } Output : Few‐shot context 𝒞 p \\mathcal{C}_{p} caligraphic_C start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT 1: 𝒞 p ← ∅ \\mathcal{C}_{p}\\leftarrow\\emptyset caligraphic_C start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ← ∅ 2: if r = comb r=\\mathrm{comb} italic_r = roman_comb then 3: compute μ sem , σ sem \\mu_{\\mathrm{sem}},\\sigma_{\\mathrm{sem}} italic_μ start_POSTSUBSCRIPT roman_sem end_POSTSUBSCRIPT , italic_σ start_POSTSUBSCRIPT roman_sem end_POSTSUBSCRIPT over EMB scores 4: compute μ lex , σ lex \\mu_{\\mathrm{lex}},\\sigma_{\\mathrm{lex}} italic_μ start_POSTSUBSCRIPT roman_lex end_POSTSUBSCRIPT , italic_σ start_POSTSUBSCRIPT roman_lex end_POSTSUBSCRIPT over BM25 scores 5: e p ← E emb ( p ) e_{p}\\leftarrow E_{\\mathrm{emb}}(p) italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ← italic_E start_POSTSUBSCRIPT roman_emb end_POSTSUBSCRIPT ( italic_p ) 6: end if 7: for each label y ′ ∈ { entail , neutral , contradict } y^{\\prime}\\in\\{\\text{entail},\\text{neutral},\\text{contradict}\\} italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ { entail , neutral , contradict } do 8: for each x ∈ 𝒟 y ′ x\\in\\mathcal{D}_{y^{\\prime}} italic_x ∈ caligraphic_D start_POSTSUBSCRIPT italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT do 9: if r = sem r=\\mathrm{sem} italic_r = roman_sem then 10: scores [ x ] ← cos ⁡ ( e p , E emb ( x ) ) \\text{scores}[x]\\leftarrow\\cos(e_{p},\\,E_{\\mathrm{emb}}(x)) scores [ italic_x ] ← roman_cos ( italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT roman_emb end_POSTSUBSCRIPT ( italic_x ) ) 11: else if r = lex r=\\mathrm{lex} italic_r = roman_lex then 12: scores [ x ] ← s BM25 ( p , x ) \\text{scores}[x]\\leftarrow s_{\\mathrm{BM25}}(p,x) scores [ italic_x ] ← italic_s start_POSTSUBSCRIPT BM25 end_POSTSUBSCRIPT ( italic_p , italic_x ) 13: else if r = comb r=\\mathrm{comb} italic_r = roman_comb then 14: s ~ sem ← cos ⁡ ( e p , E emb ( x ) ) − μ sem σ sem \\tilde{s}_{\\mathrm{sem}}\\leftarrow\\tfrac{\\cos(e_{p},E_{\\mathrm{emb}}(x))-\\mu_{\\mathrm{sem}}}{\\sigma_{\\mathrm{sem}}} over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT roman_sem end_POSTSUBSCRIPT ← divide start_ARG roman_cos ( italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT roman_emb end_POSTSUBSCRIPT ( italic_x ) ) - italic_μ start_POSTSUBSCRIPT roman_sem end_POSTSUBSCRIPT end_ARG start_ARG italic_σ start_POSTSUBSCRIPT roman_sem end_POSTSUBSCRIPT end_ARG 15: s ~ lex ← s BM25 ( p , x ) − μ lex σ lex \\tilde{s}_{\\mathrm{lex}}\\leftarrow\\tfrac{s_{\\mathrm{BM25}}(p,x)-\\mu_{\\mathrm{lex}}}{\\sigma_{\\mathrm{lex}}} over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT roman_lex end_POSTSUBSCRIPT ← divide start_ARG italic_s start_POSTSUBSCRIPT BM25 end_POSTSUBSCRIPT ( italic_p , italic_x ) - italic_μ start_POSTSUBSCRIPT roman_lex end_POSTSUBSCRIPT end_ARG start_ARG italic_σ start_POSTSUBSCRIPT roman_lex end_POSTSUBSCRIPT end_ARG 16: scores [ x ] ← α s ~ sem + ( 1 − α ) s ~ lex \\text{scores}[x]\\leftarrow\\alpha\\,\\tilde{s}_{\\mathrm{sem}}+(1-\\alpha)\\,\\tilde{s}_{\\mathrm{lex}} scores [ italic_x ] ← italic_α over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT roman_sem end_POSTSUBSCRIPT + ( 1 - italic_α ) over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT roman_lex end_POSTSUBSCRIPT 17: end if 18: end for 19: top_k ← arg ⁡ max x ∈ 𝒟 y ′ k ⁡ scores [ x ] \\text{top\\_k}\\leftarrow\\arg\\max\\nolimits^{k}_{x\\in\\mathcal{D}_{y^{\\prime}}}\\text{scores}[x] top_k ← roman_arg roman_max start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_x ∈ caligraphic_D start_POSTSUBSCRIPT italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT scores [ italic_x ] 20: 𝒞 p ← 𝒞 p ∪ top_k \\mathcal{C}_{p}\\leftarrow\\mathcal{C}_{p}\\cup\\text{top\\_k} caligraphic_C start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ← caligraphic_C start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ∪ top_k 21: end for 22: return 𝒞 p \\mathcal{C}_{p} caligraphic_C start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT"
                ]
            },
            {
                "2. Hypothesis Generation": [
                    "Given input ( p , 𝒞 p , y ) (p,\\mathcal{C}_{p},y) ( italic_p , caligraphic_C start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_y ) from stage 1, we employ a LLM to produce a hypothesis h h italic_h ."
                ]
            },
            {
                "3. Adversarial Filtering": [
                    "Once generated, the hypothesis is paired with its premise and label for classification by the target model. Hypotheses correctly classified by the model are discarded, ensuring only the misclassified examples are kept."
                ]
            },
            {
                "4. Automated Validation": [
                    "Let ℋ p = { h ∣ M ( t ) ( p , h ) ≠ y } \\mathcal{H}_{p}=\\{h\\mid M^{(t)}(p,h)\\neq y\\} caligraphic_H start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = { italic_h ∣ italic_M start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ( italic_p , italic_h ) ≠ italic_y } be the set of generated candidates. Each ( p , h ) ∈ ℋ p (p,h)\\in\\mathcal{H}_{p} ( italic_p , italic_h ) ∈ caligraphic_H start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT is validated by three LLM judges - Gemma-3-27B-IT (Google Research 2025 ) , Phi-4 (Microsoft Research 2025 ) , and Qwen3-32B (Qwen Team 2025 ) . Denote each referee’s label by v j = M j ( p , h ) v_{j}=M_{j}(p,h) italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = italic_M start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_p , italic_h ) . We retain ( p , h , y ) (p,h,y) ( italic_p , italic_h , italic_y ) only if all three agree: ∑ j = 1 3 𝟏 [ v j = y ] = 3 . \\sum_{j=1}^{3}\\mathbf{1}[v_{j}=y]\\;=\\;3. ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT bold_1 [ italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = italic_y ] = 3 . This unanimous-vote check guarantees maximal label fidelity without manual effort."
                ]
            },
            {
                "5. Iterative Retraining": [
                    "At iteration t t italic_t , let 𝒟 adv ( t ) \\mathcal{D}_{\\mathrm{adv}}^{(t)} caligraphic_D start_POSTSUBSCRIPT roman_adv end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT be the set of validated adversarial triples. We update the training set 𝒟 ( t + 1 ) = 𝒟 ∪ 𝒟 adv ( t ) \\mathcal{D}^{(t+1)}=\\mathcal{D}\\;\\cup\\;\\mathcal{D}_{\\mathrm{adv}}^{(t)} caligraphic_D start_POSTSUPERSCRIPT ( italic_t + 1 ) end_POSTSUPERSCRIPT = caligraphic_D ∪ caligraphic_D start_POSTSUBSCRIPT roman_adv end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT and fine-tune M ( t ) M^{(t)} italic_M start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT (e.g. for E = 3 E=3 italic_E = 3 epochs, learning rate η = 2 × 10 − 5 \\eta=2{\\times}10^{-5} italic_η = 2 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT , batch size B = 32 B=32 italic_B = 32 ) to obtain M ( t + 1 ) M^{(t+1)} italic_M start_POSTSUPERSCRIPT ( italic_t + 1 ) end_POSTSUPERSCRIPT . Repeat for t = 0 , … , T − 1 t=0,\\dots,T-1 italic_t = 0 , … , italic_T - 1 , progressively hardening the model.",
                    "This closed-loop process-retrieve, generate, validate (unanimously), retrain-enables VAULT to iteratively strengthen NLI models by exposing them to increasingly challenging, automatically mined adversarial examples.",
                    "The overall VAULT pipeline is summarized in Algorithm 2 .",
                    "Algorithm 2 VAULT Pipeline: Automated Adversarial RAG Input : Training set 𝒟 = { ( p i , y i ) } i = 1 N \\mathcal{D}=\\{(p_{i},y_{i})\\}_{i=1}^{N} caligraphic_D = { ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT , examples per label k k italic_k , iterations T T italic_T , retrieval mode r r italic_r Output : Enhanced model M ( T ) M^{(T)} italic_M start_POSTSUPERSCRIPT ( italic_T ) end_POSTSUPERSCRIPT 1: Train initial model M ( 0 ) M^{(0)} italic_M start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT on 𝒟 \\mathcal{D} caligraphic_D 2: for t = 0 t=0 italic_t = 0 to T − 1 T-1 italic_T - 1 do 3: 𝒟 adv ← ∅ \\mathcal{D}_{\\mathrm{adv}}\\leftarrow\\emptyset caligraphic_D start_POSTSUBSCRIPT roman_adv end_POSTSUBSCRIPT ← ∅ 4: for each ( p , y ) ∈ 𝒟 (p,y)\\in\\mathcal{D} ( italic_p , italic_y ) ∈ caligraphic_D do 5: 𝒞 p ← RETRIEVE _ CONTEXT ( p , 𝒟 , k , r ) \\mathcal{C}_{p}\\leftarrow\\mathrm{RETRIEVE\\_CONTEXT}(p,\\mathcal{D},k,r) caligraphic_C start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ← roman_RETRIEVE _ roman_CONTEXT ( italic_p , caligraphic_D , italic_k , italic_r ) 6: h ← GENERATE _ HYPOTHESIS ( p , 𝒞 p , y ) h\\leftarrow\\mathrm{GENERATE\\_HYPOTHESIS}(p,\\mathcal{C}_{p},y) italic_h ← roman_GENERATE _ roman_HYPOTHESIS ( italic_p , caligraphic_C start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_y ) 7: if M ( t ) ( p , h ) ≠ y M^{(t)}(p,h)\\neq y italic_M start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ( italic_p , italic_h ) ≠ italic_y and UNANIMOUS _ VALIDATE ( p , h , y ) \\mathrm{UNANIMOUS\\_VALIDATE}(p,h,y) roman_UNANIMOUS _ roman_VALIDATE ( italic_p , italic_h , italic_y ) then 8: 𝒟 adv ← 𝒟 adv ∪ { ( p , h , y ) } \\mathcal{D}_{\\mathrm{adv}}\\leftarrow\\mathcal{D}_{\\mathrm{adv}}\\cup\\{(p,h,y)\\} caligraphic_D start_POSTSUBSCRIPT roman_adv end_POSTSUBSCRIPT ← caligraphic_D start_POSTSUBSCRIPT roman_adv end_POSTSUBSCRIPT ∪ { ( italic_p , italic_h , italic_y ) } 9: end if 10: end for 11: Fine‐tune M ( t + 1 ) M^{(t+1)} italic_M start_POSTSUPERSCRIPT ( italic_t + 1 ) end_POSTSUPERSCRIPT on 𝒟 ∪ 𝒟 adv \\mathcal{D}\\cup\\mathcal{D}_{\\mathrm{adv}} caligraphic_D ∪ caligraphic_D start_POSTSUBSCRIPT roman_adv end_POSTSUBSCRIPT 12: end for 13: return M ( T ) M^{(T)} italic_M start_POSTSUPERSCRIPT ( italic_T ) end_POSTSUPERSCRIPT"
                ]
            },
            {
                "Hyperparameter Tuning for Retrieval": [
                    "Retrieval quality depends critically on the interpolation between our semantic and lexical similarity scores. To find the optimal combination weight α \\alpha italic_α , we perform a grid search on the SNLI training partition (1,000 examples), using BGE M3 as the embedder, a 9-shot prompt context per label, and aggregating scores across three independent judges. We cast each premise-candidate pair ( p , x ) (p,x) ( italic_p , italic_x ) as a binary relevance decision-positive if label ( x ) = label ( p ) \\mathrm{label}(x)=\\mathrm{label}(p) roman_label ( italic_x ) = roman_label ( italic_p ) , negative otherwise-and compute the combined score s comb ( p , x ) = α s ~ sem ( p , x ) + ( 1 − α ) s ~ lex ( p , x ) . s_{\\mathrm{comb}}(p,x)=\\alpha\\,\\tilde{s}_{\\mathrm{sem}}(p,x)+(1-\\alpha)\\,\\tilde{s}_{\\mathrm{lex}}(p,x). italic_s start_POSTSUBSCRIPT roman_comb end_POSTSUBSCRIPT ( italic_p , italic_x ) = italic_α over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT roman_sem end_POSTSUBSCRIPT ( italic_p , italic_x ) + ( 1 - italic_α ) over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT roman_lex end_POSTSUBSCRIPT ( italic_p , italic_x ) . For each α ∈ { 0 , 0.01 , 0.02 , … , 1.0 } \\alpha\\in\\{0,0.01,0.02,\\dots,1.0\\} italic_α ∈ { 0 , 0.01 , 0.02 , … , 1.0 } , we evaluate the area under the ROC curve (ROC AUC) across all positive/negative pairs. ROC AUC is a threshold-agnostic ranking metric that quantifies how well s comb s_{\\mathrm{comb}} italic_s start_POSTSUBSCRIPT roman_comb end_POSTSUBSCRIPT separates relevant from irrelevant examples; we identify α ∗ = 0.83 \\alpha^{*}=0.83 italic_α start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = 0.83 as the maximizer. At this setting, the ROC curve (Figure 4 ) achieves an AUC of 0.93. For all downstream experiments, we then fix α = 0.83 \\alpha=0.83 italic_α = 0.83 and set the few-shot size k = 1 k=1 italic_k = 1 per label-yielding a prompt context of three examples-to balance context richness with computational efficiency.",
                    "Figure 3: Variation of ROC AUC as a function of the semantic-lexical weighting parameter α \\alpha italic_α .",
                    "Figure 4: ROC curve at optimal α = 0.83 \\alpha=0.83 italic_α = 0.83 (AUC = 0.93)."
                ]
            },
            {
                "Avoiding Forgetness": [
                    "Fine-tuning a pretrained NLI model solely on adversarial examples 𝒟 adv \\mathcal{D}_{\\mathrm{adv}} caligraphic_D start_POSTSUBSCRIPT roman_adv end_POSTSUBSCRIPT can induce catastrophic forgetting : the model overfits the new distribution and its performance on the original SNLI data 𝒟 orig \\mathcal{D}_{\\mathrm{orig}} caligraphic_D start_POSTSUBSCRIPT roman_orig end_POSTSUBSCRIPT degrades. To prevent this, we blend original and adversarial data according to a mixing ratio",
                    "r = | 𝒟 orig | | 𝒟 adv | ∈ { 0 , 1 , 1 2 , 1 3 , 1 4 } , r\\;=\\;\\frac{\\bigl{|}\\mathcal{D}_{\\mathrm{orig}}\\bigr{|}}{\\bigl{|}\\mathcal{D}_{\\mathrm{adv}}\\bigr{|}}\\;\\in\\;\\Bigl{\\{}\\,0,\\;1,\\;\\frac{1}{2},\\;\\frac{1}{3},\\;\\frac{1}{4}\\Bigr{\\}}, italic_r = divide start_ARG | caligraphic_D start_POSTSUBSCRIPT roman_orig end_POSTSUBSCRIPT | end_ARG start_ARG | caligraphic_D start_POSTSUBSCRIPT roman_adv end_POSTSUBSCRIPT | end_ARG ∈ { 0 , 1 , divide start_ARG 1 end_ARG start_ARG 2 end_ARG , divide start_ARG 1 end_ARG start_ARG 3 end_ARG , divide start_ARG 1 end_ARG start_ARG 4 end_ARG } ,",
                    "where r = 0 r=0 italic_r = 0 indicates training exclusively on 𝒟 adv \\mathcal{D}_{\\mathrm{adv}} caligraphic_D start_POSTSUBSCRIPT roman_adv end_POSTSUBSCRIPT (i.e., no original data), and r = 1 4 r=\\frac{1}{4} italic_r = divide start_ARG 1 end_ARG start_ARG 4 end_ARG denotes one original SNLI example for every four adversarial examples.",
                    "For each retrieval mode m ∈ { sem , lex } m\\in\\{\\mathrm{sem},\\mathrm{lex}\\} italic_m ∈ { roman_sem , roman_lex } , we form the augmented training set",
                    "𝒟 ( m ) ( r ) = 𝒟 orig ∪ Sample ( 𝒟 adv ( m ) , | 𝒟 orig | / r ) \\mathcal{D}^{(m)}(r)=\\mathcal{D}_{\\mathrm{orig}}\\;\\cup\\;\\mathrm{Sample}\\!\\Bigl{(}\\mathcal{D}_{\\mathrm{adv}}^{(m)},\\,|\\mathcal{D}_{\\mathrm{orig}}|/r\\Bigr{)} caligraphic_D start_POSTSUPERSCRIPT ( italic_m ) end_POSTSUPERSCRIPT ( italic_r ) = caligraphic_D start_POSTSUBSCRIPT roman_orig end_POSTSUBSCRIPT ∪ roman_Sample ( caligraphic_D start_POSTSUBSCRIPT roman_adv end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_m ) end_POSTSUPERSCRIPT , | caligraphic_D start_POSTSUBSCRIPT roman_orig end_POSTSUBSCRIPT | / italic_r )",
                    "and fine-tune the model for T T italic_T iterations to obtain M m ( T ) M_{m}^{(T)} italic_M start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_T ) end_POSTSUPERSCRIPT . We then evaluate its overall accuracy on the combined SNLI, ANLI, and Multi-NLI benchmarks:",
                    "A m ( r ) = Accuracy ( M m ( T ) ∣ 𝒟 ( m ) ( r ) ) . A_{m}(r)\\;=\\;\\mathrm{Accuracy}\\bigl{(}M_{m}^{(T)}\\mid\\mathcal{D}^{(m)}(r)\\bigr{)}. italic_A start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ( italic_r ) = roman_Accuracy ( italic_M start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_T ) end_POSTSUPERSCRIPT ∣ caligraphic_D start_POSTSUPERSCRIPT ( italic_m ) end_POSTSUPERSCRIPT ( italic_r ) ) .",
                    "Figure 5 plots accuracies (after filtering with LLM-judges) of A BGE ( r ) A_{\\mathrm{BGE}}(r) italic_A start_POSTSUBSCRIPT roman_BGE end_POSTSUBSCRIPT ( italic_r ) , A BM25 ( r ) A_{\\mathrm{BM25}}(r) italic_A start_POSTSUBSCRIPT BM25 end_POSTSUBSCRIPT ( italic_r ) and A BGE + BM25 ( r ) A_{\\mathrm{BGE+BM25}}(r) italic_A start_POSTSUBSCRIPT roman_BGE + BM25 end_POSTSUBSCRIPT ( italic_r ) as functions of the adversarial-to-original ratio r r italic_r . All three curves climb steeply from r = 0 r=0 italic_r = 0 to r = 1 2 r=\\tfrac{1}{2} italic_r = divide start_ARG 1 end_ARG start_ARG 2 end_ARG , with BGE rising from 90.10% to 92.13% and BM25 from 90.09% to 92.00%. The combined BGE+BM25 strategy consistently outperforms either alone, increasing from 90.54% to 92.33% over the same range. Each curve reaches its maximum at r = 1 4 r=\\tfrac{1}{4} italic_r = divide start_ARG 1 end_ARG start_ARG 4 end_ARG , where the combined method peaks at 92.60%, indicating that one validated adversarial example per four originals strikes the best balance between robustness and retention. Beyond r = 1 4 r=\\tfrac{1}{4} italic_r = divide start_ARG 1 end_ARG start_ARG 4 end_ARG , further mixing yields only marginal gains.",
                    "Figure 5: Overall accuracy A m ( r ) A_{m}(r) italic_A start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ( italic_r ) on SNLI, ANLI, and Multi-NLI versus mixing ratio r r italic_r of generated adversarial examples to original SNLI, for BGE-generated (blue, dashed) and BM25-generated (orange, dash-dot) pipelines.",
                    "By selecting r ∗ = 1 4 r^{*}=\\frac{1}{4} italic_r start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = divide start_ARG 1 end_ARG start_ARG 4 end_ARG , we effectively mitigate catastrophic forgetting-preserving SNLI performance-while still reaping substantial adversarial robustness gains. This controlled mixing injects diversity into the training distribution and produces models that generalize reliably across both original and adversarial scenarios."
                ]
            }
        ],
        "Evaluation Setup: Models and Datasets": [
            "To evaluate the effectiveness of our adversarial RAG pipeline, we fine-tune and test a suite of models on three standard NLI benchmarks:",
            "Table 1: Accuracy (%) of RoBERTa-base on each test set in the zero‐shot setup only, compared to competitor methods and under adversarial mixing. “Filtered?” indicates unanimous LLM validation. Dataset RoBERTa- Base Additional Data Paraphrasing GNLI Method Filtered? 𝒓 = 𝟎 \\boldsymbol{r=0} bold_italic_r bold_= bold_0 𝒓 = 𝟏 : 𝟏 \\boldsymbol{r=1{:}1} bold_italic_r bold_= bold_1 bold_: bold_1 𝒓 = 𝟏 : 𝟐 \\boldsymbol{r=1{:}2} bold_italic_r bold_= bold_1 bold_: bold_2 𝒓 = 𝟏 : 𝟑 \\boldsymbol{r=1{:}3} bold_italic_r bold_= bold_1 bold_: bold_3 𝒓 = 𝟏 : 𝟒 \\boldsymbol{r=1{:}4} bold_italic_r bold_= bold_1 bold_: bold_4 SNLI 88.48% 89.42% 84.73% – BGE-generated No 90.98% 91.17% 91.51% 91.54% 91.55% – BGE-generated Yes 90.10% 91.21% 92.13% 92.12% 92.13% – BM25-generated No 90.03% 91.02% 91.14% 91.18% 91.19% – BM25-generated Yes 90.09% 91.20% 92.00% 92.11% 92.12% – BGE+BM25-generated No 90.11% 91.19% 91.35% 91.61% 91.68% – BGE+BM25-generated Yes 90.54% 90.78% 92.33% 92.41% 92.60% – T5-Small - - - - - - – T5-Large - - - - - - – T5-XXL - - - - - - Adversarial NLI 75.04% 77.07% 72.39% – BGE-generated No 79.07% 79.72% 79.52% 79.92% 79.47% – BGE-generated Yes 78.72% 79.12% 80.02% 79.72% 80.27% – BM25-generated No 78.07% 78.52% 78.72% 78.82% 78.88% – BM25-generated Yes 77.97% 78.62% 78.92% 79.07% 79.12% – BGE+BM25-generated No 78.11% 79.18% 78.51% 78.99% 78.91% – BGE+BM25-generated Yes 79.12% 80.43% 80.67% 80.89% 80.95% 33.00% T5-Small - - - - - - 45.72% T5-Large - - - - - - 57.87% T5-XXL - - - - - - MultiNLI 54.67% 57.61% 50.01% – BGE-generated No 69.54% 69.32% 70.22% 69.72% 71.08% – BGE-generated Yes 69.15% 69.62% 70.22% 69.97% 71.15% – BM25-generated No 68.34% 68.72% 68.92% 69.22% 69.54% – BM25-generated Yes 68.57% 68.82% 69.12% 69.47% 69.74% – BGE+BM25-generated No 68.05% 68.15% 68.81% 69.11% 70.02% – BGE+BM25-generated Yes 69.21% 69.37% 70.59% 69.81% 71.99% 82.18% T5-Small - - - - - - 90.61% T5-Large - - - - - - 91.77% T5-XXL - - - - - -",
            "• Target NLI Model: RoBERTa-base-SNLI (125M parameters) (HuggingFace 2022 ) , a RoBERTa variant pretrained on SNLI. • Generation LLM: Adversarial hypotheses are generated with Llama-4-Scout-17B-16E-Instruct (Meta AI 2025 ) . • Validation LLMs: Each candidate pair is vetted by an ensemble of three models: – Gemma-3-27B-IT (Google Research 2025 ) , – Phi-4 (Microsoft Research 2025 ) , and – Qwen3-32B (Qwen Team 2025 ) .",
            "We report results on the following NLI test sets:",
            "• SNLI test (Bowman et al. 2015 ) , the original human-annotated NLI benchmark. • ANLI (Nie et al. 2019 ) , featuring adversarial examples collected via human-in-the-loop attacks. • Multi-NLI (Williams, Nangia, and Bowman 2018 ) , a multi-genre corpus for evaluating cross-domain generalization."
        ],
        "Results": [
            "To contextualize these gains, we compare against models fine‐tuned solely on GNLI (Hosseini et al. 2024 ) , a synthetic NLI corpus of roughly 685K LLM‐generated examples.",
            "Even with only GNLI as extra supervision, RoBERTa‐Base reaches 89.42% on SNLI, 77.07% on ANLI, and 57.61% on MultiNLI. In contrast, our VAULT pipeline first generates approximately 30K adversarial candidates per retrieval strategy, then-after human validation-retains 6637 BGE‐generated and 5991 BM25‐generated examples. Injecting these targeted examples at a 1:4 ratio boosts RoBERTa-Base from 88.48% to 92.60% on SNLI, from 75.04% to 80.95% on ANLI, and from 54.67% to 71.99% on MultiNLI (Table 1 ).",
            "Table 1 also shows that even unfiltered data yields substantial gains-rising to 91.55% on SNLI at r = 1 4 r=\\frac{1}{4} italic_r = divide start_ARG 1 end_ARG start_ARG 4 end_ARG -but filtering consistently adds further improvement. Overall, a small, focused, and validated adversarial set-particularly from BGE-outperforms massive, untargeted corpora across all three benchmarks.",
            "Table 2 and Figure 6 report few-shot accuracy of our generation methods (BGE- and BM25-generated, with and without filtering) on SNLI, Adversarial NLI, and MultiNLI. Accuracy grows steadily with more in-context examples: on SNLI, unfiltered BGE rises from 87.51% at 0-shot to 91.56% at 9-shot, while filtered BGE improves from 88.18% to 92.15%, and unfiltered BM25 climbs from 87.51% to 91.22% (filtered BM25: 88.18% to 92.11%). On Adversarial NLI, filtered BGE goes from 76.27% at 0-shot to 80.26% at 9-shot, whereas unfiltered BM25 peaks at only 78.88% at 9-shot. For MultiNLI, filtered BGE again leads-growing from 67.87% to 71.15%-with unfiltered BM25 topping out at 69.54% by 9-shot (filtered BM25: 67.87 to 69.74%). Notably, 6-shot performance matches or exceeds the 1:4 mixing results in Table 1 , showing that a handful of targeted few-shot examples captures most of the gains. Overall, a small set of in-context examples-especially filtered BGE-delivers consistent improvements across all three benchmarks.",
            "Table 2: Few‐shot accuracy (%) of our generation methods on each test set. Columns indicate the number of few‐shot examples; the 6‐shot column reproduces the r = 1 : 4 r=1{:}4 italic_r = 1 : 4 results from Table 1 . Bold indicates the best performance per row. Dataset Method Filtered? 0‐shot 3‐shot 6‐shot 9‐shot SNLI BGE‐generated No 87.51% 90.05% 91.55% 91.56% BGE‐generated Yes 88.18% 90.69% 92.13% 92.15% BM25‐generated No 87.51% 89.69% 91.19% 91.22% BM25‐generated Yes 88.18% 90.67% 92.12% 92.11% BGE+BM25‐generated No 87.51% 89.98% 91.68% 91.51% BGE+BM25‐generated Yes 88.18% 90.71% 92.60% 92.51% Adversarial NLI BGE‐generated No 75.81% 77.72% 79.47% 79.47% BGE‐generated Yes 76.27% 78.76% 80.27% 80.26% BM25‐generated No 75.81% 77.37% 78.87% 78.88% BM25‐generated Yes 76.27% 77.60% 79.12% 79.10% BGE+BM25‐generated No 75.81% 77.71% 78.81% 78.91% BGE+BM25‐generated Yes 76.27% 77.81% 78.95% 80.95% MultiNLI BGE‐generated No 67.18% 69.25% 71.07% 71.08% BGE‐generated Yes 67.87% 69.02% 71.12% 71.15% BM25‐generated No 67.18% 68.07% 69.57% 69.54% BM25‐generated Yes 67.87% 68.22% 69.72% 69.74% BGE+BM25‐generated No 67.18% 69.42% 69.99% 70.02% BGE+BM25‐generated Yes 67.87% 71.00% 71.12% 71.99%",
            "Figure 6: Few-shot accuracy of generation methods by dataset."
        ],
        "Conclusion": [
            "In this work, we introduce VAULT , an adversarially‑driven data augmentation framework that cuts reliance on massive synthetic corpora while matching-or often surpassing-state‑of‑the‑art performance. Rather than fine‑tuning on hundreds of thousands of examples (e.g. 685 K in GNLI), VAULT generates ∼ \\sim ∼ 30 K adversarial candidates per retrieval strategy, validates them, and retains just 6-6.6 K samples. This lean approach yields a 4-7 point gain over GNLI‑only baselines in zero‑ and few‑shot settings on SNLI, ANLI, and MultiNLI despite using an order of magnitude less data. TF‑IDF and BERTScore analyses confirm these focused sets preserve both lexical overlap and semantic fidelity. In future work, we’ll explore automated validation heuristics, extensions to other NLI domains, and combinations of adversarial augmentation with model‑centric techniques for even greater efficiency."
        ],
        "Limitations and Future Work": [
            "While VAULT demonstrates strong gains with minimal synthetic data, it does rely on large‐scale LLMs both for generation (Llama-4-Scout-17B-16E-Instruct) and validation (Gemma-3-27B-IT, Phi-4, Qwen3-32B), which incurs nontrivial compute cost and may limit applicability in resource‐constrained settings. The unanimous‐agreement filtering criterion, while effective at ensuring high‐quality examples, may discard borderline cases that could further diversify training. Additionally, our experiments focus on English NLI benchmarks; extending to multilingual or specialized domains may require adaptation of retrieval strategies and validation ensembles. Future work will explore lightweight validation alternatives (e.g. smaller ensemble members or learned filters), adaptive retrieval budgets that allocate more examples to harder premises, and automated calibration of filtering thresholds. We also plan to evaluate VAULT in continual learning scenarios, where adversarial candidates are generated on the fly as new data arrives, and to investigate integration with model‐centric robustness techniques such as contrastive fine‐tuning and adversarial regularization."
        ],
        "Appendixes": [
            {
                "Judge Ensemble Configuration": [
                    "With the retrieval weight fixed at α = 0.83 \\alpha=0.83 italic_α = 0.83 and the generated‐to‐original example ratio set to 1:4, we evaluated the impact of varying the number of “judges” (independent LLM validators) on downstream accuracy. All experiments were run on the SNLI test set. We filtered examples by requiring unanimous agreement among the selected judges and then measured classification accuracy on the remaining items.",
                    "# Judges # Examples Accuracy (%) Judges 1 16,147 91.02 G 2 9,312 91.49 G + Q 3 6,438 92.13 G + Q + P Table 3: Filtering and accuracy under different judge ensemble sizes (SNLI test, 1:4 gen:orig, α = 0.83 \\alpha=0.83 italic_α = 0.83 ). Judges: G = Gemma-3-27B-IT (Google Research 2025 ) , Q = Qwen3-32B (Qwen Team 2025 ) , P = Phi-4 (Microsoft Research 2025 ) .",
                    "As shown in Table 3 and Figure 7 , the three-judge ensemble yields the highest accuracy (92.13%) on 6,438 filtered observations. Both the two-judge and single-judge configurations retain more examples but achieve lower accuracies of 91.49% (9,312 examples) and 91.02% (16,147 examples), respectively. Gemma-3-27B-IT consistently remains in all configurations, with Qwen3-32B joining for the two-judge setup and Phi-4 for the three-judge ensemble. We adopt the three-judge configuration for all subsequent evaluations.",
                    "Figure 7: Accuracy vs. number of judges (SNLI test, α = 0.83 \\alpha=0.83 italic_α = 0.83 , 1:4 generated:original). Points are annotated with the number of filtered examples."
                ]
            },
            {
                "Dataset Comparison": [
                    "To gain insights into the relationship between the data generated in our experiment and existing benchmarks, we first extracted the 10 most frequent non-stopwords from each dataset. This qualitative analysis highlights topical overlap and domain shifts. To quantify similarity more rigorously, we computed two complementary metrics across seven collections-SNLI Train, BGE-generated, BM25-generated, SNLI Test, Adversarial NLI, Multi-NLI, and our combined BGE+BM25-generated set: TF-IDF cosine similarity and BERTScore F1 (Zhang et al. 2019 ) .",
                    "TF-IDF Cosine Similarity. Let each dataset D D italic_D be represented by a TF-IDF vector 𝐯 D ∈ ℝ n \\mathbf{v}_{D}\\in\\mathbb{R}^{n} bold_v start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , where n n italic_n is the vocabulary size and the i i italic_i th component is v D , i = TF D , i ⋅ log ⁡ ( N DF i ) , v_{D,i}=\\mathrm{TF}_{D,i}\\cdot\\log\\!\\bigl{(}\\tfrac{N}{\\mathrm{DF}_{i}}\\bigr{)}, italic_v start_POSTSUBSCRIPT italic_D , italic_i end_POSTSUBSCRIPT = roman_TF start_POSTSUBSCRIPT italic_D , italic_i end_POSTSUBSCRIPT ⋅ roman_log ( divide start_ARG italic_N end_ARG start_ARG roman_DF start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ) , with TF D , i \\mathrm{TF}_{D,i} roman_TF start_POSTSUBSCRIPT italic_D , italic_i end_POSTSUBSCRIPT the term frequency in D D italic_D , N N italic_N the total number of datasets, and DF i \\mathrm{DF}_{i} roman_DF start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT the number of datasets containing term i i italic_i . We then define sim TFIDF ( D , D ′ ) = 𝐯 D ⋅ 𝐯 D ′ ‖ 𝐯 D ‖ ‖ 𝐯 D ′ ‖ . \\mathrm{sim}_{\\mathrm{TFIDF}}(D,D^{\\prime})=\\frac{\\mathbf{v}_{D}\\cdot\\mathbf{v}_{D^{\\prime}}}{\\|\\mathbf{v}_{D}\\|\\;\\|\\mathbf{v}_{D^{\\prime}}\\|}. roman_sim start_POSTSUBSCRIPT roman_TFIDF end_POSTSUBSCRIPT ( italic_D , italic_D start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) = divide start_ARG bold_v start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ⋅ bold_v start_POSTSUBSCRIPT italic_D start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_ARG start_ARG ∥ bold_v start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ∥ ∥ bold_v start_POSTSUBSCRIPT italic_D start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ∥ end_ARG . Figure 8 shows the resulting 7 × 7 7\\times 7 7 × 7 matrix. Notably, the combined BGE+BM25 set has a TF-IDF similarity of approximately 0.0251 with SNLI Train, 0.0188 with SNLI Test, and 0.0150 with Multi-NLI-intermediate between its BGE-only and BM25-only counterparts.",
                    "BERTScore F1. We next measure semantic overlap by applying BERTScore F1, which aligns token embeddings from a pre-trained transformer and computes an F 1 F_{1} italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT score: P = 1 | x | ∑ t ∈ x max s ∈ y ⁡ cos ( 𝐞 t , 𝐞 s ) , R = 1 | y | ∑ s ∈ y max t ∈ x ⁡ cos ( 𝐞 s , 𝐞 t ) , \\mathrm{P}=\\frac{1}{|x|}\\sum_{t\\in x}\\max_{s\\in y}\\mathrm{cos}(\\mathbf{e}_{t},\\mathbf{e}_{s}),\\mathrm{R}=\\frac{1}{|y|}\\sum_{s\\in y}\\max_{t\\in x}\\mathrm{cos}(\\mathbf{e}_{s},\\mathbf{e}_{t}), roman_P = divide start_ARG 1 end_ARG start_ARG | italic_x | end_ARG ∑ start_POSTSUBSCRIPT italic_t ∈ italic_x end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_s ∈ italic_y end_POSTSUBSCRIPT roman_cos ( bold_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_e start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) , roman_R = divide start_ARG 1 end_ARG start_ARG | italic_y | end_ARG ∑ start_POSTSUBSCRIPT italic_s ∈ italic_y end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_t ∈ italic_x end_POSTSUBSCRIPT roman_cos ( bold_e start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , bold_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , F1 = 2 ⋅ P R P + R , \\mathrm{F1}=2\\cdot\\frac{\\mathrm{P}\\,\\mathrm{R}}{\\mathrm{P}+\\mathrm{R}}, F1 = 2 ⋅ divide start_ARG roman_P roman_R end_ARG start_ARG roman_P + roman_R end_ARG , where x , y x,y italic_x , italic_y are token sequences from two datasets and 𝐞 \\mathbf{e} bold_e are contextual embeddings. Figure 9 displays the 7 × 7 7\\times 7 7 × 7 BERTScore F1 matrix. The combined set scores about 0.8658 with SNLI Train, 0.8534 with SNLI Test, 0.8458 with Adversarial NLI, and 0.8554 with Multi-NLI, again falling between its BGE-only and BM25-only pairs. These results confirm that our validated adversarial examples share both lexical and semantic patterns with standard NLI benchmarks, while still introducing novel, challenging variations.",
                    "Figure 8: Pairwise TF-IDF cosine similarity between datasets.",
                    "Figure 9: Pairwise BERTScore F1 between datasets.",
                    "From Figure 8 , we see that both BGE‐ and BM25‐generated data share moderate lexical overlap with the original SNLI Train set (cosine similarities around 0.02-0.03), but diverge more substantially from the Adversarial NLI and Multi-NLI benchmarks. In contrast, Figure 9 shows that semantically these generated datasets align much more closely with SNLI Train and SNLI Test (BERTScore F1 values above 0.85), indicating that although the surface vocabulary varies, the core contextual meaning is well preserved."
                ]
            },
            {
                "Generated Dataset Characteristics and Hypothesis Lengths": [
                    "We first examined the most frequent tokens in each corpus to identify thematic patterns. In the SNLI train (Bowman et al. 2015 ) and SNLI test (Bowman et al. 2015 ) sets, words like “man,” “woman,” and “people” dominate, reflecting descriptions of social interactions. The Adversarial NLI dataset (Nie et al. 2019 ) shifts focus to media and chronology, with top tokens such as “film,” “first,” and “scene,” while the Multi-NLI test set (Williams, Nangia, and Bowman 2018 ) uses more abstract, domain-diverse language-terms like “author,” “context,” and “claim” appear frequently.",
                    "Figure 10: Comparison of average hypothesis lengths (in characters and words) across datasets: Generated-BM25 , Generated-BGE , SNLI train (Bowman et al. 2015 ) , SNLI test (Bowman et al. 2015 ) , Adversarial NLI (Nie et al. 2019 ) , and Multi-NLI (Williams, Nangia, and Bowman 2018 ) .",
                    "Turning to our three LLM‑generated sets- Generated-BM25 , Generated-BGE and BGE+BM25 -we again see a high incidence of speculative and gender‑related terms (“could,” “would,” “woman,” “he,” “she”), confirming that all retrieval strategies surface similar thematic content with only minor stylistic differences.",
                    "Figure 10 compares the average hypothesis lengths across all seven datasets. Each of the generated sets produces the longest hypotheses-around 98-100 characters (16-17 words)-demonstrating the LLM’s tendency toward more elaborate constructions when given rich few‑shot contexts. By contrast, the SNLI train and SNLI test annotations remain quite concise ( ≈ 37 \\approx 37 ≈ 37 - 38 38 38 characters, 7-8 words), reflecting the brevity of human‑written examples. The Adversarial NLI instances average ≈ 64 \\approx 64 ≈ 64 characters (11 words), and the Multi‑NLI examples average ≈ 56 \\approx 56 ≈ 56 characters (10 words), underscoring their intermediate complexity. These length patterns highlight how our adversarial RAG pipeline generates richer, more challenging hypotheses while preserving diversity across data sources."
                ]
            },
            {
                "Retrieval Accuracy Across Similarity Metrics": [
                    "For purely lexical retrieval we employ BM25 with parameters k 1 = 1.5 k_{1}=1.5 italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 1.5 and b = 0.75 b=0.75 italic_b = 0.75 . The BM25 score for a query p p italic_p and document x x italic_x is given by s BM25 ( p , x ) = ∑ t ∈ p IDF ( t ) tf ( t , x ) ( k 1 + 1 ) tf ( t , x ) + k 1 ( 1 − b + b | x | avgdl ) , s_{\\mathrm{BM25}}(p,x)=\\sum_{t\\in p}\\mathrm{IDF}(t)\\,\\frac{\\mathrm{tf}(t,x)\\,(k_{1}+1)}{\\mathrm{tf}(t,x)+k_{1}\\!\\Bigl{(}1-b+b\\,\\tfrac{|x|}{\\mathrm{avgdl}}\\Bigr{)}}, italic_s start_POSTSUBSCRIPT BM25 end_POSTSUBSCRIPT ( italic_p , italic_x ) = ∑ start_POSTSUBSCRIPT italic_t ∈ italic_p end_POSTSUBSCRIPT roman_IDF ( italic_t ) divide start_ARG roman_tf ( italic_t , italic_x ) ( italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + 1 ) end_ARG start_ARG roman_tf ( italic_t , italic_x ) + italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( 1 - italic_b + italic_b divide start_ARG | italic_x | end_ARG start_ARG roman_avgdl end_ARG ) end_ARG , (4) and for each label y ′ y^{\\prime} italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT we retrieve the top‑ k k italic_k documents 𝒞 p lex ( y ′ ) = arg ⁡ max S ⊆ 𝒟 y ′ | S | = k ∑ x ∈ S s BM25 ( p , x ) . \\mathcal{C}_{p}^{\\mathrm{lex}}(y^{\\prime})=\\arg\\max_{\\begin{subarray}{c}S\\subseteq\\mathcal{D}_{y^{\\prime}}\\\\ |S|=k\\end{subarray}}\\sum_{x\\in S}s_{\\mathrm{BM25}}(p,x). caligraphic_C start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_lex end_POSTSUPERSCRIPT ( italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) = roman_arg roman_max start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_S ⊆ caligraphic_D start_POSTSUBSCRIPT italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL | italic_S | = italic_k end_CELL end_ROW end_ARG end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_x ∈ italic_S end_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT BM25 end_POSTSUBSCRIPT ( italic_p , italic_x ) . (5)",
                    "For embedding‑based retrieval, we first compute cosine similarity S cos ( E I , E 𝒟 ) = E I ⋅ E 𝒟 ‖ E I ‖ 2 ‖ E 𝒟 ‖ 2 , S_{\\cos}(E_{I},E_{\\mathcal{D}})=\\frac{E_{I}\\cdot E_{\\mathcal{D}}}{\\|E_{I}\\|_{2}\\,\\|E_{\\mathcal{D}}\\|_{2}}, italic_S start_POSTSUBSCRIPT roman_cos end_POSTSUBSCRIPT ( italic_E start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ) = divide start_ARG italic_E start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ⋅ italic_E start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT end_ARG start_ARG ∥ italic_E start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∥ italic_E start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG , (6) and raw dot product S dp ( E I , E 𝒟 ) = E I ⋅ E 𝒟 = ∑ i = 1 d ( E I ) i ( E 𝒟 ) i . S_{\\mathrm{dp}}(E_{I},E_{\\mathcal{D}})=E_{I}\\cdot E_{\\mathcal{D}}=\\sum_{i=1}^{d}(E_{I})_{i}\\,(E_{\\mathcal{D}})_{i}. italic_S start_POSTSUBSCRIPT roman_dp end_POSTSUBSCRIPT ( italic_E start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ) = italic_E start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ⋅ italic_E start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ( italic_E start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_E start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . (7)",
                    "We additionally assess two norm‑based distances: the L 2 L_{2} italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT distance d 2 ( E I , E 𝒟 ) = ‖ E I − E 𝒟 ‖ 2 = ∑ i = 1 d ( ( E I ) i − ( E 𝒟 ) i ) 2 , d_{2}(E_{I},E_{\\mathcal{D}})=\\|E_{I}-E_{\\mathcal{D}}\\|_{2}=\\sqrt{\\sum_{i=1}^{d}\\bigl{(}(E_{I})_{i}-(E_{\\mathcal{D}})_{i}\\bigr{)}^{2}}, italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_E start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ) = ∥ italic_E start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT - italic_E start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = square-root start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ( ( italic_E start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - ( italic_E start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG , (8) and the L 1 L_{1} italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT distance d 1 ( E I , E 𝒟 ) = ‖ E I − E 𝒟 ‖ 1 = ∑ i = 1 d | ( E I ) i − ( E 𝒟 ) i | . d_{1}(E_{I},E_{\\mathcal{D}})=\\|E_{I}-E_{\\mathcal{D}}\\|_{1}=\\sum_{i=1}^{d}\\bigl{|}(E_{I})_{i}-(E_{\\mathcal{D}})_{i}\\bigr{|}. italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_E start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ) = ∥ italic_E start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT - italic_E start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT | ( italic_E start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - ( italic_E start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | . (9)",
                    "Finally, to capture distributional discrepancies we examine the Bray-Curtis distance d BC ( E I , E 𝒟 ) = ∑ i = 1 d | ( E I ) i − ( E 𝒟 ) i | ∑ i = 1 d | ( E I ) i + ( E 𝒟 ) i | , d_{\\mathrm{BC}}(E_{I},E_{\\mathcal{D}})=\\frac{\\sum_{i=1}^{d}\\bigl{|}(E_{I})_{i}-(E_{\\mathcal{D}})_{i}\\bigr{|}}{\\sum_{i=1}^{d}\\bigl{|}(E_{I})_{i}+(E_{\\mathcal{D}})_{i}\\bigr{|}}, italic_d start_POSTSUBSCRIPT roman_BC end_POSTSUBSCRIPT ( italic_E start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ) = divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT | ( italic_E start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - ( italic_E start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT | ( italic_E start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + ( italic_E start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | end_ARG , (10) and the Canberra distance d Can ( E I , E 𝒟 ) = ∑ i = 1 d | ( E I ) i − ( E 𝒟 ) i | | ( E I ) i | + | ( E 𝒟 ) i | . d_{\\mathrm{Can}}(E_{I},E_{\\mathcal{D}})=\\sum_{i=1}^{d}\\frac{\\bigl{|}(E_{I})_{i}-(E_{\\mathcal{D}})_{i}\\bigr{|}}{\\bigl{|}(E_{I})_{i}\\bigr{|}+\\bigl{|}(E_{\\mathcal{D}})_{i}\\bigr{|}}. italic_d start_POSTSUBSCRIPT roman_Can end_POSTSUBSCRIPT ( italic_E start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ) = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT divide start_ARG | ( italic_E start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - ( italic_E start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | end_ARG start_ARG | ( italic_E start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | + | ( italic_E start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | end_ARG . (11)",
                    "Figure 11: Retrieval accuracy (%) by similarity metric for BGE+BM25, BM25, and BGE.",
                    "Figure 11 demonstrates that BGE+BM25 outperforms both BM25 alone and BGE alone across all six metrics, achieving 92.60 % 92.60\\% 92.60 % (cosine), 89.85 % 89.85\\% 89.85 % (dot product), 85.43 % 85.43\\% 85.43 % ( L 2 L_{2} italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ), 85.22 % 85.22\\% 85.22 % ( L 1 L_{1} italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ), 79.21 % 79.21\\% 79.21 % (Bray-Curtis) and 79.12 % 79.12\\% 79.12 % (Canberra). Pure BM25 and pure BGE match closely on cosine but degrade more sharply on norm‑ and distribution‑based distances, confirming the robustness of the hybrid lexical‑semantic approach."
                ]
            },
            {
                "Example - Few‑Shot Chat Sequence": [
                    "BGE based retrieval",
                    "Example 1: Few‑Shot Retrieval & Model Return Shot 1 Premise : A blond little girl enjoying a burrito. Label : entailment. Hypothesis: The girl ate a burrito. Shot 2 Premise : A young blond girl sitting down while eating. Label : entailment. Hypothesis: The girl has food. Shot 3 Premise : A blond little girl enjoying a burrito. Label : neutral. Hypothesis: The hungry girl ate a burrito at the restaurant. Shot 4 Premise : A young blond girl sitting down while eating. Label : neutral. Hypothesis: The girl is eating at a picnic. Shot 5 Premise : A blond little girl enjoying a burrito. Label : contradiction. Hypothesis: The brunette girl didn’t like the burrito. Shot 6 Premise : A young blond girl sitting down while eating. Label : contradiction. Hypothesis: The girl runs all over her house while eating because she can never sit down. Llama Generation User: Now generate a one‑sentence hypothesis that contradicts the premise above. Return only the hypothesis without narration. Assistant (Llama): The girl threw the sandwich away.",
                    "The chat sequences below present a clear few‑shot retrieval sequence for a natural language inference task. They illustrate six premise-hypothesis pairs-two each for entailment, neutral, and contradiction-and conclude with a concise model prompt. This format makes the example selection process transparent and highlights the model’s reasoning in a single, easily readable block. These examples are based solely on BGE retrieval.",
                    "Example 2: Few‑Shot Retrieval & Model Return Shot 1 Premise : An older man is drinking orange juice at a restaurant. Label : entailment. Hypothesis: A man is drinking juice. Shot 2 Premise : An older man is drinking orange juice at a restaurant. Label : entailment. Hypothesis: A man is drinking juice. Shot 3 Premise : An older man sits with his orange juice at a small table in a coffee shop while employees in bright colored shirts smile in the background. Label : neutral. Hypothesis: An elderly man sits in a small shop. Shot 4 Premise : An older man sits with his orange juice at a small table in a coffee shop while employees in bright colored shirts smile in the background. Label : neutral. Hypothesis: An older man drinks his juice as he waits for his daughter to get off work. Shot 5 Premise : An older man sits with his orange juice at a small table in a coffee shop while employees in bright colored shirts smile in the background. Label : contradiction. Hypothesis: A boy flips a burger. Shot 6 Premise : An older man is drinking orange juice at a restaurant. Label : contradiction. Hypothesis: Two women are at a restaurant drinking wine. Llama Generation User: Now generate a one‑sentence hypothesis that contradicts the premise above. Return only the hypothesis without narration. Assistant (Llama): The coffee shop is empty.",
                    "Example 3: Few‑Shot Retrieval & Model Return Shot 1 Premise : A few people in a restaurant setting, one of them is drinking orange juice. Label : entailment. Hypothesis: The diners are at a restaurant. Shot 2 Premise : A few people in a restaurant setting, one of them is drinking orange juice. Label : entailment. Hypothesis: The diners are at a restaurant. Shot 3 Premise : A few people in a restaurant setting, one of them is drinking orange juice. Label : neutral. Hypothesis: The people are eating omelettes. Shot 4 Premise : A few people in a restaurant setting, one of them is drinking orange juice. Label : neutral. Hypothesis: The people are eating omelettes. Shot 5 Premise : A few people in a restaurant setting, one of them is drinking orange juice. Label : contradiction. Hypothesis: The people are sitting at desks in school. Shot 6 Premise : A few people are observing the orange juice section at the grocery store. Label : contradiction. Hypothesis: The people are at a baseball stadium. Llama Generation User: Now generate a one‑sentence hypothesis that contradicts the premise above. Return only the hypothesis without narration. Assistant (Llama): The people are at a concert.",
                    "",
                    {
                        "Optimized BGE + BM25 Retrieval with Tuned Alpha": [
                            "The paragraph below describes an optimized retrieval approach that combines semantic search using BGE embeddings with lexical scoring via BM25. By introducing a tunable alpha parameter, the method linearly interpolates between embedding-based similarity and keyword matching-enabling a dynamic balance that maximizes relevance across diverse query types. Through cross‑validation, the optimal alpha value was determined to achieve superior retrieval performance compared to either technique alone.",
                            "Example 1: Few‑Shot Retrieval & Model Return Shot 1 Premise : A person on a horse jumps over a broken down airplane. Label : entailment. Hypothesis: A person is outdoors, on a horse. Shot 2 Premise : A person on a horse jumps over a broken down airplane. Label : entailment. Hypothesis: A person is outdoors, on a horse. Shot 3 Premise : A person on a horse jumps over a broken down airplane. Label : neutral. Hypothesis: A person is training his horse for a competition. Shot 4 Premise : A person is riding a horse, which is jumping over a grounded airplane. Label : neutral. Hypothesis: The horse jumps over a toy airplane. Shot 5 Premise : A person on a horse jumps over a broken down airplane. Label : contradiction. Hypothesis: A person is at a diner, ordering an omelette. Shot 6 Premise : A person on a horse jumps over a broken down airplane. Label : contradiction. Hypothesis: A person is at a diner, ordering an omelette. Llama Generation User: Now generate a one‑sentence hypothesis that is neutral with the premise above. Return only the hypothesis without narration. Assistant (Llama): The horse is very well trained.",
                            "Example 2: Few‑Shot Retrieval & Model Return Shot 1 Premise : Children smiling and waving at camera. Label : entailment. Hypothesis: There are children present. Shot 2 Premise : Children smiling and waving at camera. Label : entailment. Hypothesis: There are children present. Shot 3 Premise : Children smiling and waving at camera. Label : neutral. Hypothesis: They are smiling at their parents. Shot 4 Premise : Children smiling and waving at camera. Label : neutral. Hypothesis: They are smiling at their parents. Shot 5 Premise : Children smiling and waving at camera. Label : contradiction. Hypothesis: The kids are frowning. Shot 6 Premise : Children smiling and making faces to the camera. Label : contradiction. Hypothesis: The children are deceased. Llama Generation User: Now generate a one‑sentence hypothesis that contradicts the premise above. Return only the hypothesis without narration. Assistant (Llama): The children are asleep.",
                            "Example 3: Few‑Shot Retrieval & Model Return Shot 1 Premise : An older man is drinking orange juice at a restaurant. Label : entailment. Hypothesis: A man is drinking juice. Shot 2 Premise : An older man is drinking orange juice at a restaurant. Label : entailment. Hypothesis: A man is drinking juice. Shot 3 Premise : An older man is drinking orange juice at a restaurant. Label : neutral. Hypothesis: A man in a restaurant is waiting for his meal to arrive. Shot 4 Premise : An elderly man is drinking orange juice at a cafe. Label : neutral. Hypothesis: An older gentleman is enjoying his orange juice at a new cafe. Shot 5 Premise : An older man is drinking orange juice at a restaurant. Label : contradiction. Hypothesis: Two women are at a restaurant drinking wine. Shot 6 Premise : An older man is drinking orange juice at a restaurant. Label : contradiction. Hypothesis: Two women are at a restaurant drinking wine. Llama Generation User: Now generate a one‑sentence hypothesis that is neutral with the premise above. Return only the hypothesis without narration. Assistant (Llama): The restaurant has a lot of tables.",
                            ""
                        ]
                    },
                    {
                        "BM25 based retrieval": [
                            "The retrieval process relies exclusively on the BM25 algorithm, which computes scores based on term frequency, document frequency, and document length normalization. By focusing on exact keyword matches and weighting rarer terms more heavily, BM25 effectively ranks documents that share the most relevant vocabulary with the query. This approach is well-suited for tasks where precise lexical overlap is paramount and semantic generalization is less critical. However, it may miss contextually related content when synonyms or paraphrases are used.",
                            "Example 1: Few-Shot Retrieval & Model Return Shot 1 Premise : An older man is drinking orange juice at a restaurant. Label : entailment. Hypothesis: A man is drinking juice. Shot 2 Premise : An elderly man is drinking orange juice at a cafe. Label : entailment. Hypothesis: An old man is enjoying a beverage at a cafe. Shot 3 Premise : An older man is drinking orange juice at a restaurant. Label : neutral. Hypothesis: A man in a restaurant is waiting for his meal to arrive. Shot 4 Premise : An older man is drinking orange juice at a restaurant. Label : neutral. Hypothesis: A man in a restaurant is waiting for his meal to arrive. Shot 5 Premise : An older man is drinking orange juice at a restaurant. Label : contradiction. Hypothesis: Two women are at a restaurant drinking wine. Shot 6 Premise : An older man is drinking orange juice at a restaurant. Label : contradiction. Hypothesis: Two women are at a restaurant drinking wine. Llama Generation User: Now generate a one-sentence hypothesis that entails the premise above. Return only the hypothesis without narration. Assistant (Llama): A man is drinking orange juice at a restaurant.",
                            "Example 2: Few‑Shot Retrieval & Model Return Shot 1 Premise : An older man is drinking orange juice at a restaurant. Label : entailment. Hypothesis: A man is drinking juice. Shot 2 Premise : An older man is drinking orange juice at a restaurant. Label : entailment. Hypothesis: A man is drinking juice. Shot 3 Premise : An older man is drinking orange juice at a restaurant. Label : neutral. Hypothesis: A man in a restaurant is waiting for his meal to arrive. Shot 4 Premise : An older man is drinking orange juice at a restaurant. Label : neutral. Hypothesis: A man in a restaurant is waiting for his meal to arrive. Shot 5 Premise : An older man is drinking orange juice at a restaurant. Label : contradiction. Hypothesis: Two women are at a restaurant drinking wine. Shot 6 Premise : An elderly man is drinking orange juice at a cafe. Label : contradiction. Hypothesis: An elderly man is drinking apple juice at a bar. Llama Generation User: Now generate a one‑sentence hypothesis that contradicts the premise above. Return only the hypothesis without narration. Assistant (Llama): The man is drinking coffee.",
                            "Example 3: Few‑Shot Retrieval & Model Return Shot 1 Premise : A woman in a green jacket and hood over her head looking towards a valley. Label : entailment. Hypothesis: The woman is wearing green. Shot 2 Premise : A woman in a green jacket and hood over her head looking towards a valley. Label : entailment. Hypothesis: The woman is wearing green. Shot 3 Premise : A woman in a green jacket and hood over her head looking towards a valley. Label : neutral. Hypothesis: The woman is cold. Shot 4 Premise : A woman gazes over the valley below. Label : neutral. Hypothesis: she looks at the valley she owns. Shot 5 Premise : A woman in a green jacket and hood over her head looking towards a valley. Label : contradiction. Hypothesis: The woman is nake. Shot 6 Premise : A woman in a green jacket and hood over her head looking towards a valley. Label : contradiction. Hypothesis: The woman is nake. Llama Generation User: Now generate a one‑sentence hypothesis that is neutral with the premise above. Return only the hypothesis without narration. Assistant (Llama): The woman is looking at something.",
                            ""
                        ]
                    }
                ]
            }
        ]
    },
    {
        "id": "2508.00429v3",
        "title": "ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network",
        "authors": [
            "Minghao Guo",
            "Xi Zhu",
            "Jingyuan Huang",
            "Kai Mei",
            "Yongfeng Zhang"
        ],
        "Abstract": "Abstract Graph Neural Networks (GNNs) have achieved remarkable success in graph-based learning by propagating information among neighbor nodes by predefined aggregation mechanisms. However, such fixed schemes often suffer from two key limitations. On the one hand, it cannot handle the imbalance in node informativeness—some nodes are rich in information, while others remain sparse.\nOn the other hand, predefined message passing primarily leverages local structural similarity while ignoring global semantic relationships across the graph, limiting the model’s ability to capture distant but relevant information.\nTo address these limitations, we propose Retrieval-augmented Graph Agentic Network (thus named ReaGAN), an agent-based framework that addresses these limitations by empowering each node with autonomous, individual node-level decision-making. Each node is treated as an agent that independently plans its next action based on its internal memory, enabling node-level planning and adaptive message propagation. In addition, retrieval-augmented generation (RAG) is used to enable nodes to retrieve semantically relevant content and build the global relationship among the nodes in the graph.\nExtensive experiments demonstrate that ReaGAN achieves competitive performance under few-shot in-context settings, using only a frozen LLM backbone without fine-tuning. These results highlight the potential of agentic planning and integrated local-global retrieval for advancing graph machine learning.",
        "1 Introduction": [
            "Graph Machine Learning (GML) has achieved remarkable success over the past years, with Graph Neural Networks (GNNs) such as GCN (Kipf and Welling, 2017 ) , GAT (Veličković et al., 2018 ) , and GraphSAGE (Hamilton et al., 2017 ) becoming the de facto standards for representation learning over graph-structured data. These models operate through a static, globally synchronized message-passing framework, where in each layer, every node aggregates information from its neighbors using a predefined aggregation-update rule parameterized by shared weights across the graph.",
            "Figure 1: Message passing in traditional pre-defined way vs. ReaGAN’s method. (a) All nodes only do Local Aggregation. (b) Nodes aggregate in different ways. A’s is local and global; B’s is local only; C’s is global only; D does no operation.",
            "While effective in many scenarios, this paradigm suffers from a fundamental limitation: it treats all nodes uniformly, regardless of their varying local context or inherent semantics (Finkelshtein et al., 2023 ; Martinkus et al., 2023 ) . The aggregation-update process is entirely homogeneous, where each node follows the same procedure, every layer applies the same function, and no node has the capacity for individualized decision-making. However, graphs often contain a small subset of nodes that are rich in semantic content or structurally well-positioned, while others are sparsely connected, noisy, or contextually ambiguous. Thus, applying identical message-passing rules to all nodes in such settings is problematic: highly informative nodes may be overwhelmed by irrelevant inputs, while uninformative nodes fail to gather the support they need. Worse still, this homogeneous propagation can amplify noise and redundancy, thereby degrading overall representation quality. This observation motivates Challenge 1: Node-Level Autonomy — Can we endow each node with the capacity to autonomously plan its own message-passing behavior based on its internal state and local context, rather than relying on globally shared rules?",
            "In addition, most existing GNNs operate primarily based on local structural similarity, implicitly assuming that neighboring nodes share informative and task-relevant features. However, this assumption often breaks down in real-world graphs where semantically related nodes may be structurally distant, which is a pattern especially common in open-domain or heterogeneous networks (Zhu et al., 2024 ; Li et al., 2024 ) . As a result, traditional GNNs struggle to capture global semantic dependencies, limiting their ability to perform long-range reasoning or generalize beyond local structure. This limitation is especially pronounced in sparsely connected regions, where the local neighborhoods offer limited predictive value Li et al. ( 2024 ) . In such settings, retrieving semantically aligned but distant nodes becomes essential for enriching context and improving node representations. This raises Challenge 2: Local-Global Complementarity — Can we combine local-structure neighbors with global-semantic neighbors to enable a more comprehensive context-aware message passing?",
            "To address the two key challenges, we propose a new perspective on graph learning by rethinking the fundamental computational unit: we treat each node as an autonomous agent . Unlike GNNs that passively aggregate messages through static rules, each node acts as an agent who actively determines its action at each layer based on internal states and contextual signals. Inspired by general agent-based systems (Russell and Norvig, 2010 ; Wang et al., 2023 ; Xu et al., 2025 ; Sun et al., ; Gao and Zhang, 2024 ) , our agentic framework endows each node with four core components: Planning , which decides the next operations based on its current objective and context; Actions , which executes local-global aggregation strategies to interact with neighbors (Yao et al., 2022 ; Shinn et al., 2023 ) ; Memory , which stores the node’s cumulative textual features along with historical interaction traces; and Tool Use , which calls external functions like retrieval-augmented generation. In this work, we unify these components into the Retrieval-augmented Graph Agentic Network (ReaGAN) , which enables each node to make individualized, adaptive, and semantically informed decisions. This self-decision design directly addresses Challenge 1 by granting each node autonomy in its message passing, while the local-global aggregation strategy addresses Challenge 2 as the following paragraph shows.",
            "Beyond Local Aggregation which enables nodes to exchange messages with directly connected neighbors, we integrate retrieval-augmented generation (RAG) (Lewis et al., 2020 ; Su et al., 2025 ) as an external tool that empowers nodes to access semantically relevant but structurally distant information, where the entire graph is viewed as a searchable database. Each node then integrates both local structural signals and globally retrieved semantic content into its individual memory, forming a richer contextual representation for subsequent planning and interaction. Equipped with these capabilities, each node engages in an agentic workflow. Based on its current state and memory, the node prompts a frozen LLM to generate a context-aware plan, selects and executes the appropriate actions, such as local or global aggregation, optionally invokes external tools like RAG, and subsequently updates its internal memory with the acquired information.",
            "We pinpoint the key differences between traditional message passing and our agentic workflow. As shown in Figure 1 (a), standard GNNs enforce a uniform propagation rule, where all nodes solely communicate only with immediate neighbors. In contrast, Figure 1 (b) illustrates ReaGAN’s mechanism, in which each node autonomously selects its own actions and possibly reach its remote yet semantical neighbors. For example, node A performs both local and global aggregation; node B conducts only local aggregation, resembling classical GNN behavior; node C executes only global aggregation; and node D chooses to remain inactive. This demonstrates how ReaGAN supports diverse and asynchronous strategies at the node level, offering a level of node-level flexibility.",
            "We summarize our main contributions as follows:",
            "∙ \\bullet We introduce ReaGAN, an agentic graph learning framework that models each node as an autonomous agent equipped with planning, memory, action, and tool-use capabilities, moving beyond static, rule-based message passing in traditional GNNs.",
            "∙ \\bullet We introduce a hybrid aggregation mechanism that integrates local structural and global semantic information via retrieval-augmented generation, allowing nodes to dynamically access semantically relevant but structurally distant context.",
            "∙ \\bullet We conduct extensive experiments to demonstrate that ReaGAN achieves strong performance over existing traditional methods, even using a frozen LLM without fine-tuning."
        ],
        "2 Method": [
            {
                "Node As Agent": [
                    "In ReaGAN, each node is treated as an autonomous agent capable of perceiving its own state and neighborhood context, planning its next steps, executing context-aware actions, utilizing external tools, and updating its internal memory. This perspective departs from traditional synchronized message passing, instead enabling fully individualized, asynchronous, and adaptive behavior. Moreover, each node can not only aggregate information from local structural neighbors, but also retrieve semantically relevant signals from distant nodes across the graph. By doing so, ReaGAN naturally supports both node-level personalization and autonomy (addressing Challenge 1 ) and joint local-global information integration (addressing Challenge 2 )."
                ]
            },
            {
                "Agentic Formulation": [
                    "Let 𝒢 = ( 𝒱 , ℰ ) \\mathcal{G}=(\\mathcal{V},\\mathcal{E}) be an attributed graph, where each node v ∈ 𝒱 v\\in\\mathcal{V} is associated with a text feature t v t_{v} and an optional label y v ∈ 𝒴 y_{v}\\in\\mathcal{Y} . The goal is to predict labels y ^ v \\hat{y}_{v} for all unlabeled nodes. Each node is treated as an agent equipped with memory ℳ v \\mathcal{M}_{v} , interacting with a frozen large language model (LLM) through a multi-layered reasoning loop. At each layer, the node constructs a prompt based on its memory, queries the LLM for an action plan, and updates its memory accordingly. After L L layers, the node predicts a label by querying the LLM with a prompt derived from its final memory state.",
                    "In summary, each node follows a layer-wise cycle of perception , planning , action execution , and memory update , independently deciding whether to gather local/global information, make a prediction, or take no action. This fully agentic workflow is detailed in Algorithm 1 .",
                    "Algorithm 1 Layer-wise reasoning loop for agent node v v in ReaGAN 0: Text-attributed graph 𝒢 \\mathcal{G} , node v v , frozen LLM, retrieval database 𝒟 \\mathcal{D} 1: Initialize memory: ℳ v ( 0 ) ← { t v } \\mathcal{M}_{v}^{(0)}\\leftarrow\\{t_{v}\\} 2: Initialize aggregated feature: t ~ v ( 0 ) ← t v \\tilde{t}_{v}^{(0)}\\leftarrow t_{v} 3: for layer l = 1 l=1 to L L do 4: p v ( l ) ← Prompt planning ( ℳ v ( l − 1 ) ) p_{v}^{(l)}\\leftarrow\\texttt{Prompt}_{\\texttt{planning}}(\\mathcal{M}_{v}^{(l-1)}) 5: a v ( l ) ← LLM ( p v ( l ) ) a_{v}^{(l)}\\leftarrow\\texttt{LLM}(p_{v}^{(l)}) 6: for action a a in a v ( l ) a_{v}^{(l)} do 7: if a = LocalAggregation a=\\texttt{LocalAggregation} then 8: t ~ v ( l ) ← TextAgg ( t ~ v ( l − 1 ) , { t ~ u ( l − 1 ) ∣ u ∈ 𝒩 local ( v ) } ) \\tilde{t}_{v}^{(l)}\\leftarrow\\texttt{TextAgg}(\\tilde{t}_{v}^{(l-1)},\\{\\tilde{t}_{u}^{(l-1)}\\mid u\\in\\mathcal{N}_{\\text{local}}(v)\\}) 9: ℰ v ( l ) ← { ( t ~ u ( l − 1 ) , y u ) ∣ u ∈ 𝒩 local ( v ) , y u ∈ 𝒴 } \\mathcal{E}_{v}^{(l)}\\leftarrow\\{(\\tilde{t}_{u}^{(l-1)},y_{u})\\mid u\\in\\mathcal{N}_{\\text{local}}(v),y_{u}\\in\\mathcal{Y}\\} 10: ℳ v ( l ) ← ℳ v ( l − 1 ) ∪ { t ~ v ( l ) } ∪ ℰ v ( l ) \\mathcal{M}_{v}^{(l)}\\leftarrow\\mathcal{M}_{v}^{(l-1)}\\cup\\{\\tilde{t}_{v}^{(l)}\\}\\cup\\mathcal{E}_{v}^{(l)} 11: else if a = GlobalAggregation a=\\texttt{GlobalAggregation} then 12: 𝒩 global ( v ) ← RAG ( t ~ v ( l − 1 ) , top = K ) \\mathcal{N}_{\\text{global}}(v)\\leftarrow\\texttt{RAG}(\\tilde{t}_{v}^{(l-1)},\\texttt{top}=K) 13: t ~ v ( l ) ← TextAgg ( t ~ v ( l − 1 ) , { t ~ u ( l − 1 ) ∣ u ∈ 𝒩 global ( v ) } ) \\tilde{t}_{v}^{(l)}\\leftarrow\\texttt{TextAgg}(\\tilde{t}_{v}^{(l-1)},\\{\\tilde{t}_{u}^{(l-1)}\\mid u\\in\\mathcal{N}_{\\text{global}}(v)\\}) 14: ℰ v ( l ) ← { ( t ~ u ( l − 1 ) , y u ) ∣ u ∈ 𝒩 global ( v ) , y u ∈ 𝒴 } \\mathcal{E}_{v}^{(l)}\\leftarrow\\{(\\tilde{t}_{u}^{(l-1)},y_{u})\\mid u\\in\\mathcal{N}_{\\text{global}}(v),y_{u}\\in\\mathcal{Y}\\} 15: ℳ v ( l ) ← ℳ v ( l − 1 ) ∪ { t ~ v ( l ) } ∪ ℰ v ( l ) \\mathcal{M}_{v}^{(l)}\\leftarrow\\mathcal{M}_{v}^{(l-1)}\\cup\\{\\tilde{t}_{v}^{(l)}\\}\\cup\\mathcal{E}_{v}^{(l)} 16: else if a = Prediction a=\\texttt{Prediction} then 17: p v pred ← Prompt predict ( ℳ v ( l − 1 ) ) p_{v}^{\\texttt{pred}}\\leftarrow\\texttt{Prompt}_{\\texttt{predict}}(\\mathcal{M}_{v}^{(l-1)}) 18: y ^ v ← LLM ( p v pred ) \\hat{y}_{v}\\leftarrow\\texttt{LLM}(p_{v}^{\\texttt{pred}}) 19: else if a = NoOp a=\\texttt{NoOp} then 20: ℳ v ( l ) ← ℳ v ( l − 1 ) \\mathcal{M}_{v}^{(l)}\\leftarrow\\mathcal{M}_{v}^{(l-1)} {No change to memory} 21: end if 22: end for 23: end for 24: return Predicted label y ^ v \\hat{y}_{v} (if generated)"
                ]
            },
            {
                "2.1 Planning: Prompting Frozen LLM": [
                    "In ReaGAN, each node is equipped with a local planner that determines how to act at each layer. Rather than relying on a globally shared aggregation rule, we leverage a frozen large language model (LLM) to enable in-context planning, where decisions are conditioned on the node’s own memory. At each layer l l , node v v constructs a structured prompt based on its internal memory ℳ v ( l ) \\mathcal{M}_{v}^{(l)} . The prompt is then sent to a frozen LLM (e.g., LLaMA (Touvron et al., 2023 ) , Qwen (Zhou et al., 2024 ) , or DeepSeek (Team, 2024 ) ), which returns an action plan: a v ( l ) = LLM ( Prompt planning ( ℳ v ( l ) ) ) a_{v}^{(l)}=\\texttt{LLM}(\\texttt{Prompt}_{\\texttt{planning}}(\\mathcal{M}_{v}^{(l)}))",
                    "while the action(s) in a v ( l ) a_{v}^{(l)} are selected from a discrete space and described in detail in Section 2.2 . The node then parses and executes the plan, updating its memory with newly acquired information. This process is repeated for L L layers. At the final layer, the node constructs a prediction-specific prompt and queries the LLM to produce a label: y ^ v = LLM ( Prompt predict ( ℳ v ( L ) ) ) \\hat{y}_{v}=\\texttt{LLM}(\\texttt{Prompt}_{\\texttt{predict}}(\\mathcal{M}_{v}^{(L)}))",
                    "which ends up the overall planning loop illustrated in Figure 2 . To sum up, the planning process allows each node to reason independently and asynchronously, determining what to do and when to act—without any global synchronization. It forms the core mechanism that enables Challenge 1 (Node-Level Autonomy) and Challenge 2 (Global Semantic Access) to be addressed in a unified and decentralized manner.",
                    "Figure 2: Overview of ReaGAN. Each node in ReaGAN is modeled as an agent equipped with four core modules: Memory , Planning , Tools , and Action . The node stores its original information and receives local and global information into its memory, constructs a natural language prompt, and queries a frozen LLM for an action plan. The returned plan may contain one or more actions (e.g., Local Aggregation, Global Aggregation), which are executed using available tools such as RAG. The resulting outputs are written back into memory and may produce a predicted label. This forms a closed loop of perception, planning, action, and memory refinement across layers."
                ]
            },
            {
                "2.2 Action: Node-Level Decision Space": [
                    "The purpose of operating actions is to aggregate information, both from local and global way. The process of aggregation containing enhancing the text feature and collecting neighbor shots. The details will be discussed in the following sections.",
                    {
                        "2.2.1 Local Aggregation.": [
                            "We refer to directly connected nodes in the input graph as local structural neighbors , and to semantically similar but unconnected nodes retrieved via RAG as global semantic neighbors . When a node selects the Local Aggregation action, it gathers information from its local structural neighbors (e.g., 1-hop or 2-hop nodes). This action serves two primary purposes:",
                            "• Feature Enhancement. All neighbors, regardless of label availability, contribute to a contextual text aggregation process. The node summarizes the textual content of its local neighborhood—typically by generating a single aggregated text snippet t ~ v ( l ) \\tilde{t}_{v}^{(l)} that combines its own feature t v t_{v} with those of its structural neighbors. This mirrors the feature aggregation in GNNs, where embeddings are iteratively enriched from local neighbors. The aggregated feature is defined as: t ~ v ( l ) = TextAgg ( t ~ v ( l − 1 ) , { t ~ u ( l − 1 ) | u ∈ 𝒩 local ( v ) } ) \\tilde{t}_{v}^{(l)}=\\texttt{TextAgg}\\left(\\tilde{t}_{v}^{(l-1)},\\left\\{\\tilde{t}_{u}^{(l-1)}\\,\\middle|\\,u\\in\\mathcal{N}_{\\text{local}}(v)\\right\\}\\right) where TextAgg ( ⋅ ) \\texttt{TextAgg}(\\cdot) denotes a natural language-level aggregation function such as concatenation or summarization. Here, t ~ v ( l − 1 ) \\tilde{t}_{v}^{(l-1)} is the node’s aggregated feature from the previous layer (initialized as t v t_{v} ). The result t ~ v ( l ) \\tilde{t}_{v}^{(l)} is stored in memory as an updated representation of the node’s semantic environment. • Few-Shot Example Collection. From among the labeled structural neighbors, a small subset is selected as in-context examples. Each selected node contributes a (text, label) pair, which is explicitly written into the current node’s memory. This supports few-shot label prediction by providing semantically relevant examples during prompt construction: ℰ v ( l ) = { ( t ~ u ( l − 1 ) , y u ) | u ∈ 𝒩 local ( v ) ∧ y u ∈ 𝒴 } \\mathcal{E}_{v}^{(l)}=\\left\\{(\\tilde{t}_{u}^{(l-1)},y_{u})\\,\\middle|\\,u\\in\\mathcal{N}_{\\text{local}}(v)\\land y_{u}\\in\\mathcal{Y}\\right\\}",
                            "Thus, Local Aggregation provides both semantic enhancement and label grounding through neighboring information. All aggregated summaries and selected examples are written into the node’s memory buffer for use in future planning or prediction. The memory update is: ℳ v ( l ) ← ℳ v ( l − 1 ) ∪ { t ~ v ( l ) } ∪ ℰ v ( l ) \\mathcal{M}_{v}^{(l)}\\leftarrow\\mathcal{M}_{v}^{(l-1)}\\cup\\{\\tilde{t}_{v}^{(l)}\\}\\cup\\mathcal{E}_{v}^{(l)}"
                        ]
                    },
                    {
                        "2.2.2 Global Aggregation.": [
                            "The Global Aggregation action enables a node to augment its semantic context by retrieving structurally distant but semantically similar nodes from across the graph. This is achieved by invoking a retrieval tool (e.g., RAG), which queries a structure-free database constructed from all node-level textual representations. Unlike the original graph, this database preserves no edges; each entry consists of a node’s text feature t u t_{u} and its label y u y_{u} (if available). Retrieval is based purely on textual semantic similarity, typically computed via dense embedding similarity (e.g., cosine distance). Given a query node’s current textual state t v ( l − 1 ) t_{v}^{(l-1)} , the system retrieves the Top- K K most relevant global semantic neighbors: 𝒩 global ( v ) = RAG ( t v ( l − 1 ) , t o p = K ) \\mathcal{N}_{\\text{global}}(v)=\\texttt{RAG}(t_{v}^{(l-1)},top=K)",
                            "Similar to Local Aggregation, Global Aggregation also serves two main purposes:",
                            "• Feature Enhancement. The textual features of the retrieved nodes are aggregated to form an updated summary t ~ v ( l ) \\tilde{t}_{v}^{(l)} that reflects global semantics, which is appended to the node’s memory to enhance its contextual understanding beyond structural proximity: t ~ v ( l ) = TextAgg ( t ~ v ( l − 1 ) , { t ~ u ( l − 1 ) | u ∈ 𝒩 global ( v ) } ) \\tilde{t}_{v}^{(l)}=\\texttt{TextAgg}\\left(\\tilde{t}_{v}^{(l-1)},\\left\\{\\tilde{t}_{u}^{(l-1)}\\,\\middle|\\,u\\in\\mathcal{N}_{\\text{global}}(v)\\right\\}\\right) • Few-Shot Example Collection. From the retrieved nodes with known labels, a subset is selected as few-shot examples for prompt construction: ℰ v ( l ) = { ( t ~ u ( l − 1 ) , y u ) | u ∈ 𝒩 global ( v ) ∧ y u ∈ 𝒴 } \\mathcal{E}_{v}^{(l)}=\\left\\{(\\tilde{t}_{u}^{(l-1)},y_{u})\\,\\middle|\\,u\\in\\mathcal{N}_{\\text{global}}(v)\\land y_{u}\\in\\mathcal{Y}\\right\\}",
                            "Both the aggregated summary and the selected labeled examples are written into the node’s memory, which is same as the local version. Through this process, Global Aggregation provides semantic enrichment and label grounding from structure-agnostic sources. It expands each node’s informational horizon beyond its local neighborhood—especially benefiting nodes in sparse or isolated regions. This mechanism directly addresses Challenge 2 (Global Semantic Access) ."
                        ]
                    },
                    {
                        "2.2.3 NoOp.": [
                            "While seemingly trivial, the NoOp (no operation) action plays a critical role in regulating information flow. When selected, the node intentionally chooses to take no action in the current layer—effectively pausing further aggregation or decision-making.",
                            "This mechanism is essential for avoiding information over-collection, especially in cases where the memory already contains sufficient context. It helps prevents noise accumulation and supports pacing in multi-layer reasoning. By allowing nodes to wait or opt out of message passing altogether, NoOp reinforces ReaGAN’s core principle of self-decision and resource-aware adaptation ( Challenge 1 )."
                        ]
                    }
                ]
            },
            {
                "2.3 Memory: Tracking the Internal State": [
                    "As mentioned above, each node maintains a private memory buffer ℳ v \\mathcal{M}_{v} that accumulates information over time to support reasoning and prediction. This memory includes two types of content from two sources: • Local information : messages and labeled examples from Local Aggregation. • Global information : semantically similar content retrieved via Global Aggregation.",
                    "As illustrated in Figure 3 , memory entries can also be categorized along a second semantic dimension: in addition to the source type (local vs. global), we also consider the information purpose . These correspond to the following categories:",
                    "• Text Feature. The node’s raw natural language input t v t_{v} , preserved across all layers to serve as an identity anchor. • Aggregated Representations. Natural language summaries collected via Local Aggregation and Global Aggregation. These capture multi-scale contextual signals to enrich node understanding. • Selected Labeled Neighbors. A curated set of (text, label) examples drawn from both local and global sources. These are explicitly stored for use in few-shot prediction, injected into the prompt to support semantic reasoning.",
                    "Memory is updated incrementally at each layer by adding newly generated entries from the current step: ℳ v ( l ) ← ℳ v ( l − 1 ) ∪ { m i ( l ) } i = 1 k \\mathcal{M}_{v}^{(l)}\\leftarrow\\mathcal{M}_{v}^{(l-1)}\\cup\\{m_{i}^{(l)}\\}_{i=1}^{k} where { m i ( l ) } \\{m_{i}^{(l)}\\} are the new entries produced by actions such as Local Aggregation, Global Aggregation, or retrieval-based example selection at layer l l , which is equals to t ~ v ( l ) ∪ ℰ v ( l ) \\tilde{t}_{v}^{(l)}\\ \\cup\\mathcal{E}_{v}^{(l)} ).",
                    "As the core source of contextual information, the memory buffer provides the essential components for prompt construction: the original text, aggregated text, and a subset of labeled examples. This allows the prompt to accurately reflect the node’s internal state and accumulated knowledge. As each node independently controls its memory and its content evolves through executed actions, this design supports self-individualized behavior ( Challenge 1 ). Moreover, by storing semantically retrieved examples from distant nodes, memory also supports context enrichment beyond local structure ( Challenge 2 ).",
                    "Figure 3: Information flow from memory to prompt. Each node’s memory includes its original text feature, aggregated text feature from local and global neighbors, and selected labeled neighbor shots. During planning or prediction, these components are selectively injected into a natural language prompt, providing the LLM with (i) the input node’s raw identity, (ii) context-enhanced descriptions, and (iii) label-text pairs for few-shot learning. This design enables each agent to reason over multi-scale context and take personalized actions."
                ]
            },
            {
                "2.4 Tools: Global Semantic Augmentation": [
                    "To support global semantic reasoning, ReaGAN equips each node with a single external tool: Retrieval-Augmented Generation (RAG). This tool enables a node to retrieve semantically relevant information from across the entire graph—beyond its structural neighborhood. A structure-free database is constructed, consisting of all nodes’ text features and, when available, their labels. Each node contributes a single entry, and the resulting corpus 𝒟 \\mathcal{D} is indexed by textual similarity. Given a node’s current representation t v ( l ) t_{v}^{(l)} , the RAG tool performs a top- K K similarity search over the database: RAG ( t v ( l ) , t o p = K ) = TopK ( { t u | u ∈ 𝒱 } , sim ( t v ( l ) , t u ) ) \\texttt{RAG}(t_{v}^{(l)},top=K)=\\texttt{TopK}\\left(\\left\\{t_{u}\\,\\middle|\\,u\\in\\mathcal{V}\\right\\},\\texttt{sim}(t_{v}^{(l)},t_{u})\\right) where sim ( ⋅ , ⋅ ) \\texttt{sim}(\\cdot,\\cdot) denotes an embedding-based similarity function (e.g., cosine distance).",
                    "In this design, RAG functions as a modular tool that enables semantic retrieval for the Global Aggregation action. This separation ensures that memory evolution remains fully governed by agent actions, without implicit tool-side updates. By invoking RAG on demand, each node can enrich its local structural context with globally relevant, semantically aligned information—particularly beneficial for nodes situated in sparse or disconnected regions. As such, RAG plays a central role in addressing Challenge 2 (Global Semantic Access) .",
                    {
                        "Overall Execution Flow": [
                            "ReaGAN transforms each node into an autonomous agent equipped with memory, planning, and external tools. At each layer, nodes operate independently based on their internal state—without global synchronization or shared parameters.",
                            "Each reasoning layer follows the following cycle: • Perception: The node gathers contextual signals from its memory and optionally from local neighbors. • Planning: It constructs a prompt and queries the frozen LLM to decide the next action(s). • Action: The selected actions are executed—aggregating information, retrieving global content, or making a prediction—and the outcomes are written into memory.",
                            "This process repeats for L L layers (typically 1–3), after which each node outputs a predicted label. Through this decentralized execution mechanism, ReaGAN fulfills two key objectives: (1) enabling node-level autonomy and personalized decision-making ( Challenge 1 ); and (2) integrating local structural and global semantic information within a unified agentic framework ( Challenge 2 )."
                        ]
                    }
                ]
            }
        ],
        "3 Experiments": [
            "Based on the challenges identified in Section 1 , we formulate the following research questions(RQs):",
            "• RQ1 : How does ReaGAN perform on node classification tasks compared to standard GNNs? • RQ2 : How do the agentic planning mechanism and Global Aggregation contribute to performance? • RQ3 : When do agentic nodes need global semantic retrieval to do global aggregation, and how should local and global shots be balanced in prompts? • RQ4 : Does exposing label semantics improve classification accuracy in ReaGAN?",
            "Table 1: Test accuracy (%) on the Cora, Citeseer, and Chameleon datasets. Traditional GNNs rely on parametric training and fixed message passing, while ReaGAN leverages a frozen LLM for agentic planning and personalized reasoning. Model Cora Citeseer Chameleon Parametric Training (Supervised GNNs) GCN (Kipf and Welling, 2017 ) 84.71 72.56 28.18 GAT (Veličković et al., 2018 ) 76.70 67.20 42.93 GraphSAGE (Hamilton et al., 2017 ) 84.35 78.24 62.15 GPRGNN (Chien et al., 2021 ) 79.51 67.63 67.48 APPNP (Klicpera et al., 2019 ) 79.41 68.59 51.91 MLP-2 (Zhu et al., 2022 ) 76.44 76.25 46.72 MixHop (Abu-El-Haija et al., 2019 ) 65.65 49.52 36.28 Frozen LLM + Few-shot Setting ReaGAN (Ours) 84.95 ± \\pm 0.35 60.25 ± \\pm 0.36 43.80 ± \\pm 0.65",
            {
                "Datasets": [
                    "We evaluate the proposed ReaGAN on the standard node classification task using the Cora , Citeseer and Chameleon dataset. Each node corresponds to a scientific publication with a textual description, and the goal is to predict its research category. We adopt a 60%/20%/20% split for training, validation, and testing, respectively."
                ]
            },
            {
                "Baselines.": [
                    "We compare ReaGAN with a set of widely used baselines, including GCN (Kipf and Welling, 2017 ) , GAT (Veličković et al., 2018 ) , GraphSAGE (Hamilton et al., 2017 ) , APPNP (Klicpera et al., 2019 ) , GPRGNN (Chien et al., 2021 ) , MixHop (Abu-El-Haija et al., 2019 ) , and MLP-2. All models are implemented using PyTorch Geometric with standard hyperparameters and are trained on the same data splits as ReaGAN."
                ]
            },
            {
                "Reproduction Settings.": [
                    "We conduct all experiments in PyTorch using a cluster of NVIDIA RTX A6000 GPUs. The agentic node reasoning module relies on a frozen LLM, served via vLLM, using Qwen2-14B. We do not perform any fine-tuning. We report test accuracy for node classification. To ensure fair comparison, all baselines are trained under identical splits. We use all-MiniLM-L6-v2 to transfer text to embedding. More experimental details are provided in Appendix ."
                ]
            },
            {
                "3.1 Overall Performance on Node Classification": [
                    "To answer RQ1 , we report node classification accuracy for all baselines and for ReaGAN in Table 1 . Despite using no trainable parameters, ReaGAN achieves competitive performance compared to fully supervised GNNs, while enabling node-level autonomy and reasoning via a frozen LLM."
                ]
            },
            {
                "3.2 Impact of Agentic Planning and Global Retrievals": [
                    "To answer RQ2 , we perform ablations on ReaGAN’s two core components: node-level planning and global retrieval. Removing prompt planning forces all nodes to follow a fixed action sequence, leading to notable accuracy drops due to the lack of context-specific behavior. Disabling global retrieval (Local Only) limits each node to structural neighbors, which underperforms in sparse graphs. Conversely, the Global Only variant removes structural input and struggles in well-connected graphs, showing the need for both structural and semantic signals. Experimental results are shown on Table 2 which shows that both agentic planning and global semantic access are essential. Removing either component leads to measurable accuracy degradation, validating the effectiveness of our solution to the autonomy and semantic challenges.",
                    "Table 2: Ablation study on Cora, Citeseer, and Chameleon (Test Accuracy %). ReaGAN’s performance depends on both agentic planning and local-global integration. Model Variant Cora Citeseer Chameleon ReaGAN (Full) 84.95 60.25 43.80 No Prompt Planning 79.83 35.87 38.29 Local Only 81.67 58.73 25.60 Global Only 79.67 33.45 24.94",
                    "Table 3: Prompt Memory Strategy vs. Accuracy. Comparison of two prompt construction strategies across three datasets. Strategy A includes both local and global memory in the prompt. Strategy B includes global memory only when fewer than two local entries are available. Dataset Strategy Accuracy (%) Cora A 84.95 Cora B 83.02 Citeseer A 50.14 Citeseer B 60.25 Chameleon A 43.80 Chameleon B 38.29",
                    "To address RQ3 , we investigate how the balance between local and global memory in the prompt affects node classification accuracy. We perform an ablation study comparing two prompt construction strategies: Strategy A includes both local and global memory in the prompt; Strategy B includes global memory only when fewer than two local examples are available. As shown in Table 3 , Strategy A consistently performs better on structurally dense graphs like Cora and Chameleon, where high-quality local memory is readily available. In contrast, on the sparser Citeseer graph, Strategy B outperforms Strategy A by selectively avoiding potentially noisy global memory when local context is sufficient. The performance gap across datasets can be attributed to differences in structural density and label locality. Cora and Chameleon have stronger local homophily, making global memory broadly useful, while Citeseer benefits more from selective use due to sparser connections and noisier neighborhoods. As for RQ4 , Table 4 reveals that exposing label semantics consistently harms accuracy, as LLMs tend to overfit to general label names like “machine learning” and make biased guesses.",
                    "Table 4: Effect of Label Semantics on Accuracy. Showing label names (e.g., \"Rule Learning\") in the prompt harms performance, as LLMs tend to overfit to label wording rather than reasoning from memory. We anonymize all labels (e.g., \"Label_2\") in our final design. Dataset Label Names Visible Accuracy (%) Cora Yes 76.83 Cora No (Label_ID only) 84.95 Citeseer Yes 42.11 Citeseer No (Label_ID only) 60.25"
                ]
            }
        ],
        "4 Related Works": [
            {
                "4.1 Graph Neural Networks and Message Passing": [
                    "Graph Neural Networks (GNNs) such as GCN (Kipf and Welling, 2017 ) , GraphSAGE (Hamilton et al., 2017 ) , and GAT (Veličković et al., 2018 ) have become the dominant paradigm in graph machine learning. These models rely on fixed, layer-wise message passing schemes that aggregate information from each node’s neighbors using predefined update functions. While effective, this rigid design limits expressiveness and can cause issues like over-smoothing and over-squashing. To overcome these limitations, several works have proposed more flexible message passing strategies. CoGNN (Finkelshtein et al., 2023 ) introduces cooperative agents that decide whether to broadcast or listen during each round, enabling more adaptive communication. However, these agents still rely on hand-crafted utility functions and rule-based execution. In contrast, our method allows each node to independently plan and execute its own message passing actions using an LLM-based agentic mechanism, and extends the communication space to include global semantic neighbors, not just local structural ones."
                ]
            },
            {
                "4.2 Large Language Models for Graph Tasks": [
                    "Recent work has explored the use of LLMs for graph learning (Shu et al., 2024 ; Zhao et al., 2024 ) . PromptGFM (Zhu et al., 2024 ) converts nodes into text-based prompts and uses LLMs to learn a graph vocabulary for classification. In-context RAG (Li et al., 2024 ) frames node classification as a retrieval-augmented generation task, where textual neighbors are fetched and provided to the LLM for reasoning. These methods effectively leverage LLM capabilities but do not enable node-level autonomy or action planning. By contrast, we use LLMs not as passive inference engines but as active planners—the cognitive core of each agent node. The LLM decides what action to take next, what context to gather, and when to predict."
                ]
            },
            {
                "4.3 Agent-Based Graph Learning": [
                    "Several prior studies have introduced the notion of “agents” in graphs, but differ from our formulation. AgentNet (Martinkus et al., 2023 ) trains neural agents to walk the graph and distinguish structures via learned exploration policies, but these agents are part of a supervised model and do not make autonomous decisions. GraphAgent (Zhang et al., 2023 ) and GAgN (Liu et al., 2024 ) also propose node-agent architectures for adversarial defense and resilience, but use hardcoded 1-hop views or restricted inference logic rather than learned behaviors. AgentGNN (Liu et al., 2022 ) applies the agent-based paradigm to spatiotemporal graphs, modeling each node as a temporal processor. While similar in spirit, their approach is limited to sequential data and does not support open-ended, action-driven interaction or reasoning. Our formulation is fundamentally different: we treat each node as a full-fledged intelligent agent, with the ability to observe, reason, and act using LLM-powered prompts, without predefined roles, hardcoded transitions, or limited interaction space."
                ]
            }
        ],
        "5 Conclusion": [
            "We introduced ReaGAN, a novel framework that treats each node in a graph as an autonomous agent capable of planning, acting, and reasoning through interactions with a frozen LLM. Unlike traditional GNNs that apply fixed, synchronous message passing rules to all nodes, ReaGAN enables self-decision at the node level—empowering each node to determine what information to gather, how to interact with others, and when to make predictions based on its own memory and context. Through this agentic formulation, ReaGAN seamlessly integrates both local structural signals and global semantic information via a unified prompting interface. Each node operates independently, combining Local Aggregation, global retrieval, and memory-guided few-shot reasoning to support fully individualized behavior. Importantly, ReaGAN achieves competitive performance using only a frozen LLM, without any gradient-based training or model fine-tuning. This highlights the promise of structured prompting and autonomous planning as a plug-and-play alternative to traditional GNNs. By shifting from rigid, fixed-rule aggregation to retrieval-augmented, node-specific decision making, ReaGAN opens a new direction for graph learning with LLM-powered agents.",
            ""
        ],
        "6 Limitations and Future Work": [
            "ReaGAN opens a new perspective for graph learning by treating each node as a standalone agent with perception, planning, and action capabilities. While our current focus is on node classification over static text-attributed graphs and results are promising, this line of research also need to extend more experiments, like using more models and testing more graph tasks (Zhu et al., 2023 ) . Furthermore, this agentic formulation naturally extends to broader scenarios.",
            "In future work, we envision ReaGAN as a foundational layer in multi-agent systems (Li et al., 2025 ) , where each node-agent can participate in decentralized decision-making (Huang et al., 2025 ) or inter-agent communication. For instance, its ability to reason locally and retrieve information globally makes it a strong fit for modular systems (Gershon et al., 2025 ) or routing-based architectures (Mei et al., 2025a ) , where nodes may act as autonomous modules coordinating across a shared substrate. Additionally, to scale ReaGAN under constrained resources, we propose integrating it with agent orchestration frameworks such as AIOS (Mei et al., 2025b ) —enabling parallelized, resource-aware scheduling of reasoning agents across large graphs. Rather than being limited to graph classification, we see ReaGAN as a general blueprint for scalable agentic inference—well suited for future systems that demand fine-grained, context-aware, and communication-capable reasoning units."
        ]
    },
    {
        "id": "2508.00602v1",
        "title": "LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection\n  and Leakage Attacks",
        "authors": [
            "Francesco Panebianco",
            "Stefano Bonfanti",
            "Francesco Trovò",
            "Michele Carminati"
        ],
        "Abstract": "Abstract The generalization capabilities of Large Language Models (LLMs) have led to their widespread deployment across various applications. However, this increased adoption has introduced several security threats, notably in the forms of jailbreaking and data leakage attacks. Additionally, Retrieval Augmented Generation (RAG), while enhancing context-awareness in LLM responses, has inadvertently introduced vulnerabilities that can result in the leakage of sensitive information. Our contributions are twofold. First, we introduce a methodology to analyze historical interaction data from an LLM system, enabling the generation of usage maps categorized by topics (including adversarial interactions). This approach further provides forensic insights for tracking the evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a model-agnostic framework that combines static analysis for forensic insights with dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique identifies topic groups and detects anomalous patterns, allowing for proactive defense mechanisms.\nWe empirically evaluate LeakSealer under two scenarios: (1) jailbreak attempts, employing a public benchmark dataset, and (2) PII leakage, supported by a curated dataset of labeled LLM interactions. In the static setting, LeakSealer achieves the highest precision and recall on the ToxicChat dataset when identifying prompt injection. In the dynamic setting, PII leakage detection achieves an AUPRC of 0.97 0.97 0.97 , significantly outperforming baselines such as Llama Guard.",
        "1 Introduction": [
            "Large Language Models (LLMs) have revolutionized numerous tasks due to their emergent capabilities, establishing them as essential components across various applications yang_harnessing_2024 . LLMs are particularly powerful as question-answering systems, especially when integrated in a Retrieval-Augmented Generation (RAG) gao_retrieval-augmented_2023 pipeline, which incorporates relevant documents into the query context. RAG integrates context-dependent information, leading to more comprehensive and accurate responses. However, retrieved documents may contain classified information such as Personally Identifiable Information (PII). This sensitive data, if compromised, could be exploited by malicious actors for personal gain, such as through unauthorized disclosure or sale Sancho2023 . LLMs are susceptible to Prompt Injection, which can cause the model to deviate from its intended functionality, potentially producing toxic or harmful outputs (LLM Jailbreaking) shen__2023 or leaking sensitive data zeng_good_2024 . Existing solutions, such as Llama Guard inan_llama_2023 and LLM-As-A-Judge huang_empirical_2024 ; zheng_judging_2024 , rely on LLM-based approaches, requiring costly inference for each new sample. Thus, a service provider would perform twice the number of inference calls to the model to safeguard it against threats.",
            "The detection of past attacks can provide critical insights, enabling the identification of known adversaries. This process reveals patterns for active prevention of future attacks. The final output is a detailed usage map of a service, including trends and usage patterns that suggest product improvements.",
            "Original Contributions. The main contributions of our work are: • A general approach to perform historical analysis on interactions with an LLM. The method provides a usage map and forensic insight from the system conversation history; • LeakSealer, a lightweight semipervised framework for analyzing LLM systems under static and dynamic conditions. In the static scenario, it minimizes human effort in the historical analysis. In the dynamic approach, it serves as an active defense; • We produce a diverse dataset of labeled LLM conversations to detect leakage of PII from an RAG context. • We open-source both LeakSealer and the PII dataset to allow reproducibility and foster future research.",
            "More specifically, we introduce an approach to perform historical analysis on a corpus of interactions with an LLM. The method identifies semantic fingerprints of interactions with the LLM and groups matching samples for easier human inspection. The final output reports a comprehensive usage map that highlights topics of interest in the real-world application of an LLM system. For instance, the map can reveal whether a group of users is using a platform designed for AI-generated cooking recipes to suggest medications. The method allows the service provider to adjust the system’s design to accommodate spontaneous usage and provides forensic insight by grouping interactions with similar attack patterns and tagging them with characterizing keywords (e.g., topics outside the platform guidelines or attempts to leak phone numbers). Additionally, repeated analysis can offer temporal tracking of jailbreak attack patterns, facilitating monitoring of their evolution.",
            "Our second contribution is LeakSealer , a model-agnostic framework to address prompt injection attacks on standalone and RAG-based systems. The framework consists of two approaches that seamlessly blend into one another. The first is a static approach, applicable to an already deployed system. It implements the historical analysis methodology through an insightful usage of clustering and text analysis. The second is a dynamic approach, which leverages actionable patterns identified in the static approach to implement active defenses and counter ongoing attacks. Effective filtering is achieved in this scenario through a Human-In-The-Loop (HITL) pipeline. This allows for further specialization of the model in correctly identifying private PII versus publicly available PII. LeakSealer employs computationally efficient procedures, facilitating more cost-effective, rapid, and efficient detection compared to existing baselines. LeakSealer’s source code will be publicly available to support reproducibility and future development.",
            "We assess the performance of LeakSealer under two adversarial scenarios: (1) attempts by an attacker to jailbreak the LLM to generate unauthorized content and (2) targeted leakage of PII. For the first scenario, experiments are conducted using toxic interaction benchmarks commonly employed in the literature (OpenAI Content Moderation Dataset openai_2023_holistic and ToxicChat lin2023toxicchat ). For the second scenario, we address the absence of a high-quality dataset for defending against PII leakage by curating a dataset consisting of labeled interactions with an LLM. We will open-source the dataset as a benchmark for future research in this domain. In the static setting, LeakSealer performs exceptionally well in recognizing groups of interactions affected by prompt injection. The purity of identified clusters on the ToxicChat evaluation set reaches 0.97 0.97 0.97 , and its F1-score on both the ToxicChat and the PII dataset is the largest. LeakSealer also leads in performance in the dynamic setting. The Area Under the Precision-Recall Curve (AUPRC) is 0.97 0.97 0.97 , while the second-best baseline, Llama Guard, only reaches 0.84 0.84 0.84 ."
        ],
        "2 Background on Security of LLMs": [
            "This section introduces the background concepts of prompt injection and PII leakage in the context of LLMs. To the interested reader, we deferred to Appendix E the basic background on LLMs.",
            "Prompt Injection. Prompt Injection is an attack on LLMs, which allows malicious users to influence the model’s behavior, leading it to operate outside its predefined parameters shen__2023 ; yu_dont_2024 ; inan_llama_2023 ; xu_llm_2024 ; deng_masterkey_2024 . As is common in information security, the vulnerability arises from improper separation between instructions and data (user input). This allows malicious user inputs to modify or extend the model’s system prompt. One of the first instances of this attack is doing anything now (DAN), a roleplaying prompt devised by the jailbreak community to bypass ChatGPT restrictions shen__2023 . Jailbreak prompts employ various strategies to bypass content restrictions, often misleading the model with seemingly benign requests. Yu et al. yu_dont_2024 identify four categories of jailbreak prompts. The first is Disguised Intent , where the prompt frames a harmful request as a research inquiry or joke. The second is Role-playing , which involves prompting the LLM to adopt fictitious personas or scenarios that enable circumvention of safeguards. Other approaches include Structured Response prompts, which manipulate the format or language of the response, and Virtual AI Simulation , which directs the model to emulate another AI system with specific capabilities. Hybrid strategies are also possible, combining multiple techniques.",
            "PII Leakage. Prompt Injection can also represent a significant threat to the confidentiality of sensitive data. Indeed, inputs given to machine learning models can contain Personally Identifiable Information (PII). Prior work has shown how this data can be leaked from either the training set kim_propile_2024 or documents in the context zeng_good_2024 . In the latter case, the LLM answers are based on relevant documents fetched from a database. Such systems are known as Retrieval-Augmented Generation (RAG) systems gao_retrieval-augmented_2023 . In RAG systems, prompt injection primarily threatens data confidentiality, as adversaries can manipulate the system to disclose sensitive information from documents returned by the model’s retrieval engine zeng_good_2024 .",
            "Defenses. Many defenses have been devised against Prompt Injection, the simplest being defensive prompting, which consists of prompt engineering to instruct the LLM to ignore orders of jailbreak prompts. Defensive prompting can be augmented with the usage of special tokens that the model is trained to associate with the separation of the system prompt and the user input (e.g., <|start_header_id|> for Llama 3 dubey_llama_2024 ). Another technique that can be employed against prompt injection is using an external arbitrator model to determine whether there has been a violation of system instructions. Examples of such arbitrators are LLM-as-a-judge huang_empirical_2024 ; zheng_judging_2024 and Llama Guard inan_llama_2023 ."
        ],
        "3 Related Work": [
            "LLM Jailbreak Defenses. Inan et al. inan_llama_2023 introduce a risk taxonomy for LLMs and present Llama Guard, a safeguard model based on Llama 2 that identifies forbidden content according to the specified taxonomy. Zhang et al. zhang_parden_2024 propose PARDEN, a defensive measure that asks the LLM to repeat its response. This approach prompts the LLM to reevaluate its generated output, resulting in a response such as \"I can’t do that\". Liu et al. liu_prompt_2023 present a framework to formalize prompt injection attacks and benchmark existing attacks and defenses. Zhao et al. zhao_defending_2024 present L ayer-specific Ed iting (LED), a technique to enhance the resilience of LLMs against prompt injection attacks. The work reveals the importance of the early layers of the model in the identification of harmful prompts. LED mitigates attacks by aligning safety layers toward benign behavior.",
            "PII Leak Attack. Kim et al. kim_propile_2024 propose ProPILE, a probing tool to allow PII owners to find out when an LLM is susceptible to leakage of their personal information included in training data. Zeng and Zhang et al. zeng_good_2024 demonstrate how leakage attacks can be performed against documents retrieved by a RAG system. Some mitigations on the RAG step are also evaluated (Re-ranking, Summarization, and Distance Thresholding). The work also shows how the usage of a RAG pipeline reduces the generation of information memorized from training data. Finally, Evertz et al. evertz_whispers_2024 explore confidentiality problems in the LLM domain. The threat is modeled as a secret key retrieval game. The system prompt provides the key in this setting since no RAG pipeline has been implemented. 1 1 1 Additional related works are deferred to Appendix G .",
            "Discussion of Prior Work. Prior research has investigated the issue of personally identifiable information (PII) leakage in the RAG context of LLMs, proposing preliminary mitigation strategies within a narrow experimental setup zeng_good_2024 . These studies have relied on datasets such as the Enron email corpus EnronEmailDataset2015 , potentially confounding the distinction between leakage from retrieved context and model training. Other studies evertz_whispers_2024 have framed the problem as a secret key extraction task from contextual information, analogous to challenges encountered in Capture The Flag (CTF) cybersecurity scenarios. While insightful, this approach does not fully capture the nuanced nature of PII leakage. Additionally, research has examined leakage from pretraining data kim_propile_2024 , though this represents a distinct problem from leakage occurring in a RAG-based retrieval setting. As for jailbreak defenses, most prior solutions rely on another LLM (e.g., LLM-As-A-Judge huang_empirical_2024 ; zheng_judging_2024 , Llama Guard inan_llama_2023 ) or require repeated prompting for each incoming sample (e.g., PARDEN zhang_parden_2024 ). Despite being effective and model-agnostic, such defenses significantly inflate computational costs. Conversely, white-box approaches such as the LED method zhao_defending_2024 don’t require repeated inference. Instead, they necessitate partial retraining to align the model. For this reason, their deployment is also expensive (particularly in the case of frequent concept drift). In addition, their dependence on the model architecture makes their deployment infeasible on closed-source commercial models, which include many top-ranked LLMs in public benchmarks lmarena . Overall, existing solutions require additional resource-intensive operations either at inference or training time."
        ],
        "4 Threat Model and PII Leakage Formalization": [
            {
                "4.1 Attacker and Defender Models": [
                    "We consider a closed-box threat model in which the adversary interacts with the LLM solely through query-based access. The adversary lacks access to historical conversation records and does not have the ability to influence or poison the training data used to develop the model. The adversary is assumed to possess limited prior knowledge of the LLM’s internal architecture. In particular, the adversary is unaware of whether the model utilizes an RAG pipeline or whether the underlying document corpus includes personally identifiable information (PII). The adversary’s goals are as follows: (1) Jailbreaking: To induce the LLM into generating unauthorized, inappropriate content, potentially damaging the reputation of the system provider. (2) Information Extraction: To extract sensitive or confidential information from the LLM, with a particular emphasis on the leakage of PII, thereby violating the privacy of individuals whose data may be implicitly contained within the model.",
                    "We assume a defender with query-only access to the LLM (potentially an external service provider). This model requires a model-agnostic protection for the LLM. Less restrictive scenarios where the LLM service provider is the defender remain applicable."
                ]
            },
            {
                "4.2 Formalizing PII Leakage": [
                    "Let ℳ \\mathcal{M} caligraphic_M denote the LLM as a remote oracle, 𝒟 r \\mathcal{D}_{r} caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT the retrieval corpus, and 𝒞 \\mathcal{C} caligraphic_C the context resolved by the retrieval module. Given a set of possible queries 𝒬 \\mathcal{Q} caligraphic_Q , an RAG system processes an input query q ∈ 𝒬 q\\in\\mathcal{Q} italic_q ∈ caligraphic_Q by retrieving a set of documents { d 1 , … , d k } ⊆ 𝒟 r \\{d_{1},\\ldots,d_{k}\\}\\subseteq\\mathcal{D}_{r} { italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } ⊆ caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT to construct the context 𝒞 \\mathcal{C} caligraphic_C , which is then concatenated with q q italic_q and passed to ℳ \\mathcal{M} caligraphic_M to generate the response a ∈ 𝒜 a\\in\\mathcal{A} italic_a ∈ caligraphic_A : a = ℳ ( q , 𝒞 ) , a=\\mathcal{M}(q,\\mathcal{C}), italic_a = caligraphic_M ( italic_q , caligraphic_C ) , (1) where 𝒜 \\mathcal{A} caligraphic_A is the space of all the available responses. PII is information that can be used to identify an individual uniquely. Let ℐ \\mathcal{I} caligraphic_I denote the set of all PII attributes (e.g., names, addresses, identification numbers). A document d ∈ 𝒟 r d\\in\\mathcal{D}_{r} italic_d ∈ caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT may contain PII attributes represented as π ( d ) ⊆ ℐ \\pi(d)\\subseteq\\mathcal{I} italic_π ( italic_d ) ⊆ caligraphic_I . The RAG system is said to leak PII if a a italic_a reveals π ( d ) \\pi(d) italic_π ( italic_d ) , formally: ∃ π ∈ ℐ , π ∈ π ( d ) ∧ π ∈ a . \\exists\\pi\\in\\mathcal{I},\\pi\\in\\pi(d)\\wedge\\pi\\in a. ∃ italic_π ∈ caligraphic_I , italic_π ∈ italic_π ( italic_d ) ∧ italic_π ∈ italic_a . (2)",
                    "We formalize the privacy risk R R italic_R as the probability of PII leakage over the operation of the RAG system. Let P ( π ∈ a | q , 𝒞 ) P(\\pi\\in a|q,\\mathcal{C}) italic_P ( italic_π ∈ italic_a | italic_q , caligraphic_C ) denote the conditional probability of leaking a specific PII attribute π \\pi italic_π given the query q q italic_q and context 𝒞 \\mathcal{C} caligraphic_C . The privacy risk is defined as: R = 𝔼 q ∼ 𝒬 [ ∑ π ∈ ℐ P ( π ∈ a | q , 𝒞 ) ] . R=\\mathbb{E}_{q\\sim\\mathcal{Q}}\\bigg{[}\\sum_{\\pi\\in\\mathcal{I}}P(\\pi\\in a|q,\\mathcal{C})\\bigg{]}. italic_R = blackboard_E start_POSTSUBSCRIPT italic_q ∼ caligraphic_Q end_POSTSUBSCRIPT [ ∑ start_POSTSUBSCRIPT italic_π ∈ caligraphic_I end_POSTSUBSCRIPT italic_P ( italic_π ∈ italic_a | italic_q , caligraphic_C ) ] . (3)",
                    "The complexity of the task derives from the fact that the set ℐ \\mathcal{I} caligraphic_I is only partially defined; we only have a limited subset of examples of sensitive attributes derived from historical analysis (see Section 5.1 ). If ℐ \\mathcal{I} caligraphic_I were fully defined, we could construct a closed-form filter to exclude sensitive attributes from responses preemptively. However, the concept of sensitive information is not trivially encoded and does not lend itself to straightforward formalization."
                ]
            }
        ],
        "5 Proposed Methodology": [
            "In the current work, we leverage the pattern-matching capabilities of unsupervised machine learning models applied to the extracted embeddings to identify whether π ∈ ℐ \\pi\\in\\mathcal{I} italic_π ∈ caligraphic_I is a substring of the generated response a a italic_a and understand if a leakage occurred. A high-level scheme of the proposed framework, namely LeakSealer, is presented in Figure 1 and the corresponding pseudo-code is presented in Algorithm 1 .",
            "Figure 1: Visual representation of the LeakSealer approach.",
            "Algorithm 1 LeakSealer 0: Historical 𝒯 = { ( q t , a t ) } t = 1 N \\mathcal{T}=\\{(q_{t},a_{t})\\}_{t=1}^{N} caligraphic_T = { ( italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT 1: E ← { Φ ( q t ∥ a t ) | ( q t , a t ) ∈ 𝒯 } E\\leftarrow\\{\\Phi(q_{t}\\|a_{t})\\;|\\;(q_{t},a_{t})\\in\\mathcal{T}\\} italic_E ← { roman_Φ ( italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) | ( italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ∈ caligraphic_T } 2: E r e d ← DimReduce ( E ) E_{red}\\leftarrow\\textsc{DimReduce}(E) italic_E start_POSTSUBSCRIPT italic_r italic_e italic_d end_POSTSUBSCRIPT ← DimReduce ( italic_E ) 3: 𝒢 ← 𝒦 ( E r e d ) \\mathcal{G}\\leftarrow\\mathcal{K}(E_{red}) caligraphic_G ← caligraphic_K ( italic_E start_POSTSUBSCRIPT italic_r italic_e italic_d end_POSTSUBSCRIPT ) // Collect exemplars 4: 𝒮 sel ← { μ j ∣ 𝒢 j ∈ 𝒢 } \\mathcal{S}_{\\text{sel}}\\leftarrow\\{\\mu_{j}\\mid\\mathcal{G}_{j}\\in\\mathcal{G}\\} caligraphic_S start_POSTSUBSCRIPT sel end_POSTSUBSCRIPT ← { italic_μ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∣ caligraphic_G start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∈ caligraphic_G } 5: 𝒴 sel ← AskHumanFeedback ( 𝒮 sel , 𝒴 top ) \\mathcal{Y}_{\\text{sel}}\\leftarrow\\textsc{AskHumanFeedback}(\\mathcal{S}_{\\text{sel}},\\mathcal{Y}_{\\text{top}}) caligraphic_Y start_POSTSUBSCRIPT sel end_POSTSUBSCRIPT ← AskHumanFeedback ( caligraphic_S start_POSTSUBSCRIPT sel end_POSTSUBSCRIPT , caligraphic_Y start_POSTSUBSCRIPT top end_POSTSUBSCRIPT ) 6: Y ← PropagateLabels ( 𝒢 , 𝒴 sel ) Y\\leftarrow\\textsc{PropagateLabels}(\\mathcal{G},\\mathcal{Y}_{\\text{sel}}) italic_Y ← PropagateLabels ( caligraphic_G , caligraphic_Y start_POSTSUBSCRIPT sel end_POSTSUBSCRIPT ) 7: ℱ ← TrainClassifier ( E r e d , Y ) \\mathcal{F}\\leftarrow\\textsc{TrainClassifier}(E_{red},Y) caligraphic_F ← TrainClassifier ( italic_E start_POSTSUBSCRIPT italic_r italic_e italic_d end_POSTSUBSCRIPT , italic_Y )",
            {
                "5.1 The Framework": [
                    "LeakSealer needs as input a set of N ∈ ℕ N\\in\\mathbb{N} italic_N ∈ blackboard_N historical tuples of queries, context and corresponding answers 𝒯 = { ( q t , a t ) } t = 1 N \\mathcal{T}=\\{(q_{t},a_{t})\\}_{t=1}^{N} caligraphic_T = { ( italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT from which it extracts the corresponding embedding E E italic_E generated from the LLM (Line 1 ). Here, we generate the embedding induced by the query/answer pair thanks to the answer’s availability. Since commonly the embeddings have high dimensionality and are representative of an input space that may be significantly larger than the one for which it is used in the context of 𝒯 \\mathcal{T} caligraphic_T , it applies an unsupervised dimensionality reduction, obtaining the corresponding reduced embeddings E r e d E_{red} italic_E start_POSTSUBSCRIPT italic_r italic_e italic_d end_POSTSUBSCRIPT . The compressed embeddings E r e d E_{red} italic_E start_POSTSUBSCRIPT italic_r italic_e italic_d end_POSTSUBSCRIPT act as the semantic fingerprint , as the correlation of samples with similar topics corresponds to the similarity of the embeddings. The next step consists in performing a clustering 𝒦 \\mathcal{K} caligraphic_K of the reduced embeddings E r e d E_{red} italic_E start_POSTSUBSCRIPT italic_r italic_e italic_d end_POSTSUBSCRIPT (Line 3 ) that produces a set of clusters 𝒢 = { 𝒢 1 , … , 𝒢 M } \\mathcal{G}=\\{\\mathcal{G}_{1},\\ldots,\\mathcal{G}_{M}\\} caligraphic_G = { caligraphic_G start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , caligraphic_G start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT } and corresponding centroids { μ 1 , … , μ M } \\{\\mu_{1},\\ldots,\\mu_{M}\\} { italic_μ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_μ start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT } . In the case the clustering produces, in addition to clusters, a set of outliers, we consider them as separated clusters whose centroids are the sample itself. Thanks to this step, prompt injection and PII leakage attacks will most likely be identified as outliers or assigned to groups sharing similar prompting patterns. The clustering approach allows us to ask for the human feedback if each of the clusters 𝒢 j \\mathcal{G}_{j} caligraphic_G start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is malicious or legitimate (Line 5 ). The human evaluator will determine whether they believe the interaction is compliant with the usage policies of their platform. This can include filtering toxic content or flagging an information leakage incident. The information provided by the human feedback is propagated to all the samples present in the clusters, meaning that the semantic fingerprint of the group represents a series of examples of the respective class (malicious or safe). This allows to build a vector Y = ( y 1 , … , y N ) Y=(y_{1},\\ldots,y_{N}) italic_Y = ( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ) , Y ∈ { 0 , 1 } N Y\\in\\{0,1\\}^{N} italic_Y ∈ { 0 , 1 } start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT where each element is 1 1 1 if the corresponding sample ( q t , a t ) (q_{t},a_{t}) ( italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) belongs to a malicious cluster and 0 otherwise (Line 6 ). The last step uses the reduced embeddings E r e d E_{red} italic_E start_POSTSUBSCRIPT italic_r italic_e italic_d end_POSTSUBSCRIPT of the queries/answers pairs from the history 𝒯 \\mathcal{T} caligraphic_T and trains a classifier ℱ \\mathcal{F} caligraphic_F (Line 7 ) in a semi-supervised way semisupervised .",
                    "After training, LeakSealer can be used as an active defense in a dynamic setting to check the maliciousness of a new request, i.e., a tuple with query q ′ q^{\\prime} italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , and answer a ′ a^{\\prime} italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT . Indeed, the unseen sample ( a ′ , q ′ ) (a^{\\prime},q^{\\prime}) ( italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) is processed to generate a prediction of the class y ′ y^{\\prime} italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT to discriminate whether the sample is safe or malicious. Finally, upon inspection of the representing cluster, we can also infer additional information about the possible nature of the request.",
                    "We remark that LeakSealer is presented here in its generic form. The specific implementation of the different elements of the framework, i.e., specifying the dimensionality reduction, clustering, and classification training details, is included in Appendix B ."
                ]
            },
            {
                "5.2 Static Setting": [
                    "The LeakSealer framework can be employed to analyze historical query-response pairs generated by an LLM, thereby enabling the application of the historical analysis approach introduced in this paper. We refer to this usage modality of LeakSealer as static setting . In this scenario, we use only the clustering 𝒢 \\mathcal{G} caligraphic_G to extract information about the queries/answer pairs in 𝒯 \\mathcal{T} caligraphic_T . This analysis selects outliers and representative samples from each cluster μ j \\mu_{j} italic_μ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT as key data points for subsequent processing. Further analyses to characterize interaction groups are performed exclusively on these samples. Indeed, appropriately chosen representatives effectively capture cluster-defining characteristics, serving as proxies for their respective groups. This approach optimizes computational efficiency and minimizes the overhead on human analysts by restricting the analysis to a single representative sample for each group of interactions.",
                    "This procedure requires an additional step to generate a report detailing the historical usage map of the deployed model. The report gives service providers insight into deviating usage patterns with respect to the system design. Such discrepancies are not always attacks, but could constitute a different way the user base uses the model. The report associates each cluster or outlier with two relevant pieces of information: the interaction count | G j | |G_{j}| | italic_G start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | and a set of keywords k ( 𝒲 j ) k(\\mathcal{W}_{j}) italic_k ( caligraphic_W start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) that identify the topic 𝒲 j \\mathcal{W}_{j} caligraphic_W start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT . The first is the size of the cluster, and it correlates with the popularity of the interaction pattern (or attack pattern in case of malicious interaction). The latter provides a reference to detect undesired topics in user interactions at a glance, streamlining the analysis and filtering process. LeakSealer addresses this step using a zero-shot query to an LLM applied to the exemplar. Nevertheless, given that the operation is constrained to exemplars, the computational overhead remains relatively low, even if this requires employing an LLM query."
                ]
            }
        ],
        "6 Experimental Evaluation": [
            "We present an evaluation of the performance of LeakSealer in the static and dynamic settings. In the static case, we measure the framework’s ability to produce homogeneous conversation groups in terms of their safety. This is relevant for its forensic applications and ability to serve as a preliminary analysis for the dynamic procedure. Subsequent steps of the static approach consist of topic modeling and graphical presentation of results. These procedures are not part of the evaluation as they are already established and present no significant research challenge in the context of this work. 2 2 2 In Appendix A , we provide a complete static report generated by LeakSealer for the interested reader. As for the dynamic procedure, we test LeakSealer as an active defense alongside state-of-the-art baselines. Since our application framework is intended for black-box interaction, our evaluation does not include white-box defenses (e.g., LED zhao_defending_2024 ). Chosen baselines are instead LLM-as-a-judge , and Llama Guard inan_llama_2023 .",
            "Models. We evaluate LeakSealer against Llama Guard 3 8B inan_llama_2023 , the latest version of the Llama Guard model. For LLM-As-A-Judge models, we select some of the latest open-source and commercial LLM models: Llama 3.1 8B dubey_llama_2024 , DeepSeek-R1 (DistilLlama 8B) deepseekai2025deepseekr1incentivizingreasoningcapability , Ministral 8B Mistral2024Ministraux , and GPT-4o openai_2024_gpt4o . Llama Guard 3 is built upon the Llama model and, as such, requires prompting as any other LLM. Following the reference study on Llama Guard, which focuses on toxicity detection, we replicate the system prompt and extend it to include complete instructions for PII leakage detection. This extension incorporates additional categories of personal data that were not covered in the original. The LLM-As-A-Judge is implemented as an oracle tasked by few-shot prompting. This means that both a description of the task and some example pairs of input and output are provided as the system prompt. A deeper discussion of the prompting choices of the baselines is done in Appendix D . All open-source models are tested on fp16 precision. Inference is performed on a virtual machine equipped with Tesla P100 GPU with 16 GB VRAM, 29 GB available work RAM, Intel(R) Xeon(R) CPU @ 2.00GHz, 2 physical CPU cores, and 4 logical cores.",
            "Datasets. For the Jailbreak evaluation, we focus on toxic content filtering due to its prevalence in the literature. We employ two widely recognized datasets: the OpenAI Content Moderation Dataset openai_2023_holistic and the ToxicChat Dataset lin2023toxicchat . The first is a collection of N = 1 , 680 N=1,680 italic_N = 1 , 680 samples, each annotated with a multiclass label associated with the OpenAI moderation API taxonomy. This taxonomy specifies the type of toxic content that can be identified (e.g., sexual, hate, violence). For our purposes, we treat all flagged samples as malicious ( y t = 1 y_{t}=1 italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1 ), irrespective of the specific category of violation, as our primary task is to classify samples as either malicious or safe. The ToxicChat Dataset provides a more extensive benchmark, containing N = 10166 N=10166 italic_N = 10166 high-quality input-output pairs designed to evaluate content moderation in real-world user-AI interactions. Collected from the Vicuna online demo, these samples are annotated with toxicity and jailbreak labels. Notably, all jailbreak samples are also labeled as toxic, enabling a unified focus on detecting toxic content in diverse scenarios. We employ a custom-designed dataset for the second evaluation, which addresses the detection of PII leakage.",
            {
                "6.1 The PII Dataset": [
                    "Existing datasets containing PII, such as the Enron email corpus included in The Pile pile , have been employed in prior research kim_propile_2024 for purposes aligned with the dataset’s nature. However, the choice of such datasets can result in a biased evaluation. Notably, these datasets are explicitly included in the pretraining corpora of most publicly available LLMs. Consequently, distinguishing between information leaks originating from pretraining data and those stemming from an RAG system presents a significant challenge, particularly in the context of this study. Other datasets, such as the widely used GretelAI PII Masking V1 Dataset gretel-pii-docs-en-v1 , offer limited applicability for our task. These datasets are specifically designed to train Named Entity Recognition (NER) models for identifying PII within text corpora. However, they do not account for the dynamics of information extraction mediated by LLMs or evaluate whether the generated outputs reference information retrieved from an RAG context. Finally, other works on the leakage of private information have assessed leakage attacks on strings like private keys, which do not represent the variety of PII of real data subjects evertz_whispers_2024 .",
                    "Our Dataset. We curate a dataset for the task of detecting PII extraction from an RAG system. The data collection focuses on diversity in document types and scenarios, balancing the inclusion of sensitive and non-sensitive information. The dataset collects N = 1 , 048 N=1,048 italic_N = 1 , 048 interactions with an RAG-based LLM system. Each conversation is manually labeled by a human evaluator for the occurrence of information leakage. The RAG database used for the generation of the samples contains emails (primarily work-related), invoices, prescriptions, instruction manuals, electronic datasheets, and scholarship rankings. Each type of document was generated through distinct methodologies to ensure realistic and varied data. Invoices and prescriptions were procedurally generated using Python scripts, enforcing templates populated with synthetic information (e.g., names, addresses, account numbers). Emails were crafted with Llama 3.1 8B dubey_llama_2024 , using procedural instructions that optionally incorporated randomly generated PII into the context. The remaining document types, such as instruction manuals and datasheets, were generated with GPT-4o openai_2024_gpt4o and Gemini 1.0 Pro google_2024_gemini to enrich the dataset with broader language patterns and styles. To maximize quality, all LLM-generated content underwent manual review and correction. This post-generation process ensured logical consistency, enhanced linguistic variety, and eliminated potential artifacts of machine generation that might bias downstream evaluation. We remark that in the produced dataset, no real-world personal information was used in the dataset . Instead, synthetic names, locations, and email domains were sampled randomly from comprehensive lists of plausible options. Numeric data, such as account or prescription numbers, was generated within realistic bounds to mimic actual records while preserving the dataset’s artificial nature. This approach mitigates privacy risks while maintaining fidelity to real-world scenarios.",
                    "Dataset Structure. Our dataset includes both the original source documents and a structured JSON dictionary with all interactions between the user and the LLM. This open-access resource is designed to facilitate the development of privacy-preserving systems and benchmarks. Each JSON entry includes the RAG context, the user question, the corresponding LLM response (influenced by the context), and a manually assigned leakage label indicating whether unauthorized information was revealed. Appendix C provides additional details regarding the dataset."
                ]
            },
            {
                "6.2 Evaluation in the Static Setting": [
                    "This initial analysis focuses on the performance of LeakSealer in the static setting, specifically evaluating its ability to separate safe samples from unsafe ones in an analysis of past interactions. This evaluation also assesses the significance of exemplars as cluster representatives. Indeed, a robust implementation of the our approach needs to produce both homogeneous clusters and representatives that effectively characterize the whole cluster.",
                    "(a) OpenAI dataset (b) ToxicChat dataset (c) PII dataset Figure 2: Precision-Recall curves for Static LeakSealer on tested datasets, varying the γ \\gamma italic_γ threshold of exemplar ratio.",
                    "Evaluation Metrics. We evaluate LeakSealer in the static setting with the metrics of cluster purity , accuracy, precision, recall, F1-score, and Area Under the Precision-Recall Curve (AUPRC). Purity P P italic_P , is formally defined as follows: P = 1 N ∑ k = 1 K max j ⁡ | C k ∩ T j | , P=\\frac{1}{N}\\sum_{k=1}^{K}\\max_{j}|C_{k}\\cap T_{j}|, italic_P = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_max start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | italic_C start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∩ italic_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | , (4) is a typical extrinsic evaluation metric for clustering. It is calculated by dividing the number of correctly assigned points (i.e., those belonging to the dominant class in each cluster) by the total number of data points N N italic_N . Ranging from 0 to 1, a purity score of 1 indicates perfectly homogeneous clusters (each cluster contains points from only one class), while lower scores indicate a lack of congruence with the classes. While purity is easy to interpret, it does not penalize creating many small clusters, so it may not fully capture clustering quality in scenarios where fewer, larger clusters are desirable. In our setting, we expect many small clusters belonging to different topics, with more than one presenting malicious interactions.",
                    "However, purity does not account for class imbalance, which is a common scenario in this type of task. Therefore, we also include a second set of extrinsic metrics based on classification. We assign each cluster’s label to the majority class among the examples of a cluster, thus using the representatives to characterize the whole group of interactions. Metrics from this task are the typical measures employed in evaluating a classifier: accuracy, precision, recall, and F1 score. When purity and accuracy metrics yield identical values, it indicates that the performance of the hypothetical classifier is equivalent to assigning each cluster the label determined by the majority vote of its constituent samples. This can be generalized using a threshold, which we indicate with γ \\gamma italic_γ . The classification results will range from strictly conservative (e.g., all clusters are considered unsafe) when the γ \\gamma italic_γ is low to non-conservative (e.g., all clusters are considered safe) with a high threshold. We plot the precision-recall (PR) curves for the LeakSealer evaluation, comparing them against the results of baseline methods. For the PR curve, the γ \\gamma italic_γ parameter controls the proportion of unsafe exemplars required to declare the whole cluster unsafe. Other baselines do not rely on the proportion of exemplars to provide a prediction. As such, we plot them as single precision-recall pairs in the plot.",
                    "Table 1: Static LeakSealer approach extrinsic evaluation. Metric OpenAI Dataset ToxicChat Dataset PII Dataset Purity 0.85 0.97 0.77 Accuracy 0.84 0.96 0.75 Precision 0.73 0.79 0.68 Recall 0.75 0.75 0.88 F1-score 0.74 0.77 0.77",
                    "Table 2: Precision, Recall, and F1 Score of Static LeakSealer against baselines. Model OpenAI ToxicChat PII Precision Recall F1 Precision Recall F1 Precision Recall F1 Llama Guard 3 0.79 0.78 0.79 0.74 0.38 0.50 0.82 0.46 0.59 Judge (DeepSeek-R1) 0.55 0.68 0.61 0.65 0.58 0.61 0.58 0.48 0.53 Judge (GPT-4o) 0.64 0.95 0.77 0.74 0.49 0.59 0.81 0.64 0.72 Judge (Ministral) 0.64 0.93 0.76 0.67 0.56 0.61 0.62 0.65 0.64 Judge (Llama 3.1) 0.54 0.93 0.69 0.60 0.41 0.49 0.70 0.55 0.62 LeakSealer 0.73 0.75 0.74 0.79 0.75 0.77 0.68 0.88 0.77",
                    "Results. Table 1 shows the evaluation results on static LeakSealer alone on all three datasets. Purity and accuracy are close to being the same value, indicating that majority voting among a cluster’s exemplars yields comparable results as majority voting across all samples within the cluster. This shows the effectiveness of employing HDBSCAN’s exemplars as representatives for the whole cluster. The highest purity is observed in the ToxicChat dataset, with a value of 0.97 0.97 0.97 . This is likely a side effect of class imbalance (around 92 % 92\\% 92 % of samples are safe and 8 % 8\\% 8 % unsafe). Nevertheless, this distribution reflects a realistic scenario and is likely even more pronounced in real-world data. Table 2 shows the results of the static LeakSealer method against baselines regarding precision and recall. These results are also plotted on a Precision-Recall curve in Figure 2 . In the figure, the points represent the baselines, whereas the curve shows how the two metrics change with the proportion of positive exemplars needed to deem a cluster unsafe. Across baselines, LeakSealer achieves the highest F1-score ( 0.77 0.77 0.77 ) on both ToxicChat and the PII dataset and closely follows the best baselines on the OpenAI dataset. Recall is particularly large in the PII domain, where LeakSealer reaches 0.88 0.88 0.88 (by far the highest among the baselines). By comparison, the second best is Ministral, with only 0.65 0.65 0.65 . This large gap demonstrates the effectiveness of our approach in identifying a broader range of past leakage scenarios. As for recall on the OpenAI Content Moderation Dataset, GPT-4o has the highest across baselines, closely followed by competitors such as Llama 3.1 and Ministral. The likely explanation for this margin with respect to ToxicChat relates to the size of the dataset. Notably, the OpenAI Content Moderation dataset comprises 1 , 680 1,680 1 , 680 samples, whereas ToxicChat contains 10 , 166 10,166 10 , 166 samples, suggesting that LeakSealer’s performance may scale better with the dataset size. It has to be noted that LLMs are trained on web-crawled data, which raises concerns regarding potential overfitting due to the availability of certain datasets online. Notably, models such as GPT-4o, Mistral, and Llama 3.1 have demonstrated strong performance. In contrast, LeakSealer employs a semi-supervised methodology without pretraining, thereby avoiding this source of bias. Furthermore, the PII dataset used in this study is unlikely to be subject to similar biases, as it has not been previously published and, therefore, could not have been incorporated into any training datasets.",
                    "Finally, we present an example of historical analysis (static LeakSealer) on the ToxicChat benchmark, summarized in Figure 3 . The figure presents a subset of the identified clusters for illustrative purposes. 3 3 3 Complete report in Appendix A . For example, Cluster 14 captures jailbreak attempts aimed at eliciting model responses related to criminal activity, while Cluster 21 includes prompts involving personal information pertaining to an individual whose name has been redacted.",
                    "Group Size Keywords Clusters • 2 50 humor, mathematics, books, problems, jokes • 12 655 numbers, sequence, trillions, counting, mathematics • 14 41 murder , crime , logic, puzzle, riddle • 18 882 bathroom, mixer, faucet, minimalist, design • 21 2048 [ redacted ], context, details, person, information Figure 3: Essential static LeakSealer Report.",
                    "(a) OpenAI dataset (b) ToxicChat dataset (c) PII dataset Figure 4: Precision-Recall curves for dynamic LeakSealer and Llama Guard 3 as the classification threshold varies.",
                    "Table 3: Evaluation results in the Dynamic setting. AUPRC does not apply to LLM-As-A-Judge techniques. Dataset Technique Acc. Prec. Rec. F1 AUPRC OpenAI Llama Guard 3 0.86 0.77 0.79 0.78 0.90 Judge (DeepSeek-R1) 0.74 0.59 0.65 0.62 – Judge (GPT-4o) 0.83 0.66 0.94 0.78 – Judge (Llama 3.1) 0.72 0.54 0.92 0.68 – Judge (Ministral) 0.84 0.70 0.91 0.79 – LeakSealer 0.83 0.73 0.75 0.74 0.83 ToxicChat Llama Guard 3 0.94 0.66 0.38 0.48 0.56 Judge (DeepSeek-R1) 0.95 0.63 0.59 0.61 – Judge (GPT-4o) 0.95 0.69 0.46 0.55 – Judge (Llama 3.1) 0.93 0.50 0.35 0.41 – Judge (Ministral) 0.95 0.64 0.58 0.61 – LeakSealer 0.96 0.84 0.63 0.72 0.76 PII Llama Guard 3 0.64 0.91 0.31 0.46 0.84 Judge (DeepSeek-R1) 0.58 0.60 0.46 0.52 – Judge (GPT-4o) 0.75 0.87 0.59 0.70 – Judge (Llama 3.1) 0.66 0.76 0.45 0.57 – Judge (Ministral) 0.60 0.60 0.59 0.59 – LeakSealer 0.91 0.88 0.95 0.92 0.97"
                ]
            },
            {
                "6.3 Evaluation in the Dynamic Setting": [
                    "We now evaluate LeakSealer not for analysis of past conversations but for its ability to defend against unseen samples. To train the classification models, we make a random 80 % − 20 % 80\\%-20\\% 80 % - 20 % split for the training and test sets, respectively. We evaluate Accuracy, Precision, Recall, F1, and AUPRC (when applicable). These results can be found in Table 3 . Additionally, we plot the Precision-Recall curves as in the static scenario (Figure 2 ). The Precision-Recall curves exhibit non-monotonic behavior. This is caused by the absence of a strict inverse relationship between precision and recall when varying the classification threshold θ \\theta italic_θ . Instead, one metric can change while the other remains constant. Consequently, the plots can form sawtooth patterns, resulting in non-monotonic behavior. Please note that LLM-As-A-Judge baselines do not produce logits (prediction confidence values), and, therefore, only LeakSealer and Llama Guard 3 are compared on the PR curves. In Table 3 , the performance of LLM-As-A-Judge is compared with logit-producing baselines with results at 0.5 0.5 0.5 prediction threshold in Table 3 . On the ToxicChat dataset, LeakSealer outperforms all baselines across all key metrics, achieving an accuracy of 0.96 0.96 0.96 , precision of 0.84 0.84 0.84 , recall of 0.63 0.63 0.63 , and an AUPRC of 0.72 0.72 0.72 . In comparison, Llama Guard 3 attains an AUPRC of 0.56 0.56 0.56 . While other baselines exhibit comparable accuracy, their low F1 scores indicate this is likely a side effect of class imbalance. On the PII benchmark, LeakSealer also achieves the highest performance across most metrics, with an accuracy of 0.91 0.91 0.91 , recall of 0.95 0.95 0.95 , F1-score of 0.92 0.92 0.92 , and an AUPRC of 0.97 0.97 0.97 . Precision follows closely at 0.88 0.88 0.88 , only slightly below Llama Guard 3’s 0.91 0.91 0.91 . Once again, LeakSealer’s recall is significantly higher than other baselines ( 0.96 0.96 0.96 ). The second best are GPT-4o and Ministral, with just 0.59 0.59 0.59 . On the OpenAI dataset, LeakSealer continues to demonstrate competitive performance. However, the highest scores for each metric are achieved by different models. Llama Guard 3 attains the highest accuracy ( 0.86 0.86 0.86 ), precision ( 0.77 0.77 0.77 ), and AUPRC ( 0.90 0.90 0.90 ), GPT-4o achieves the highest recall ( 0.94 0.94 0.94 ), whereas Ministral achieves the best F1-score ( 0.79 0.79 0.79 ). Again, in section 6.2 , we commented on possible explanations for these figures."
                ]
            }
        ],
        "7 Discussion": [
            "Static Setting Discussion. LeakSealer shows strong performance across various datasets, particularly excelling in recall for the PII dataset and achieving the highest metrics in ToxicChat. While some baseline models exhibit marginal advantages in precision for specific datasets, LeakSealer remains highly competitive, particularly with higher data availability. GPT-4o and competitor language models demonstrate some margin over LeakSealer in terms of performance on the OpenAI Content Moderation Dataset. This discrepancy may be attributed to the dataset’s relatively small size compared to the more extensive ToxicChat dataset and the possibility of bias in the evaluation introduced in LLM models at the pre-training stage. LeakSealer is not affected by similar bias since its semi-supervised pipeline does not involve pre-training.",
            "Dynamic Setting Discussion. As a deployed defense against unseen attack attempts, LeakSealer consistently outperformed all baselines on all comparative metrics on the ToxicChat benchmark. Notably, the precision is 0.84 0.84 0.84 , surpassing the second-best by a large margin. Similarly, on the PII dataset, LeakSealer significantly surpasses the baselines on most metrics, with a notable AUPRC of 0.97 0.97 0.97 . The results position LeakSealer as the best choice to actively defend against PII leakage. While Llama Guard had a slight advantage in precision ( 0.91 0.91 0.91 ), this came at the expense of recall ( 0.31 0.31 0.31 ), resulting in numerous false negatives. On the OpenAI Dataset, the results across baselines are mixed. Llama Guard 3 achieves the best accuracy, precision, and AUPRC. On the other hand, GPT-4o achieves the best recall. Ministral achieves the best F1 score. Overall, LeakSealer achieves a comparable performance to Llama Guard 3.",
            "Performance remarks. LeakSealer is a lightweight solution, particularly at inference time. The final classification model in the dynamic approach is selected based on its cross-validation performance among Support Vector Machines (SVM), Random Forest (RF), XGBoost, and k-nearest Neighbors (k-NN) candidates. These traditional machine learning architectures are inherently efficient, offering lower latency than LLMs or encoder-based alternatives. Moreover, these architectures do not require costly, high-performance hardware accelerators for deployment, making them more accessible and resource-efficient. The overhead on the human element is minimized by collecting similar samples in groups and selecting meaningful representatives. The experimental evaluation has shown that the exemplars effectively characterize the cluster from a semantic perspective. Finally, selecting keywords that summarize the exemplars further simplifies the analysis.",
            "Adaptability and Concept Drift. Over time, new incoming samples may form distinct clusters representing emerging topics or novel attack strategies - a phenomenon known as concept drift . In the case of LeakSealer, updating the active defense model to accommodate such drift simply involves running HDBSCAN on recent data points and forwarding the resulting exemplars and outliers to a human evaluator. Consequently, the semi-supervised nature of LeakSealer facilitates seamless adaptation to evolving data distributions. In contrast, existing baseline methods typically require costly, supervised retraining procedures to cope with concept drift."
        ],
        "8 Conclusions": [
            "Prompt Injection attacks pose significant threats to LLM systems, potentially compromising service reputation and sensitive data confidentiality, particularly in RAG scenarios. This paper introduced LeakSealer, a lightweight and model-agnostic framework designed for the historical analysis of conversations with an LLM system (static setting) and active defense (dynamic setting). In the first setting, LeakSealer demonstrated superior performance in detecting interactions affected by prompt injection attacks, achieving high cluster purity (0.97 on the ToxicChat dataset). Moreover, it outperformed baselines in personally identifiable information (PII) leakage detection tasks, showing a notable recall improvement (0.88 compared to 0.65 for the best-performing baseline). ). In dynamic scenarios, LeakSealer exhibited enhanced performance with an area under the precision-recall curve (AUPRC) of 0.97, surpassing the leading baseline, Llama Guard (0.84).",
            "Limitations and Future Work. To address the inherently broad and context-dependent nature of prompt injection and PII leakage, toxicity of generated text was adopted as a representative measure for evaluating unsafe behaviors, aligning with standard practices in related literature. Additionally, a curated and diverse dataset for PII leakage detection has been developed and will be open-sourced alongside LeakSealer. Future research directions include adapting our approach for online training scenarios, improving its adaptability to highly-dynamic scenarios.",
            ""
        ]
    },
    {
        "id": "2508.00579v2",
        "title": "MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for\n  Document Question-Answering with Hierarchical Index and Multi-Granularity\n  Retrieval",
        "authors": [
            "Ziyu Gong",
            "Yihua Huang",
            "Chengcheng Mai"
        ],
        "Abstract": "Abstract The multi-modal long-context document question-answering task aims to locate and integrate multi-modal evidences (such as texts, tables, charts, images, and layouts) distributed across multiple pages, for question understanding and answer generation. The existing methods can be categorized into Large Vision-Language Model (LVLM)-based and Retrieval-Augmented Generation (RAG)-based methods. However, the former were susceptible to hallucinations, while the latter struggled for inter-modal disconnection and cross-page fragmentation. To address these challenges, a novel multi-modal RAG model, named MMRAG-DocQA, was proposed, leveraging both textual and visual information across long-range pages to facilitate accurate question answering. A hierarchical indexing method with the integration of flattened in-page chunks and topological cross-page chunks was designed to jointly establish in-page multi-modal associations and long-distance cross-page dependencies. By means of joint similarity evaluation and large language model (LLM)-based re-ranking, a multi-granularity semantic retrieval method, including the page-level parent page retrieval and document-level summary retrieval, was proposed to foster multi-modal evidence connection and long-distance evidence integration and reasoning. Experimental results performed on public datasets, MMLongBench-Doc and LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in understanding and answering modality-rich and multi-page documents. Our code is available at https://github.com/Gzy1112/MMRAG-DocQA.",
        "Introduction": [
            "Document question-answering (Doc-QA) aims to answer questions based on document content. With the rise of Large Vision-Language Models (LVLMs), Doc-QA research (Ding et al. 2022 ; Mishra et al. 2019 ; Ma et al. 2025 ; Tanaka et al. 2023 ; Luo et al. 2024 ) has transitioned from simple text-based approaches to more sophisticated multi-modal methods. In multi-modal long-context Doc-QA (Suri et al. 2025 ; Cho et al. 2024 ; Han et al. 2025 ) , commonly seen in scientific papers, business reports, and instructional manuals, etc., relevant evidences needed to answer a question are often scattered across multiple pages and modalities, including texts, tables, charts, layouts and images, which poses challenges for complex multi-modal comprehension and long-distance reasoning over scattered content.",
            "Existing methods for multi-modal long-context Doc-QA can be categorized into two types: LVLM-based methods (Lu et al. 2024 ; Dong et al. 2024 ; Hu et al. 2024 ) and RAG-based methods (Xing et al. 2025 ; Xia et al. 2024 , 2025 ) . LVLM-based methods directly processed the entire document by using large multi-modal models, jointly encoding both textual and visual features. However, these models tended to suffer context length limitations, insufficient long-range reasoning capabilities, and fact hallucinations. RAG-based methods focused on retrieving relevant evidences from the document before answer generation, which improved scalability but still failed to capture complex multi-modal dependencies and overlook cross-page evidence.",
            "(a) Multi-modal connection (b) Long-distance reasoning Figure 1: Two challenges for multi-modal long-context document question-answering.",
            "Based on the above research status of existing methods for the multi-modal Doc-QA, we listed two key challenges and outlined the corresponding solutions, as follows:",
            "Challenge 1: Absence of multi-modality connections. Multi-modal long-context document question-answering requires the integration of multi-modal information to synthesize accurate answers, rather than relying solely on either textual or visual cues. As shown in Figure 1(a) , for the question “how do 5% of the Latinos see economic upward mobility for their children?”, several textual evidences are relatively easy to be retrieved, such as plain text and layout-based image caption, since the question shares common keywords and semantic meaning with these textual content. However, the actual answer “Less well off” resides within the visual image, which struggles to yield a high retrieval score due to few direct textual overlaps and semantic cues belonging to the pie chart. Therefore, it is essential to establish connections between the corresponding visual elements and surrounding textual information.",
            "Solution 1: Build multi-modal connections by combining in-page indexing structure with page-level parent page retrieval method. To address the challenge of multi-modality disconnection, a flattened in-page indexing strategy combined with the page-level parent-page retrieval was presented. Since semantically relevant textual and visual elements tend to co-occur in the same page, the parent page contains multi-modal retrieved contents related to the answer, which helps to bridge different modalities. This method transformed textual evidence into entry points for accessing associated visual information, enabling the model to aggregate multi-model evidence more effectively.",
            "Challenge 2: Lack of cross-page evidence linking and reasoning ability. Another difficulty in multi-modal long-context Doc-QA is the isolation of evidence dispersed across different pages, which requires models to reason and aggregate long-distance cross-page evidences. As shown in Figure 1(b) , for the question “I’m a Macbook Air user in Mexico. According to this guidebook, which number shall I call for seeking AppleCare service and support?”, the relevant evidences span multiple modalities, including plain text and tables, and are distributed across different pages. Specifically, the actual phone number “1-800-275-2273” appears in a table on page 55, while the explanatory instruction, indicating that users should “call the support center number nearest you” is located on page 54. This demands the model to associate cross-page evidences and perform multi-step reasoning across multiple document sections, which is also not available in most existing models.",
            "Solution 2: Achieve long-distance reasoning by combining cross-page indexing structure with document-level summary retrieval method. To solve this problem, a topological cross-page indexing strategy combined with document-level summary-based retrieval was proposed. Semantically related content from different pages are grouped together through clustering and summarized by large language models, thereby promoting the retrieval scope to span across multiple pages. This method helped to aggregate evidence scattered across different pages, promoting long-distance reasoning.",
            "Considering the above challenges and solution ideas, a novel retrieval-augmented generation method, named MMRAG-DocQA, was proposed for the multi-modal long-context document question-answering task. The major contributions can be summarized as follows: • In proposed MMRAG-DocQA, a hierarchical index structure with flattened in-page and topological cross-page chunks was proposed to establish correlations between diverse modalities and multiple pages. • A multi-granularity retrieval method with page-level parent page retrieval and document-level summary retrieval was also proposed to facilitate multi-modal evidence connection and long-distance evidence integration and reasoning. • Extensive experiments conducted on two public datasets, MMLongBench-Doc and LongDocURL, verified the superiority and effectiveness of our method for the multi-modal long-context Doc-QA task.",
            "Figure 2: Overview of MMRAG-DocQA with hierarchical index and multi-granularity retrieval for multi-modal Doc-QA. ( P n P_{n} is the parent page of c n c_{n} , c l s K cls_{K} is the clustered block, ‘CoT’ denotes chain-of-thought and ‘SO’ denotes a structured output format.)"
        ],
        "Our MMRAG-DocQA Methodology": [
            {
                "Task Description": [
                    "The objective of multi-modal long-context Doc-QA is to answer questions based on document content that contains both textual and visual content (e.g., texts, tables, charts, layouts and images), where relevant evidences may be dispersed across multiple pages and modalities.",
                    "Figure 2 presents an overview of our MMRAG-DocQA method. Given a query Q Q and its corresponding document D = { d 1 , d 2 , … , d N } D=\\{d_{1},d_{2},...,d_{N}\\} , where N N is denoted as the page number and d i d_{i} represents the i i -th page.The multi-modal long-context Doc-QA can be formalized as follows: Y a n s \\displaystyle Y_{ans} = L L M θ ( y i | y < i , Q , R q c o n t e x t , P C o T ) \\displaystyle={LLM}_{\\theta}(y_{i}|y_{<i},Q,R_{q}^{context},P_{CoT}) (1) R q c o n t e x t \\displaystyle R_{q}^{context} = M M R e t r i e v e r ( H i e r I n d e x ( D ) , Q ) \\displaystyle=MMRetriever(HierIndex(D),Q) (2) where Y a n s Y_{ans} is the answer response generated by L L M θ {LLM}_{\\theta} , R q c o n t e x t R_{q}^{context} represents multi-modal retrieved evidences, H i e r I n d e x ( ⋅ ) HierIndex(\\cdot) is the hierarchical index structure for constructing retrievable corpus of D D , M M R e t r i e v e r ( ⋅ ) MMRetriever(\\cdot) is the proposed multi-granularity retriever that searches for evidences related to Q Q from document corpus, and P C o T P_{CoT} is a Chain-of-Thought (CoT) prompting strategy, which is specifically designed to guide the model in generating step-by-step reasoning paths during answer generation."
                ]
            },
            {
                "Hierarchical Index Construction Method with Multi-Modal Semantic Encoding": [
                    "For each page d i = { T i , V i } d_{i}=\\{T_{i},V_{i}\\} in document D D , the textual content, T i T_{i} , consisted of three key components: (1) pure-text that preserved in original form; (2) tables that serialized into arranged sequences to retain its row-column-cell structure; (3) layout-based text. The composition of T i T_{i} was denoted as T i = T i T e x t ∪ T i T a b l e ∪ T i L a y o u t T_{i}=T_{i}^{Text}\\cup T_{i}^{Table}\\cup T_{i}^{Layout} .",
                    "The visual context, V i V_{i} , was transformed into descriptive textual information via LVLMs, denoted as T i D e s T_{i}^{Des} , converting visual semantics into text-based semantic encoding and retrieval space. Meanwhile, the raw images and charts have also been retained as the original and complete visual features, denoted as V i r a w V_{i}^{raw} , to alleviate information loss. V i V_{i} can be represented as V i = T i D e s ∪ V i R a w V_{i}=T_{i}^{Des}\\cup V_{i}^{Raw} .",
                    "Therefore, textual and visual content of documents can be separately defined as follows: T c o r p u s \\displaystyle T^{corpus} = T T e x t ∪ T T a b l e ∪ T L a y o u t ∪ T D e s \\displaystyle=T^{Text}\\cup T^{Table}\\cup T^{Layout}\\cup T^{Des} (3) V c o r p u s \\displaystyle V^{corpus} = V R a w \\displaystyle=V^{Raw} (4)",
                    "Based on the multi-modal content obtained above, two complementary hierarchical index structures were constructed at different levels, with the following formula: I = H i e r I n d e x ( T C o r p u s ) = { I i n , I c r o s s } I=HierIndex(T^{Corpus})=\\{I_{in},I_{cross}\\} (5) where 𝑰 𝒊 𝒏 \\boldsymbol{I_{in}} represents the flattened in-page index for establishing the association of different modality information within one page, and 𝑰 𝒄 𝒓 𝒐 𝒔 𝒔 \\boldsymbol{I_{cross}} denotes the topological cross-page index for establishing the interaction of long-distance cross-page information.",
                    "A. The Flattened In-Page Index. The flattened in-page indexing converted textual information (i.e., T i c o r p u s T_{i}^{corpus} for each document page d i d_{i} ) into a page-level list of smaller and uniformly-sized textual chunks, enabling direct access to pieces of evidence and facilitating finer-grained evidence extraction for retrieval. These textual chunks in i i -th page of document D D can be denoted as c i = { c i , 1 , c i , 2 , … , c i , K i } c_{i}=\\{c_{i,1},c_{i,2},...,c_{i,K_{i}}\\} , where K i K_{i} is the chunk number of i i -th page. Language models (LMs) were utilized for encoding the text attributes, thereby learning representations that capture their semantic meaning, denoted as Z i c = L M ( c i ) Z_{i}^{c}=LM(c_{i}) . Thus, the flattened in-page index of document D D was defined as: I i n = ⋃ i = 1 N I ( Z i c ) I_{in}=\\bigcup_{i=1}^{N}I(Z_{i}^{c}) (6) where N N is the page number of document D D and I ( ⋅ ) I(\\cdot) represents the index of the encoded text chunks.",
                    "B. The Topological Cross-Page Index. The topological cross-page indexing was conducted at the document-level scope, partitioning the entire textual content (i.e., T c o r p u s T^{corpus} ) into clustered blocks with similar semantic meanings for encoding and summarizing the similar semantics. These textual blocks (denoted as B = { b 1 , b 2 , … , b K } B=\\{b_{1},b_{2},...,b_{K}\\} , where K K is the number of textual blocks in document D D ) were also encoded by language models, represented as Z i b = L M ( b i ) Z_{i}^{b}=LM(b_{i}) .",
                    "Through iterative processing, all textual blocks were organized into a topological tree, where leaf nodes retained original attributes, intermediate nodes aggregated semantically related cross-page blocks with the gaussian mixture model for clustering the text blocks, and the root node summarized the topic-level semantics of the document with the large language model, thereby creating a multi-scale representation that captures both local multi-modal details and global cross-page document structure.",
                    "For instance, at each layer l l , leaf embeddings Z i b , l − 1 Z_{i}^{b,l-1} were clustered, and summaries were generated as follows: G K l ( l ) \\displaystyle G_{K_{l}}^{(l)} = G M M C l u s t e r ( { Z i b , l − 1 } ) \\displaystyle=GMM_{Cluster}(\\{Z_{i}^{b,l-1}\\}) (7) S K l ( l ) \\displaystyle S_{K_{l}}^{(l)} = ⋃ g j ( l ) ∈ G K l ( l ) L L M S u m m a r y ( g j ( l ) ) \\displaystyle=\\bigcup_{g_{j}^{(l)}\\in G_{K_{l}}^{(l)}}LLM_{Summary}(g_{j}^{(l)}) (8) where K l K_{l} is the number of clusters, and then summaries were re-embed to form higher-layer nodes, denoted as Z i b , l = L M ( S K l ( l ) ) Z_{i}^{b,l}=LM(S_{K_{l}}^{(l)}) .",
                    "Therefore, the topological cross-page index of document D D was formulated as: I c r o s s = ⋃ l = 0 L { I ( Z i b , l ) | i = 1 , 2 , … , K l } I_{cross}=\\bigcup_{l=0}^{L}\\{I(Z_{i}^{b,l})|i=1,2,...,K_{l}\\} (9) where I ( ⋅ ) I(\\cdot) represents the index of the encoded textual nodes."
                ]
            },
            {
                "Multi-granularity Retrieval Method": [
                    "To alleviate the disconnection problem between multi-modal information and the difficulty of long-distance reasoning, a multi granularity content retriever with page-level parent-page retrieval by in-page index and document-level summary retrieval by cross-page index was presented, by searching for evidence related to the question in the corpus with hierarchical indexing. Our multi-granularity content retriever can be defined as follows: M M R e t r i e v e r ( H i e r I n d e x ( D ) , Q ) = \\displaystyle MMRetriever(HierIndex(D),Q)= (10) M M R e t r i e v e r ( I i n , Q ) ∪ M M R e t r i e v e r ( I c r o s s , Q ) \\displaystyle MMRetriever(I_{in},Q)\\cup MMRetriever(I_{cross},Q)",
                    "A. Page-level Parent Page Retrieval Method with LLM-based Re-ranking for Modality Connection. Since semantically related textual and visual evidence tend to be distributed on the same page, the parent-page retrieval method was proposed to augment the integration of multi-modal information. Firstly, we retrieved the textual content chunks most relevant to the question, and then associated chunks with their corresponding parent page. These parent pages contained more semantically similar visual and structural information, such as tables, layouts, charts, and images.",
                    "Specifically, given the query Q Q , we calculated the similarity between the embedded query and textual chunks with the flattened index I i n I_{in} , and then selected the Top K most relevant chunks based on semantic similarity as follows: C q = { c i , j c } = a r g T o p K Z i , j c ∈ I i n S i m ( L M ( Q ) , Z i , j c ) C_{q}=\\{c_{i,j}^{c}\\}=argTopK_{Z_{i,j}^{c}\\in I_{in}}Sim(LM(Q),Z_{i,j}^{c}) (11) where c i , j c c_{i,j}^{c} is the j j -th chunk on page i i of document D D and can be navigated to its source page P i P_{i} . The set of retrieved parent pages was obtained as P q = { P a r e n t P a g e ( c i , j c | c i , j c ∈ C q ) } P_{q}=\\{ParentPage(c_{i,j}^{c}|c_{i,j}^{c}\\in C_{q})\\} .",
                    "To further improve parent page retrieval, a fine-grained LLM-based re-ranking method was proposed to select pages that were more relevant to the question, where the large language model (LLM) was guided to rate the relevance between those retrieved pages P q P_{q} and the problem Q Q on a scale of 0 to 1. The final reordered pages based on the scores allocated by the LLM can be defined as: P q f i n a l = a r g T o p K P i ∈ P q L L M _ S c o r e ( Q , P i ) P_{q}^{final}=argTopK_{P_{i}\\in P_{q}}LLM\\_Score(Q,P_{i}) (12)",
                    "Meanwhile, we searched for images belonging to the parent page set P q f i n a l P_{q}^{final} in visual corpus V C o r p u s V^{Corpus} (denoted as V q V_{q} ), and then used the Large Vision-Language Model (LVLM) to provide highly-relevant evidence related to the problem for the images (denoted as V q f i n a l V_{q}^{final} ), which can be formulated as follows: V q = { v k ∈ P q f i n a l | v k ∈ V C o r p u s } \\displaystyle V_{q}=\\{v_{k}\\in P_{q}^{final}|v_{k}\\in V^{Corpus}\\} (13) V q f i n a l = L V L M ( Q , V q ) \\displaystyle V_{q}^{final}=LVLM(Q,V_{q}) (14)",
                    "Ultimately, through parent page retrieval and LLM-based re-ranking, the page-level multi-modal content retrieved by in-page indexing was defined as: M M R e t r i e v e r ( I i n , Q ) = P q f i n a l + V q f i n a l \\displaystyle MMRetriever(I_{in},Q)=P_{q}^{final}+V_{q}^{final} (15)",
                    "B. Document-level Summary Retrieval Method for Long-Distance Reasoning. To associate and reason the long-distance evidence fragments across multiple pages, we utilized the topological indexing to achieve document-level retrieval of summaries across multiple pages, serving as a supplement to the page-level retrieval method.",
                    "Based on the given question Q Q , we calculated the semantic similarity between all nodes in the topological structure and the question, and then selected the Top K relevant ones. Therefore, the document-level multi-modal content retrieved by cross-page indexing was denoted as follows: M M R e t r i e v e r ( I c r o s s , Q ) = S u m m a r y q f i n a l \\displaystyle MMRetriever(I_{cross},Q)=Summary_{q}^{final} (16) = a r g T o p K Z i b , l ∈ I c r o s s S i m ( L M ( Q ) , Z i b , l ) \\displaystyle=argTopK_{Z_{i}^{b,l}\\in I_{cross}}Sim(LM(Q),Z_{i}^{b,l})",
                    "Model Evidence Source Evidence Page Acc. F1 TXT LAY CHA TAB FIG SIN MUL UNA OCR(Tesseract ( Smith 2007 ) ) + Large Language Models(LLMs) \\textit{OCR(Tesseract\\cite[citep]{(\\@@bibref{AuthorsPhrase1Year}{Tesseract}{\\@@citephrase{ }}{})})}+\\textit{Large Language Models(LLMs)} QWen-Plus (Qwen 2024 ) 17.4 15.6 7.4 7.9 8.8 14.2 10.6 42.2 18.9 13.4 DeepSeek-V2 (Liu et al. 2024 ) 27.8 19.6 8.8 17.0 9.4 20.2 15.4 48.1 24.9 19.6 Claude-3 Opus (Anthropic 2024 ) 30.8 30.1 16.4 24.4 16.3 32.0 18.6 30.9 26.9 24.5 Gemini-1.5-Pro (Gemini et al. 2024 ) 29.3 15.9 12.5 17.7 11.5 21.2 16.4 73.4 31.2 24.8 GPT-4o (OpenAI 2024 ) 41.1 23.4 28.5 38.1 22.4 35.4 29.3 18.6 30.1 30.5 Large Visual Language Models(LVLMs) DeepSeek-VL (Lu et al. 2024 ) 7.2 6.5 1.6 5.2 7.6 5.2 7.0 12.8 7.4 5.4 InternLM-XC2-4KHD (Dong et al. 2024 ) 9.9 14.3 7.7 6.3 13.0 12.6 7.6 9.6 10.3 9.8 mPLUG-DocOwl 1.5 (Hu et al. 2024 ) 8.2 8.4 2.0 3.4 9.9 7.4 6.4 6.2 6.9 6.3 Qwen-VL (Bai et al. 2023 ) 5.5 9.0 5.4 2.2 6.9 5.2 7.1 6.2 6.1 5.4 GPT-4V (Achiam et al. 2023 ) 34.4 28.3 28.2 32.4 26.8 36.4 27.0 31.2 32.4 31.2 RAG methods ColBERTv2 (Santhanam et al. 2022 ) + LLaMA-3.1-8B 23.7 17.7 14.9 24.0 11.9 25.7 12.2 38.1 23.5 19.7 M3DocRAG (Cho et al. 2024 ) (page=4) 30.0 23.5 18.9 20.1 20.8 32.4 14.8 5.8 21.0 22.6 MMRAG-DocQA (OURS) (page=4) 41.6 30.2 40.9 48.9 25.1 48.5 31.7 74.9 48.2 41.4 MMRAG-DocQA (OURS) (page=10) 45.9 34.4 44.9 51.1 37.5 53.5 36.8 76.2 52.3 46.0 Table 1: Experimental results on the MMLongBench-Doc dataset. The highest performance was bolded and the second best performance (except for ours) was underlined. ‘page=n’ represents the number of pages retrieved from the parent page. ‘SIN’, ‘MUL’ and ‘UNA’ separately denote singe-page, cross-page and unanswerable questions.",
                    "Model Acc. OCR(Tesseract) + Large Language Models(LLMs) \\textit{OCR(Tesseract)}+\\textit{Large Language Models(LLMs)} LLaVA-OneVision (Li et al. 2025 ) 23.3 Qwen-VL (Bai et al. 2023 ) 25.0 Gemini-1.5-Pro (Gemini et al. 2024 ) 32.0 GPT-4o (OpenAI 2024 ) 34.7 O1-preview (OpenAI 2024 ) 35.8 Large Visual Language Models(LVLMs) InternLM-XC2.5 (Dong et al. 2024 ) 2.4 mPLUG-DocOwl2 (Hu et al. 2024 ) 5.3 Pixtral (Agrawal et al. 2024 ) 5.6 Llama-3.2 (Meta 2024) 9.2 LLaVA-OneVision (Li et al. 2025 ) 22.0 Qwen-VL (Bai et al. 2023 ) 30.6 RAG methods ColBERTv2+LLaMA-3.1-8B 49.1 M3DocRAG (Jain et al. 2025 ) (page=10) 52.2 MMRAG-DocQA(OURS) (page=4) 52.4 MMRAG-DocQA(OURS) (page=10) 57.2 Table 2: Experimental results on the LongDocURL dataset. The highest performance was bolded and the second best performance (except for ours) was underlined."
                ]
            },
            {
                "Answer Generation": [
                    "For each question Q Q , the retrieved evidences from corpus, such as P q f i n a l P_{q}^{final} , V q f i n a l V_{q}^{final} and S u m m a r y q f i n a l Summary_{q}^{final} , were integrated into the context window of the large language model (LLM), and the final answer was generated as follows: Y a n s \\displaystyle Y_{ans} = L L M θ ( y i | y < i , Q , R q c o n t e x t , P C o T ) \\displaystyle=LLM_{\\theta}(y_{i}|y_{<i},Q,R_{q}^{context},P_{CoT}) (17) R q c o n t e x t \\displaystyle R_{q}^{context} = P q f i n a l ∪ V q f i n a l ∪ S u m m a r y q f i n a l \\displaystyle=P_{q}^{final}\\cup V_{q}^{final}\\cup Summary_{q}^{final} (18)",
                    "To enhance response quality, a chain-of-thought (CoT) prompting method with a structured output (SO) format was also proposed for answer reasoning, which enabled direct extraction of final answers without the need for lengthy parsing. Details of prompts can be found in Appendix."
                ]
            }
        ],
        "Experiments": [
            {
                "Dataset": [
                    "We evaluated models on two public datasets for multi-modal Doc-QA. MMLongBench-Doc (Ma et al. 2024b ) comprises 135 long PDF documents, each containing an average of 47.5 pages and 21,214 tokens. It consists of 1,082 expert-annotated questions. LongDocURL (Deng et al. 2025 ) is constructed upon 396 long PDF documents, with an average length of 85.6 pages and 43,622.6 tokens. It collects 2,325 high-quality question-answer pairs. Their answers rely on evidences from multi-modalities and multi-pages."
                ]
            },
            {
                "Implementation Details": [
                    "Docling (Livathinos et al. 2025 ) was used for pdf parsing. The off-the-shelf LLMs/LVLMs were utilized for answer generation. All experiments were conducted on a single NVIDIA A100 GPU. More implementation details are in Appendix."
                ]
            },
            {
                "Metrics": [
                    "For MMLongBench-Doc and LongDocURL, we followed their official evaluation setups. We reported the accuracy of distinct evidence modality types and evidence pages. The generalized accuracy and F1 score were also recorded.",
                    "Variants Parent Page Retrieval Summary Retrieval Visual Info TXT LAY CHA TAB FIG SIN MUL UNA Acc. MMRAG-DocQA ✓(page=10) ✓ ✓ 45.9 34.4 44.9 51.1 37.5 53.5 36.8 76.2 52.3 MMRAG-DocQA v 1 v_{1} ✓(page=10) ✓ × \\times 41.0 22.8 27.3 50.9 21.9 42.0 29.4 88.2 46.9 MMRAG-DocQA v 2 v_{2} ✓(page=1) ✓ ✓ 40.6 27.6 31.9 39.7 28.5 51.2 18.9 83.9 46.6 MMRAG-DocQA v 3 v_{3} ✓(page=1) × \\times ✓ 34.7 25.1 28.6 34.3 25.7 47.2 13.9 84.8 43.3 MMRAG-DocQA v 4 v_{4} × \\times ✓ ✓ 26.4 15.6 22.8 23.8 21.9 32.2 14.0 90.1 37.5 Table 3: Ablation experiments for parent page retrieval with flattened in-page index, summary retrieval with topological cross-page index and visual information on the MMLongBench-Doc dataset."
                ]
            },
            {
                "Main Results": [
                    "We compared our MMRAG-DocQA with existing SOTA LVLM/LLM-based and RAG-based methods on the MMLongBench-Doc and LongDocURL datasets.",
                    "MMLongBench-Doc. Table 1 listed the performance of models on the MMLongBench-Doc dataset. We observed that: (1) Both LVLM-based methods and LLM-based methods with OCR-parsed documents exhibited poor performance and struggled with multi-modal comprehension and long-distance reasoning for long-context document. Our MMRAG-DocQA surpassed the best-performing LVLM, i.e., GPT-4V, by 19.9% and 14.8% in generalized accuracy and F1 score, respectively. (2) Compared with the RAG-based SOTA M3DocRAG, when the page number was four, MMRAG-DocQA achieved superiority on all metrics, with improvements of 27.2% and 18.8% on generalized accuracy and F1 score. When the page number was set to ten, MMRAG-DocQA achieved better performance with an accuracy of 52.3% and a F1 score of 46.0%. To be specific, the accuracy of visual charts (CHA) and figures (FIG) separately increased to 44.9% and 37.5%. The accuracy of multi-page (MUL) and unanswerable-questions (UNA) attained 36.8% and 76.2%, respectively. (3) These experimental results proved the advantages of our model in multiple modality understanding and long-distance reasoning, and the capability of alleviating hallucinations caused by LLMs.",
                    "LongDocURL. Table 2 showed the performance of models on the LongDocURL dataset. Similar phenomena were found: (1) Our MMRAG-DocQA achieved better performance, compared to the top-performing LVLM (i.e., Qwen2-VL) and LLM (i.e., o1-preview). (2) MMRAG-DocQA method surpassed the current SOTA M3DocRAG by a margin of 5% in terms of generalized accuracy, when the page number was set to ten. (3) These results further validated the effectiveness of our approach for the connection of multi-modality and the link of cross-page evidence.",
                    "Variants Parent Page Retrieval Summary Retrieval Visual Info Acc. MMRAG-DocQA ✓(page=10) ✓ ✓ 55.7 MMRAG-DocQA v 1 v_{1} ✓(page=10) ✓ × \\times 53.1 MMRAG-DocQA v 2 v_{2} ✓(page=1) ✓ ✓ 50.4 MMRAG-DocQA v 3 v_{3} ✓(page=1) × \\times ✓ 47.4 MMRAG-DocQA v 4 v_{4} × \\times ✓ ✓ 31.2 Table 4: Ablation experiments for parent page retrieval with flattened in-page index, summary retrieval with topological cross-page index and visual inforamtion on the LongDocURL dataset."
                ]
            },
            {
                "Ablation Experiments": [
                    "MMLongBench-Doc. Table 3 presented ablation results on the MMLongBench-Doc dataset. After removing all visual information, the performance of the variant model MMRAG-DocQA v 1 {}_{v_{1}} decreased to 46.9%, which highlighted the importance of simultaneously integrating textual and visual information for the RAG-based method in Doc-QA. MMRAG-DocQA v 2 {}_{v_{2}} restricted the number of parent page from ten to one, resulting in a performance decline to 46.6%. The drop suggested that expanding the scope of parent page retrieval can enhance model performance. The removal of summary retrieval in MMRAG-DocQA v 3 {}_{v_{3}} degraded the generalized accuracy to 43.3%, indicating that the document-level summary retrieval is essential for aggregating evidences across multiple pages. After discarding the page-level parent page retrieval, MMRAG-DocQA v 4 {}_{v_{4}} showed the lowest accuracy of 37.5%, thereby confirming the importance of parent page retrieval in integrating multi-modal content.",
                    "It is worth noting that, the accuracy of unanswerable questions (UNA) in four variant models increased, suggesting that overly broad retrieval may introduce distractors in unanswerable cases.",
                    "LongDocURL. Table 4 reported similar ablation results on the LongDocURL dataset. The accuracy of MMRAG-DocQA v 1 {}_{v_{1}} declined with the loss of visual information, proving the indispensability of multi-modal information. When the scope of parent page was restricted to one, as in MMRAG-DocQA v 2 {}_{v_{2}} , the generalized accuracy dropped to 50.4%, highlighting that retrieving a broader set of parent pages is important for contextual grounding oriented at question answering. The removal of summary retrieval in MMRAG-DocQA v 3 {}_{v_{3}} reduced the accuracy to 47.4%, emphasizing the critical role of topological cross-page summary chunks in capturing connections between dispersed evidence across pages. The worst-performing variant, MMRAG-DocQA v 4 {}_{v_{4}} , which removed parent pages, yielded an average accuracy of 31.2%, suggesting the role of parent page retrieval in enabling rich multi-modal comprehension.",
                    "(a) Page Number. (b) Summary Number. Figure 3: The trend of our MMRAG-DocQA model performance changing with the page number and summary number on the MMLongBench-Doc dataset."
                ]
            },
            {
                "Parameter Analysis on Content Size": [
                    "Figure 3 showed the parameter analysis of retrieved page number and summary number for answer generation. MMRAG-DocQA reached peak accuracy and F1 score when the parent page count was set to 10, but began to decline when the number of pages exceeded 10, which suggested that blindly increasing the page count may negatively impact performance. Meanwhile, as the number of summaries increased, the performance of MMRAG-DocQA initially improved and subsequently declined, reaching its peak when the summary count was 10, which further supported the notion that an excessive amount of content may introduce irrelevant or distracting information, thereby negatively impacting the quality of the responses. Therefore, we set the number of pages and summaries to 10 to ensure that the retrieved content contains more evidence and avoids noisy information.",
                    "Cornerstone LLMs MMLongBench-Doc Acc. LongDocURL Acc. Qwen-turbo 52.3 55.7 GPT-4o 46.7 57.2 DeepSeek-chat 51.8 57.0 ERNIE-turbo 45.9 48.6 Table 5: Extension on different LLMs for answer generation.",
                    "(a) Test Case in MMLongBench-Doc. (b) Test Case in LongDocURL. Figure 4: Case Study on dataset MMLongBench-Doc and LongDocURL to compare the answer response of our MMRAG-DocQA and LVLM-based methods (such as GPT-4o, DeepSeek-chat, Qwen-VL-Plus and ERNIE-Turbo)."
                ]
            },
            {
                "Extension on LLMs for Answer Generation": [
                    "As shown in Table 5 , four different LLMs were selected as the cornerstone models to evaluate the performance impact on the document question-answering task. We found that Qwen-turbo achieved the highest accuracy at 52.3% on the MMLongBench-Doc dataset, while GPT-4o attained the best performance with an accuracy of 57.2% on the LongDocURL dataset. These results demonstrated that our proposed Retrieval-Augmented Generation (RAG)-based method, MMRAG-DocQA, achieved excellent response on different large language models, which again validated the universality and transferability of our method."
                ]
            },
            {
                "Case Study": [
                    "Figure 4 demonstrated two cases from the MMLongBench-Doc and LongDocURL datasets. The first question asked the comparison of fan numbers, with the answer required integrating textual statistics of Vietnam from page 28 with visual data of Appota on page 31. VLVM-based methods, such as Deepseek-chat and Qwen-VL-Plus, failed to answer the question. However, MMRAG-DocQA correctly retrieved fan count from separate pages and concluded that Appota had more followers. The second question asked the number of 1492-MCA branch circuit breakers’ rating, with the answer found in structured tables across page 110 and 111. GPT-4o and Qwen-VL-Plus gave incorrect answers of 9 and 10, respectively, which failed to identify all the entries. However, MMRAG-DocQA correctly located the relevant table and identified the rating values. In summary, these cases again demonstrated that our method is adaptable for the comprehension of multi-modal and multi-page evidences that retrieved from document content."
                ]
            }
        ],
        "Related Work": [
            "Document question-answering has progressed from processing textual documents (Joshi et al. 2017 ; Kwiatkowski et al. 2019 ; Zhu et al. 2022 ) to tackle lengthy documents involved with multi-modal elements and complicated structures across multiple pages (Ma et al. 2024b ; Deng et al. 2025 ; Hui, Lu, and Zhang 2024 ) , which demands capability of modality comprehension and long-distance reasoning.",
            "LVLM-based Methods for Multi-modal Doc-QA Task. Large Vision-Language Models (Bai et al. 2023 ; Hu et al. 2024 ; Dong et al. 2024 ) were regarded as an effective solution to handle multi-modal document question-answering, since they combined the deep linguistic capabilities of large language models with advanced visual processing for document images. However, Ma et al. (Ma et al. 2024b ) and Deng et al. (Deng et al. 2025 ) indicated that LVLMs still faced challenges in integrating evidences from different modalities and pages, and were prone to hallucinations.",
            "RAG-based Methods for Multi-modal Doc-QA Task. Traditional Retrieval-Augmented Generation (RAG)-based models (Lewis et al. 2020 ; Khattab and Zaharia 2020 ) exhibited a uni-modality bias, predominantly relying on textual information while inadequately incorporating visual evidence from documents. To fully exploit visual elements within documents, Colpali (Faysse et al. 2025 ) , DSE (Ma et al. 2024a ) and VisRAG (Yu et al. 2025 ) directly encoded the images of document pages for retrieval. MDocAgent (Han et al. 2025 ) separately used text-based and image-based agents to handle textual and visual information, thereby obtaining critical information within their respective modalities in the retrieval phrase and generating refined answers. However, these existing RAG-based methods often overlooked the mutual connections between different modalities of information and remained inadequate in addressing cross-page integration and reasoning challenges."
        ],
        "Conclusion": [
            "A retrieval-augmented generation method (MMRAG-DocQA) was presented for multi-modal long-context document question-answering. A hierarchical index structure with flattened in-page and topological cross-page index was constructed to establish multi-modal connection and long-distance linkage. A multi-granularity retrieval with page-level parent page retrieval and document-level summary retrieval was proposed for searching required evidences, which were scattered across multi-modalities and multi-pages. Experiments conducted on two public datasets demonstrated the superiority of MMRAG-DocQA in multi-modal long-context Doc-QA."
        ],
        "Acknowledgments": [
            "This work was supported by the National Natural Science Foundation of China (Nos.61572250 and 62476135). Jiangsu Province Science & Tech Research Program(BE2021729), Open project of State Key Laboratory for Novel Software Technology, Nanjing University (KFKT2024B53), Jiangsu Province Frontier Technology Research and Development Program (BF2024005), Nanjing Science and Technology Research Project (202304016) and Collaborative Innovation Center of Novel Software Technology and Industrialization, Jiangsu, China."
        ],
        "Appendix": [
            {
                "Research Motivation": [
                    "Multi-modal long-context document question-answering (Doc-QA) involves answering queries by analyzing and integrating evidences across texts, tables, charts, images and layouts within multiple pages, requiring the capability of multi-modal connection and long-distance reasoning. However, there are currently few multi-modal long-context Doc-QA methods equipped with these abilities, which are worth further research.",
                    "Figure 5 illustrates the capabilities required for multi-modal long-context Doc-QA and the advantage of our proposed MMRAG-DocQA method. For the question “What percentage of respondents of the sector in which 15% are doing promotions to customers over Wi-Fi use wifi at stores?”, evidences required for the answer are scattered in the visual charts on page 11 and page 14. To correctly answer this question, multi-modal Doc-QA methods need to establish multi-modal connections. Due to the common keywords and similar semantics, the question is relatively easy to retrieve the textual titles “% RESPONDENTS USING WIFI AT STORES” in page 11 and “Are you doing promotions to customers over Wi-Fi?” in page 14. Therefore, it is crucial to associate these textual captions with their surrounding related visual bar charts. Multi-modal Doc-QA methods also need to achieve long-distance reasoning to synthesize information on these non-consecutive pages. To be specific, Doc-QA methods first require to identify the “Hospitality” industry (the only category with a 15% promotion from the visual chart on page 14), and then retrieve its corresponding 100% Wi-Fi usage rate from the data on page 11.",
                    "Our MMRAG-DocQA first confirmed from page 14 that hospitality is the only industry that conducts a 15% promotion through Wi-Fi, then found the proportion of respondents in the hotel industry who use Wi-Fi in stores from page 11, and finally concluded the correct answer 100 through reasonable multi-step reasoning. This indicates that our MMRAG-DocQA has the ability of modality correlation and long-distance reasoning, which is crucial for integrating multi-modal cross-page evidences to answer questions.",
                    "Figure 5: Necessities of multi-modal connection and long-distance reasoning for multi-modal long-context Doc-QA methods."
                ]
            },
            {
                "Implementation Details": [
                    "In the settings of our MMRAG-DocQA, we used docling to achieve pdf parsing and used off-the-shelf Large Vision-Language Models (such as Qwen-vl-plus) to describe visual information in documents. For the flattened in-page index, the chunk size was set to 300 tokens (about 15 sentences) and encoded by text-embedding-v4 provided by Qwen. To avoid information loss due to text split, a text overlap with 50 tokens was added. For the topological cross-page index, the chunk size was set to 100 (about 5 sentences) and encoded by multi-qa-mpnet-base-cos-v1 provided by Sentence-Transformer. The gaussian mixture model was used for chunk clustering, and Gpt-3-turbo was adopted for summarizing the clustered chunks. For multi-granularity evidence retrieval, we selected 10 highest-scored parent pages and summaries as the input context for each question. We used Large Vision-Language Models (such as Qwen-turbo) to conduct retrieval re-ranking. We directly used off-the-shelf LLMs for answer generation. All experiments were conducted on a single NVIDIA A100 GPU."
                ]
            },
            {
                "Prompt Setting": [
                    "For answer generation in multi-modal long-context document question-answering, a multi-step reasoning method was proposed for encompassing evidence curation and chain-of-thought reasoning for the retrieved relevant sources.",
                    {
                        "Prompt for Answer Generation": [
                            "The prompt for answer generation can be divided into several parts: • Template for Context and Questions • General Guidelines • Response Formats • Question-Answer Examples",
                            "A. Template for Context and Questions",
                            "Here is the context:{context} Here is the question:{question}",
                            "B. General Guidelines",
                            "You are a RAG (Retrieval-Augmented Generation) answering system. Your task is to answer the given question based only on information from the pdf report, which is uploaded in the format of relevant evidences extracted using RAG. Before giving a final answer, carefully think out loud and step by step. Pay special attention to the wording of the question. - Keep in mind that the content containing the answer may be worded differently than the question. - The question was autogenerated from a template, so it may be meaningless or not applicable to the given report.",
                            "C. Response Formats",
                            "The response format consists of four parts: (1) Step By Step Analysis, (2) Reasoning Summary, (3) Relevant Pages, and (4) Final Answer.",
                            "Step By Step Analysis Detailed step-by-step analysis of the answer with at least 5 steps and at least 150 words. Pay special attention to the wording of the question to avoid being tricked.",
                            "Reasoning Summary Concise summary of the step-by-step reasoning process. Around 50 words.",
                            "Relevant Pages List of page numbers containing information directly used to answer the question. Include only: - Pages with direct answers or explicit statements. - Pages with key information that strongly supports the answer. Do not include pages with only tangentially related information or weak connections to the answer. At least one page should be included in the list.",
                            "Final Answer Note: different prompts for different data types.",
                            "The expected answer data for questions includes various types, such as List, Integer, String and Float. Different prompt variants have been proposed to achieve higher-quality responses.",
                            "Final Answer [List] A list of values extracted from the context. Each value should be: - For strings: exactly as it appears in the context - For numbers: converted to appropriate type (int or float) - For nested lists: maintain the original structure - Return ‘Not answerable’ if information is not available in the context",
                            "Final Answer [Integer] An integer value is expected as the answer. - Pay attention to units (thousands, millions, etc.) and adjust accordingly - Round to nearest integer if necessary - Return ‘Not answerable’ if: - The value is not an integer - Information is not available - Currency mismatch occurs",
                            "Final Answer [String] A string value is expected as the answer. - Extract exactly as it appears in the context - Do not modify or summarize the text - Return ‘Not answerable’ if information is not available in the context",
                            "Final Answer [Float] A floating-point number is expected as the answer. - Maintain original decimal precision from the context - Pay attention to units (thousands, millions, etc.) and adjust accordingly - Return ‘Not answerable’ if: - The value is not a number - Information is not available - Currency mismatch occurs",
                            "D. Question-Answer Examples",
                            "Depending on different data types, several question-answer examples are also provided to improve response quality, which contain step by step analysis, reasoning summary, relevant pages and final answer. It helps the large language model to respond required structured output format for different answer types.",
                            "Examples of answer type as a list, a float value, an integer value and a string are shown as follows:",
                            "Examples [List] Question What are the quarterly revenue figures for Apple Inc. in 2022? Answer step_by_step_analysis: 1. The question asks for quarterly revenue figures for Apple Inc. in 2022, which implies we need to find four distinct values corresponding to each quarter. 2. Examining the context, we find a table titled ‘Quarterly Financial Results’ on page 45 that lists revenue figures for each quarter of 2022. 3. The table shows: Q1(123.9B), Q2(97.3B), Q3(82.96B), Q4(90.15B). 4. We verify these are indeed revenue figures by checking the column header and accompanying notes. 5. The values are extracted exactly as presented, converted to float type for consistency. reasoning_summary: The ‘Quarterly Financial Results’ table on page 45 provides the exact quarterly revenue figures for 2022, which are extracted and converted to float values. relevant_pages: [45] final_answer: [123.9, 97.3, 82.96, 90.15]",
                            "Examples [Integer] Question How many employees did Tesla Inc have at the end of 2022? Answer step_by_step_analysis: 1. The question asks for the number of Tesla Inc. employees at the end of 2022, which should be a whole number. 2. On page 56 of the annual report, we find the statement: ‘As of December 31, 2022, we employed approximately 127,855 full-time employees worldwide.’ 3. The number 127,855 is explicitly stated as the employee count. 4. We verify this is a global total by checking the context which mentions ‘worldwide’. 5. No unit conversion is needed as this is already a direct count. reasoning_summary: Page 56 explicitly states Tesla employed 127,855 full-time employees worldwide as of December 31, 2022. relevant_pages: [56] final_answer: 127855",
                            "Examples [String] Question What’s the percentage of people who are democrats and voted in the last election compared to the entire population in 2024? Answer step_by_step_analysis: 1. Question requires two precise data points: democrat voters and total population 2. Searched for ‘2024 election’ references - none found 3. Checked all demographic sections - no voting breakdown by party 4. Verified document metadata - report finalized Q3 2023 (pre-election) 5. Attempted alternative queries - no matching tables/charts 6. Conclusion: Data unavailable in this report reasoning_summary: Document contains no 2024 election data (pre-dates election) and lacks democrat-specific voting percentages, making question unanswerable. relevant_pages: [] final_answer: Not Answerable",
                            "Examples [Float] Question What was the gross profit margin percentage for NVIDIA Corporation in Q3 2022? Answer step_by_step_analysis: 1. The question asks for NVIDIA’s gross profit margin percentage in Q3 2022, which should be a decimal number. 2. On page 32 of the quarterly report, we find the statement: ‘Gross margin for the quarter was 53.6%, down from 56.1% in the prior quarter.’ 3. The value 53.6% is explicitly stated as the gross margin for the quarter. 4. We verify this is for Q3 2022 by checking the report header and date. 5. The percentage is converted to its decimal equivalent (53.6). reasoning_summary: Page 32 states NVIDIA’s Q3 2022 gross margin was 53.6%, which is converted to the decimal value 53.6. relevant_pages: [32] final_answer: 53.6"
                        ]
                    },
                    {
                        "Prompt for LLM-Based Re-Ranking": [
                            "The prompt for LLM-based re-ranking in the page-level parent page retrieval can be devided into two parts: (1) Template for Pages and Questions, (2) General Guidelines.",
                            "A. Template for Pages and Questions",
                            "Here is the query: {query} Here is the retrieved text block: {retrieved_page}",
                            "B. General Guidelines",
                            "You are a RAG (Retrieval-Augmented Generation) retrievals ranker. You will receive a query and retrieved text block related to that query. Your task is to evaluate and score the block based on its relevance to the query provided. Instructions: 1. Reasoning: Analyze the block by identifying key information and how it relates to the query. Consider whether the block provides direct answers, partial insights, or background context relevant to the query. Explain your reasoning in a few sentences, referencing specific elements of the block to justify your evaluation. Avoid assumptions—focus solely on the content provided. 2. Relevance Score (0 to 1, in increments of 0.1): 0 = Completely Irrelevant: The block has no connection or relation to the query. 0.1 = Virtually Irrelevant: Only a very slight or vague connection to the query. 0.2 = Very Slightly Relevant: Contains an extremely minimal or tangential connection. 0.3 = Slightly Relevant: Addresses a very small aspect of the query but lacks substantive detail. 0.4 = Somewhat Relevant: Contains partial information that is somewhat related but not comprehensive. 0.5 = Moderately Relevant: Addresses the query but with limited or partial relevance. 0.6 = Fairly Relevant: Provides relevant information, though lacking depth or specificity. 0.7 = Relevant: Clearly relates to the query, offering substantive but not fully comprehensive information. 0.8 = Very Relevant: Strongly relates to the query and provides significant information. 0.9 = Highly Relevant: Almost completely answers the query with detailed and specific information. 1 = Perfectly Relevant: Directly and comprehensively answers the query with all the necessary specific information. 3. Additional Guidance: - Objectivity: Evaluate block based only on their content relative to the query. - Clarity: Be clear and concise in your justifications. - No assumptions: Do not infer information beyond what’s explicitly stated in the block.",
                            ""
                        ]
                    }
                ]
            }
        ]
    },
    {
        "id": "2508.00476v1",
        "title": "GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting\n  Transcripts",
        "authors": [
            "Jeongwoo Kang",
            "Markarit Vartampetian",
            "Felix Herron",
            "Yongxin Zhou",
            "Diandra Fabre",
            "Gabriela Gonzalez-Saez"
        ],
        "Abstract": "Abstract This paper documents GETALP’s submission to the Third Run of the Automatic Minuting Shared Task at SIGDial 2025. We participated in Task B: question-answering based on meeting transcripts.\nOur method is based on a retrieval augmented generation (RAG) system and Abstract Meaning representations (AMR). We propose three systems combining these two approaches.\nOur results show that incorporating AMR leads to high-quality responses for approximately 35% of the questions and provides notable improvements in answering questions that involve distinguishing between different participants (e.g., who questions).",
        "1 Introduction": [
            "The 2025 edition of the Automatic Minuting (AutoMin) Shared Task introduces, for the first time, a question-answering challenge based on extensive meeting transcripts. This task (task B) involves generating accurate answers grounded in long conversational data.",
            "To address this challenge, we propose a retrieval-augmented generation (RAG) approach enriched with Abstract Meaning Representation (AMR). Specifically, we leverage Information Retrieval (IR) techniques to identify and extract relevant passages from large transcripts based on a given question. Relevant passages are identified using both dense sentence embeddings and synthetic queries generated via the Doc2Query model Nogueira et al. ( 2019 ) . To represent the relationships described in the meeting, we include a Knowledge graph from an AMR of the retrieved sentences. These graphs are then translated into natural language descriptions. Finally, we utilize the capabilities of large language models (LLMs) to generate accurate responses using both the user question and the retrieved context. Our approach thus consists of two main stages: (1) context construction, and (2) answer generation.",
            "To analyze the impact of AMR-based context, we develop and evaluate three system variants: 1. IR-only: Using only the retrieved sentences (from sentence and Doc2Query representations). 2. IR+AMR: Using both the retrieved sentences and their AMR natural language descriptions. 3. AMR-only: Using only the AMR natural language descriptions of the retrieved sentences.",
            "Finally, we evaluate each variant using the LLM-as-Judge metric Kim et al. ( 2023 ) , and we further conduct a manual evaluation to qualitatively assess the performance of the systems using the same scale."
        ],
        "2 Related Work": [
            {
                "2.1 QA based on Meeting Transcripts": [
                    "Previous work on question answering from meeting transcripts has explored both extractive and generative approaches. Apel et al. ( 2023 ) address real questions in meeting dialogues using an extractive model that jointly predicts answers and detects when no answer is present; the authors report moderate performance and note the difficulty of handling ambiguous or unanswered questions. Prasad et al. ( 2023 ) use models like Longformer and RoBERTa to extract multi-span answers from full or partial transcripts, but highlight that performance remains well below human level due to the complexity of long, dispersed dialogues. Pan et al. ( 2024 ) propose a two-step approach that first compresses transcripts using summarization, then applies QA models to the shortened text; results improve with compression, though performance depends heavily on the quality of the summaries. Golany et al. ( 2024 ) introduce a RAG pipeline where relevant segments are retrieved and used to generate answers; this approach improves handling of dispersed information but can be sensitive to retrieval errors."
                ]
            },
            {
                "2.2 RAG": [
                    "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020 ) is a framework designed to enhance the performance of LLMs by incorporating external knowledge through information retrieval. Instead of relying solely on the model’s parametric memory, RAG systems retrieve relevant documents from a collection —such as a database or the Internet— and use these documents as additional context to ground the model’s generation. This paradigm has proven effective for injecting up-to-date or domain-specific knowledge into LLMs and improving factual consistency in their outputs. In typical RAG pipelines, user queries are first augmented with retrieved passages, which are then fed into the LLM to generate responses that are both informative and grounded in external sources. A key advantage of RAG is its ability to mitigate the \"lost in the middle\" phenomenon (Liu et al., 2024 ) —where LLMs overlook relevant content located in the middle of long contexts — by ensuring that only the most relevant content is presented to the model. However, RAG systems also face notable challenges, notably in effectively managing long contexts and multi-document question answering."
                ]
            },
            {
                "2.3 Meaning representation for question answering": [
                    "Previous work leveraged meaning representations for question answering tasks (Kapanipathi et al., 2021 ; Wang et al., 2023 ) . Meaning representation represents meanings of a text in a structured form such as a graph, tree, or formal logic expressions. Corporating structured information into QA systems provides a few advantages. First, meaning representation reduces ambiguity by explicitly encoding one plausible interpretation among many others. For example, in the following sentence “Kevin told Tom that he broke the glass,” it is unclear whether ‘he’ refers to Kevin or Tom. This ambiguity can be resolved by explicitly representing its meaning. Second, meaning representation provides information in canonical form regardless of the surface-level variations-especially syntatic ones. For example, “Mary bought the flower.” and “The flower was bought by Mary” are expressed identically in a meaning representation, thereby reducing the search space in information retrieval systems. Because of these advantages, meaning representation is widely adopted in traditional QA systems.",
                    "Among many meaning representation frameworks, Abstract Meaning Representation (Banarescu et al., 2013 , AMR) has gained popularity due to its broad semantic coverage and availability of annotated data. AMR encodes meaning of texts as a rooted, directed and acyclic graph (see Figure 2.3 ). In AMR graph, the graph nodes are either: Propbank predicate ( e.g., sell-01 in Figure 2.3 ) or English words ( e.g., man and flower in Figure 2.3 ) or AMR-speicifc entities ( e.g., date-entity and ordinal-entity). Edges between nodes are labeled to indicate semantic relations between the connected nodes. For example, in Figure 2.3 , :ARG0 and :ARG1 respectively indicates that man is the agent of sell-01 and flower is the object of the same predicate. AMR graph can also be serialized in a textual format (see Figure 2 ), which is both human and machine-readable. AMR also uses variables to identify each node, e.g., s, m and f in Figure 2 . It can also be decomposed into a set of triples that represent the underlying graph structure.",
                    "Figure 1: AMR graph for “A man breaks a window.” (s / sell-01 :ARG0 (m / man) :ARG1 (f / flower)) Figure 2: AMR graph linearized in text format.",
                    "With ongoing paradigm shift with large langage model, however, the advantage of using AMR as an input for downstream tasks has been questioned. For example, Jin et al. ( 2024 ) argues that AMR, in its traditional graph form, is not optimal for LLMs, showing that incorporating it offers no improvement across five different NLP subtasks. On the contrary, Zhang et al. ( 2025 ) presents evidence supporting the usefulness of AMR when its format is adapted for LLMs. They argue that since LLM is heavily trained with human languages, the structured format of AMR may not align well with their training. To address it, they propose translating the graph into a set of textual descriptions by converting each triple of an AMR graph into a natural language sentence. They show that these natural language descriptions of an AMR graph improve the performance of various downstream tasks, both in zero-shot and fine-tuning scenarios. Following their work, we corporate AMR into QA systems while converting its structured form into natural language descriptions."
                ]
            }
        ],
        "3 Methodology": [
            {
                "3.1 Dataset Description": [
                    "We use the two datasets provided for AutoMin TaskB: the ELITR Minuting Corpus and the ELITR-Bench Dataset.",
                    "ELITR Minuting Corpus (Nedoluzhko et al., 2022 ) consists of transcripts of meetings in Czech and English. On average, each transcript contains 7,000 words, involves 5.9 speakers, and includes 727 speaker turns. ELITR-Bench Dataset Thonet et al. ( 2024 ) 1 1 1 https://github.com/utter-project/ELITR-Bench/tree/main contains questions to be answered using the English transcripts from the ELITR Minuting Corpus, splitted in two corpus: Dev and Test. In total, the Dev split comprises 10 meetings with 141 questions and is used for model validation prior to submission. The Test split, used for the final evaluation in the shared task, includes 8 meetings with 130 questions. While only the English transcripts are used for the task, the questions are in English (monolingual setting) or in Czech (cross-lingual setting)."
                ]
            },
            {
                "3.2 RAG System Overview": [
                    "Our system follows a two-stage RAG architecture (1) context construction, and (2) Answer generation. Given an input Question, denoted as Q Q italic_Q , and a meeting transcript, denoted as D O C DOC italic_D italic_O italic_C , the system produces an answer A A italic_A that is based on the content of the transcript.",
                    "In the context construction stage, we apply information retrieval (IR) techniques to identify and extract relevant passages from the transcript D O C DOC italic_D italic_O italic_C , based on the input question Q Q italic_Q . We denote this context as the relevant context C r C_{r} italic_C start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT . Using C r C_{r} italic_C start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT we construct a second context using AMR which is finally translated in natural language, we denote this context as C a m r C_{amr} italic_C start_POSTSUBSCRIPT italic_a italic_m italic_r end_POSTSUBSCRIPT .",
                    "In the Answer generation stage, an LLM reads the context C C italic_C and the query Q Q italic_Q to generate the final answer A A italic_A following a specific prompting strategy.",
                    {
                        "Context C r C_{r} italic_C start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT construction: IR": [
                            "To construct the relevant context C r C_{r} italic_C start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , we implement an information retrieval setup that combines two complementary strategies: dense sentence embeddings and Doc2Query-based document expansion. Each sentence in the transcript is not only encoded as a dense vector but also represented by a set of synthetic queries generated using the Doc2Query model (Nogueira et al., 2019 ) . We index both the sentence embeddings and the synthetic queries using FAISS (Douze et al., 2025 ) for efficient similarity search.",
                            "At retrieval time, given a question Q Q italic_Q , we retrieve (1) the most similar sentences based on the dense embedding similarity to Q Q italic_Q , and (2) the sentences whose generated queries are most similar to Q Q italic_Q in the Doc2Query index. The union of these results forms the initial set of relevant sentences. To improve coherence, we expand each selected sentence with its immediate context: one preceding and one following sentence from the transcript. We observed that, in some cases, the answer to the question was actually contained in the sentence closest to the most similar one. The final IR-based context C r C_{r} italic_C start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT consists of this expanded set of relevant passages, ordered by their original position in the transcript to preserve the sequential structure of the transcript."
                        ]
                    },
                    {
                        "Context C a m r C_{amr} italic_C start_POSTSUBSCRIPT italic_a italic_m italic_r end_POSTSUBSCRIPT construction: AMR for QA": [
                            "To enrich the retrieved context and improve answer generation, we incorporate AMRs, derived from the selected sentences in C r C_{r} italic_C start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT . Following the work of Zhang et al. ( 2025 ) as described in 2.3 , we convert an AMR graph into its natural language descriptions. Specifically, we apply this conversion to the context retrieved in the information retrieval step ( C r C_{r} italic_C start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ). Since the code has yet to be provided by Zhang et al. ( 2025 ) , 2 2 2 At the moment of writing, June 2025. we use our own implementation for this process. We refer the readers to the original article for detailed description and examples.",
                            "Converting AMR into its natural language descriptions consists of 3 steps: 1) Extracting a set of triples from a given AMR graph 2) Translate each triple into a sentence 3) Polish each sentence using LLM. For the first step, we used library penman (Goodman, 2020 ) . The second step requires pre-defined rules to translate each semantic role into a sentence, e.g., (John, :ARG0, rob-01) → \\rightarrow → ‘John is the doer of rob-01 (to engage in or commit robbery)’. This may produce an unnatural text that needs to be polished for natural effect. This step is done in the third step using LLM. Following the original work, we provide some examples for the prompt to polish the text. As a result, for example, ‘John is the doer of rob-01 (to engage in or commit robbery)’ is polished as ‘John robs something.’",
                            "The natural descriptions of AMR graphs form the C a m r C_{amr} italic_C start_POSTSUBSCRIPT italic_a italic_m italic_r end_POSTSUBSCRIPT context, which can be provided either alone or alongside with the original sentences depending on our system variant. This is further detailed in the next section."
                        ]
                    },
                    {
                        "Answer A A italic_A generation: Prompting LLM": [
                            "We use a large language model (LLM) as the backbone of the answer generation component. Given the constructed context C C italic_C , which may include the IR-based context C r C_{r} italic_C start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , the AMR-derived context C a m r C_{amr} italic_C start_POSTSUBSCRIPT italic_a italic_m italic_r end_POSTSUBSCRIPT , or both, and the question Q Q italic_Q , the LLM generates the final answer A A italic_A .",
                            "We experiment with three variants of the input context provided to the LLM: 1. IR-only: Using only the retrieved sentences based on sentence and Doc2Query representations ( C r C_{r} italic_C start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ), along with the question Q Q italic_Q . 2. IR+AMR: Using both the retrieved sentences ( C r C_{r} italic_C start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) and their AMR-based natural language descriptions ( C a m r C_{amr} italic_C start_POSTSUBSCRIPT italic_a italic_m italic_r end_POSTSUBSCRIPT ), along with the question Q Q italic_Q . 3. AMR-only: Using only the AMR-based natural language descriptions of the retrieved sentences ( C a m r C_{amr} italic_C start_POSTSUBSCRIPT italic_a italic_m italic_r end_POSTSUBSCRIPT ), along with the question Q Q italic_Q ."
                        ]
                    }
                ]
            }
        ],
        "4 Experiments": [
            {
                "4.1 Models": [
                    "We implement our RAG pipeline using the following components :",
                    {
                        "Context Construction: IR": [
                            "For the sentence-level representation in the IR module, we use the all-MiniLM-L6-v2 sentence embedding model 3 3 3 https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 . For Doc2Query, we use the DocTTTTTquery model trained on the MS-MARCO dataset 4 4 4 https://huggingface.co/castorini/doc2query-t5-base-msmarco ."
                        ]
                    },
                    {
                        "Context Construction: AMR": [
                            "For AMR-to-text conversion, we use the meta-llama/Llama-3.1-8B-Instruct model 5 5 5 https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct , prompted with the following instruction: ⬇ You are an AI language assistant . Your job is to improve and rewrite a list of sub - sentences ( input_sub_sentences ) so they flow naturally and resemble fluent , natural language . Use the input_original_sentence as context to guide your rewrites . Follow the format and style shown in the examples . Only output the final polished sentences . Do not include any explanations ."
                        ]
                    },
                    {
                        "Answer Generation": [
                            "We use the same meta-llama/Llama-3.1-8B-Instruct model to generate answers, with the following instruction: ⬇ You are an AI assistant that answers questions using retrieved meeting information . Provide only the most relevant 1-2 sentence answer extracted directly from the content . Follow these rules : Be extremely concise - just the core fact , Use exact terms / phrases from the retrieved content , and never add analysis , disclaimers or \" Based on ...\"."
                        ]
                    },
                    {
                        "From English to Czech": [
                            "We control the output language by adding the instruction ‘‘Answer in Czech.’’ to the prompt when needed."
                        ]
                    }
                ]
            },
            {
                "4.2 Evaluation": [
                    "For the evaluation of Task B, predictions are evaluated by the LLM-as-a-judge metric, which uses large language models as automated judges to assess the quality of responses. As used in Thonet et al. ( 2025 ); Zheng et al. ( 2023 ); Chiang and Lee ( 2023 ) , these models will compare the system-generated answers with the human-crafted gold reference answer for each given query. In this experiment, Prometheus model was used as implemented by the authors of Kim et al. ( 2023 ) . Prometheus is a 13B open-source language model fine-tuned to serve as an evaluator capable of assessing long-form responses based on user-provided rubrics and reference answers. We follow the Prometheus scale of 0 to 5, where 0 is when an answer is not generated in the intended language, and 5 is when the response to evaluate is essentially equivalent to the reference answer. As the final evaluation of the task is in range 0 to 10, we rescale our scores accordingly."
                ]
            }
        ],
        "5 Results": [
            "In this section, we propose an evaluation of the model based on LLM-as-judges for the Czech dataset only, and both LLM-as-judges and human evaluation for the English dataset. We evaluate the significant difference between the different experiments using a t-test.",
            {
                "Czech results": [
                    "Table 1 shows the mean and standard deviation from the scores as provided by LLM-as-judges. Figure 3 displays violin plots of the score distribution. We decided to remove the 0 score, as it was also triggered in cases where the reference answer was in English (e.g., [ORGANIZATION1] ).",
                    "No significant difference was observed between the results obtained for each of our three proposed architectures.",
                    "Figure 3: Automatic evaluation using LLM-as-judges. Scores are between 0 and 10. Violin plot with mean distribution as blue line. *** for p ≤ \\leq ≤ 0.005"
                ]
            },
            {
                "English results": [
                    "We used different human annotators to manually evaluate the performance of each of our three configurations. Each annotator evaluated only a part of the dataset, and there was no cross-over between annotators. Table 2 shows the mean and standard deviation of the scores provided by both LLM and Human evaluators. Human scores are higher than automatic LLM-as-judges scores. We can observe from Figure 4 and Figure 5 that the evaluation is consistent between humans and LLMs. In both cases, GETALP@Automin and GETALP@AutoMin_amr obtain higher scores than GETALP@Automin_amr_only, and no significant difference is observed between GETALP@Automin and GETALP@Automin_amr. When comparing both automatic and manual scores for the two best configurations, as displayed in Figure 6 , we could not identify a model that outperforms the other.",
                    "Figure 4: Automatic evaluation using LLM-as-judges. Scores are between 2 and 10. Violin plot with distribution mean as blue line. *** for p ≤ \\leq ≤ 0.005",
                    "Figure 5: Human evaluation. Scores are between 2 and 10. Violin plot with distribution mean as blue line. *** for p ≤ \\leq ≤ 0.005",
                    "Ground Truth Ours Who is leaving the project? [PERSON10] [PERSON10] will be leaving. What is the application deadline for the PhD program? End of the year The PhD application deadline is the end of the year. Who had to pause the meeting for a few minutes? [PERSON2] PERSON2 had to pause the meeting for a few minutes. Table 3: Examples where LLM-as-judge gave a score of 2 while evaluators gave a score of either 8 or 10, for both GETALP@Automin and GETALP@Automin_amr models. The answer displayed comes from one of the two models.",
                    "Ground Truth Ours What is [PERSON4] focusing on now? Multi-source machine translation [PERSON4] is putting together and also working on the censorship component, which is one part of the pipeline after the [PROJECT4]. What is the current delay of the language id system? 3 seconds The current delay of the language ID system is not explicitly stated. How did [PERSON7] qualify the experience of the latest recording session assessment? It was qualified as \"disastrous\". Unfortunately, the provided information does not contain a direct quote from [PERSON7] regarding the experience of the latest recording session assessment. Table 4: Examples where both LLM-as-judge and evaluators gave a score 2, for both GETALP@Automin and GETALP@Automin_amr models. The answer displayed comes from one of the two models.",
                    "Figure 6: Human versus automatic evaluation. Scores are between 2 and 10. Violin plot with distribution mean as blue line. *** for p ≤ \\leq ≤ 0.005",
                    "Ground-truth answers provided by the dataset are not always complete sentences, but are often sentence fragments or short pieces of information, as shown in Table 4 and Table 3 . However, given the text generation capabilities of LLMs, we would expect a correct answer to be a full sentence conveying the correct information. Out of 130 questions, 46 received a human evaluation score of 8 or 10 for GETALP@Automin_amr, and 49 out of 130 for GETALP@Automin. The two systems obtained the same score for 91 of the 130 questions, while in 18 questions the AMR-based solution performed better than IR-only. Interestingly, half of these 18 questions correspond to WHO questions. Among the 45 WHO questions in total, AMR achieved the same or a better score in 39 of them."
                ]
            }
        ],
        "6 Conclusion": [
            "Our participation in the AutoMin 2025 Shared Task focused on developing RAG system for question answering over long meeting transcripts. To address the challenges of this task, we combined dense retrieval with Doc2Query-based document expansion and enriched the retrieved content using AMR. We explored three variants of our system: using only the retrieved passages, combining them with their AMR-based natural language descriptions, and using only the AMR descriptions.",
            "Our results suggest that AMR contexts can improve the quality of generated answers, particularly for questions involving entity resolution or semantic roles, such as identifying the responsible person for a task or determining who is experiencing an issue (e.g., \"Who is experiencing disk space issues?\"). Future work includes refining the AMR-to-text generation process, better integrating AMR into context construction, and selectively applying AMR in question types where structured semantic information offers the most benefit."
        ],
        "Limitations": [
            "Our approach relies on a large language model (Llama 3.1 8B) for both AMR-to-text generation and final answer generation. This significantly increases computational demands and limits the feasibility of our system in resource-constrained environments. To carry out our experiments, we required high-performance GPUs, including an NVIDIA RTX A6000 and NVIDIA H100. Furthermore, although we prompt the model to produce answers in Czech, many of the underlying components, such as sentence embeddings and the Doc2Query model, are primarily trained on English data. This can result in reduced answer quality in non-English outputs and potential inconsistencies in multilingual behavior."
        ],
        "Acknowledgments": [
            "This work is being carried out as part of the AugmentIA Chair and supported by the Grenoble INP Foundation, thanks to sponsorship from the Artelia Group. This chair also receives state funding managed by the French National Research Agency under France 2030, reference ANR-23-IACL-0006 (MIAI Cluster). In addition, this work is supported by the CREMA project (Coreference REsolution into MAchine translation) funded by the French National Research Agency (ANR), contract number ANR-21-CE23-0021-01."
        ]
    }
]