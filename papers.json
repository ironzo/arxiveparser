[
    {
        "id": "2508.01005v1",
        "title": "MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented\n  Generation",
        "authors": [
            "Yiqun Chen",
            "Erhan Zhang",
            "Lingyong Yan",
            "Shuaiqiang Wang",
            "Jizhou Huang",
            "Dawei Yin",
            "Jiaxin Mao"
        ],
        "url": "https://arxiv.org/abs/2508.01005v1",
        "Abstract": "In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has\nbecome pivotal in enhancing response accuracy and reducing hallucination\nissues. The architecture of RAG systems varies significantly, encompassing\nsingle-round RAG, iterative RAG, and reasoning RAG, each tailored to address\ndifferent types of queries. Due to the varying complexity of real-world\nqueries, a fixed RAG pipeline often struggles to balance performance and cost\nefficiency across different queries. To address this challenge, we propose an\nadaptive RAG framework called MAO-ARAG, which leverages multi-agent\norchestration. Our adaptive RAG is conceived as a multi-turn framework.\nSpecifically, we define multiple executor agents, representing typical RAG\nmodules such as query reformulation agents, document selection agent, and\ngeneration agents. A planner agent intelligently selects and integrates the\nappropriate agents from these executors into a suitable workflow tailored for\neach query, striving for high-quality answers while maintaining reasonable\ncosts. During each turn, the planner agent is trained using reinforcement\nlearning, guided by an outcome-based reward (F1 score) and a cost-based\npenalty, continuously improving answer quality while keeping costs within a\nreasonable range. Experiments conducted on multiple QA datasets demonstrate\nthat our approach, which dynamically plans workflows for each query, not only\nachieves high answer quality but also maintains both cost and latency within\nacceptable limits.The code of MAO-ARAG is on\nhttps://github.com/chenyiqun/Agentic-RAG.",
        "Main": "",
        "Tuples": [
            [
                "Summary",
                "In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has\nbecome pivotal in enhancing response accuracy and reducing hallucination\nissues. The architecture of RAG systems varies significantly, encompassing\nsingle-round RAG, iterative RAG, and reasoning RAG, each tailored to address\ndifferent types of queries. Due to the varying complexity of real-world\nqueries, a fixed RAG pipeline often struggles to balance performance and cost\nefficiency across different queries. To address this challenge, we propose an\nadaptive RAG framework called MAO-ARAG, which leverages multi-agent\norchestration. Our adaptive RAG is conceived as a multi-turn framework.\nSpecifically, we define multiple executor agents, representing typical RAG\nmodules such as query reformulation agents, document selection agent, and\ngeneration agents. A planner agent intelligently selects and integrates the\nappropriate agents from these executors into a suitable workflow tailored for\neach query, striving for high-quality answers while maintaining reasonable\ncosts. During each turn, the planner agent is trained using reinforcement\nlearning, guided by an outcome-based reward (F1 score) and a cost-based\npenalty, continuously improving answer quality while keeping costs within a\nreasonable range. Experiments conducted on multiple QA datasets demonstrate\nthat our approach, which dynamically plans workflows for each query, not only\nachieves high answer quality but also maintains both cost and latency within\nacceptable limits.The code of MAO-ARAG is on\nhttps://github.com/chenyiqun/Agentic-RAG."
            ]
        ],
        "section_summaries": [
            [
                "Summary",
                "In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has\nbecome pivotal in enhancing response accuracy and reducing hallucination\nissues. The architecture of RAG systems varies significantly, encompassing\nsingle-round RAG, iterative RAG, and reasoning RAG, each tailored to address\ndifferent types of queries. Due to the varying complexity of real-world\nqueries, a fixed RAG pipeline often struggles to balance performance and cost\nefficiency across different queries. To address this challenge, we propose an\nadaptive RAG framework called MAO-ARAG, which leverages multi-agent\norchestration. Our adaptive RAG is conceived as a multi-turn framework.\nSpecifically, we define multiple executor agents, representing typical RAG\nmodules such as query reformulation agents, document selection agent, and\ngeneration agents. A planner agent intelligently selects and integrates the\nappropriate agents from these executors into a suitable workflow tailored for\neach query, striving for high-quality answers while maintaining reasonable\ncosts. During each turn, the planner agent is trained using reinforcement\nlearning, guided by an outcome-based reward (F1 score) and a cost-based\npenalty, continuously improving answer quality while keeping costs within a\nreasonable range. Experiments conducted on multiple QA datasets demonstrate\nthat our approach, which dynamically plans workflows for each query, not only\nachieves high answer quality but also maintains both cost and latency within\nacceptable limits.The code of MAO-ARAG is on\nhttps://github.com/chenyiqun/Agentic-RAG.",
                "RAG systems include single-round, iterative, and reasoning RAG. A fixed pipeline struggles to balance performance and cost. MAO-ARAG adapts with multi-agent orchestration, using planner and executor agents to tailor workflows for each query, balancing quality and costs (F1 score) via reinforcement learning."
            ]
        ],
        "general_summary": "**Research Overview**\nThe research addresses the challenge of balancing performance and cost efficiency in Retrieval-Augmented Generation (RAG) systems for question-answering tasks.\n\n**Methodology & Approach**\nMAO-ARAG proposes an adaptive RAG framework that leverages multi-agent orchestration, defining multiple executor agents and a planner agent. The planner agent intelligently selects and integrates the appropriate agents into a suitable workflow tailored for each query using reinforcement learning.\n\n**Key Contributions & Findings**\nThe proposed approach achieves high answer quality while maintaining reasonable costs and latency on multiple QA datasets. Experiments demonstrate that MAO-ARAG dynamically plans workflows for each query, balancing performance and cost.\n\n**Technical Details**\nMAO-ARAG utilizes a multi-turn framework with planner and executor agents, trained using reinforcement learning with outcome-based rewards (F1 score) and cost-based penalties.\n\n**Limitations & Future Work**\nThe current limitations of MAO-ARAG include its reliance on specific QA datasets and the need for further evaluation on diverse real-world queries. Future work may involve exploring other reward functions or incorporating additional agents to enhance flexibility.\n\n**Practical Implications**\nMAO-ARAG has the potential to improve the performance and efficiency of RAG-based QA systems, enabling more accurate and cost-effective query answering in various applications."
    },
    {
        "id": "2509.07987v1",
        "title": "Automated Trading System for Straddle-Option Based on Deep Q-Learning",
        "authors": [
            "Yiran Wan",
            "Xinyu Ying",
            "Shengzhen Xu"
        ],
        "url": "https://arxiv.org/abs/2509.07987v1",
        "Abstract": "Straddle Option is a financial trading tool that explores volatility premiums\nin high-volatility markets without predicting price direction. Although deep\nreinforcement learning has emerged as a powerful approach to trading automation\nin financial markets, existing work mostly focused on predicting price trends\nand making trading decisions by combining multi-dimensional datasets like blogs\nand videos, which led to high computational costs and unstable performance in\nhigh-volatility markets. To tackle this challenge, we develop automated\nstraddle option trading based on reinforcement learning and attention\nmechanisms to handle unpredictability in high-volatility markets. Firstly, we\nleverage the attention mechanisms in Transformer-DDQN through both\nself-attention with time series data and channel attention with multi-cycle\ninformation. Secondly, a novel reward function considering excess earnings is\ndesigned to focus on long-term profits and neglect short-term losses over a\nstop line. Thirdly, we identify the resistance levels to provide reference\ninformation when great uncertainty in price movements occurs with intensified\nbattle between the buyers and sellers. Through extensive experiments on the\nChinese stock, Brent crude oil, and Bitcoin markets, our attention-based\nTransformer-DDQN model exhibits the lowest maximum drawdown across all markets,\nand outperforms other models by 92.5\\% in terms of the average return excluding\nthe crude oil market due to relatively low fluctuation.",
        "Main": "",
        "Tuples": [
            [
                "Summary",
                "Straddle Option is a financial trading tool that explores volatility premiums\nin high-volatility markets without predicting price direction. Although deep\nreinforcement learning has emerged as a powerful approach to trading automation\nin financial markets, existing work mostly focused on predicting price trends\nand making trading decisions by combining multi-dimensional datasets like blogs\nand videos, which led to high computational costs and unstable performance in\nhigh-volatility markets. To tackle this challenge, we develop automated\nstraddle option trading based on reinforcement learning and attention\nmechanisms to handle unpredictability in high-volatility markets. Firstly, we\nleverage the attention mechanisms in Transformer-DDQN through both\nself-attention with time series data and channel attention with multi-cycle\ninformation. Secondly, a novel reward function considering excess earnings is\ndesigned to focus on long-term profits and neglect short-term losses over a\nstop line. Thirdly, we identify the resistance levels to provide reference\ninformation when great uncertainty in price movements occurs with intensified\nbattle between the buyers and sellers. Through extensive experiments on the\nChinese stock, Brent crude oil, and Bitcoin markets, our attention-based\nTransformer-DDQN model exhibits the lowest maximum drawdown across all markets,\nand outperforms other models by 92.5\\% in terms of the average return excluding\nthe crude oil market due to relatively low fluctuation."
            ]
        ],
        "section_summaries": [
            [
                "Summary",
                "Straddle Option is a financial trading tool that explores volatility premiums\nin high-volatility markets without predicting price direction. Although deep\nreinforcement learning has emerged as a powerful approach to trading automation\nin financial markets, existing work mostly focused on predicting price trends\nand making trading decisions by combining multi-dimensional datasets like blogs\nand videos, which led to high computational costs and unstable performance in\nhigh-volatility markets. To tackle this challenge, we develop automated\nstraddle option trading based on reinforcement learning and attention\nmechanisms to handle unpredictability in high-volatility markets. Firstly, we\nleverage the attention mechanisms in Transformer-DDQN through both\nself-attention with time series data and channel attention with multi-cycle\ninformation. Secondly, a novel reward function considering excess earnings is\ndesigned to focus on long-term profits and neglect short-term losses over a\nstop line. Thirdly, we identify the resistance levels to provide reference\ninformation when great uncertainty in price movements occurs with intensified\nbattle between the buyers and sellers. Through extensive experiments on the\nChinese stock, Brent crude oil, and Bitcoin markets, our attention-based\nTransformer-DDQN model exhibits the lowest maximum drawdown across all markets,\nand outperforms other models by 92.5\\% in terms of the average return excluding\nthe crude oil market due to relatively low fluctuation.",
                "Straddle Option uses reinforcement learning and attention mechanisms to automate trading without predicting price direction.\nExisting work has high computational costs and unstable performance in high-volatility markets.\nA novel reward function focuses on long-term profits, neglecting short-term losses over a stop line.\nThe model identifies resistance levels for reference when uncertainty occurs.\n\nNumbers: \n* 92.5%\n* 3 (markets: Chinese stock, Brent crude oil, Bitcoin)\n\nDates:\nNone mentioned"
            ]
        ],
        "general_summary": "**Research Overview**\nThis research addresses the challenge of automating trading in high-volatility markets without predicting price direction using reinforcement learning and attention mechanisms. The proposed system is motivated by the limitations of existing work that focuses on predicting price trends with high computational costs.\n\n**Methodology & Approach**\nThe proposed Automated Trading System for Straddle-Option uses a deep Q-learning approach, specifically Transformer-DDQN, which leverages attention mechanisms to handle unpredictability in high-volatility markets. The model combines self-attention and channel attention techniques to process time series data and multi-cycle information. A novel reward function is designed to focus on long-term profits by neglecting short-term losses over a stop line.\n\n**Key Contributions & Findings**\nThe proposed system exhibits the lowest maximum drawdown across all three markets (Chinese stock, Brent crude oil, and Bitcoin) and outperforms other models by 92.5% in terms of average return, excluding the crude oil market due to relatively low fluctuation. The model identifies resistance levels for reference when uncertainty occurs.\n\n**Technical Details**\nThe Transformer-DDQN model uses self-attention and channel attention techniques to process time series data and multi-cycle information. A novel reward function is designed to focus on long-term profits by neglecting short-term losses over a stop line.\n\n**Limitations & Future Work**\nThe current limitations of the system include its performance in low-volatility markets, where existing models may outperform it due to relatively lower fluctuation. Future work could explore adapting the attention mechanisms to handle such scenarios.\n\n**Practical Implications**\nThis research has significant practical implications for automating trading in high-volatility markets without predicting price direction. The proposed system can provide a more stable and profitable approach to straddle option trading, with potential applications in various financial markets."
    },
    {
        "id": "2510.07718v1",
        "title": "SUBQRAG: sub-question driven dynamic graph rag",
        "authors": [
            "Jiaoyang Li",
            "Junhao Ruan",
            "Shengwei Tang",
            "Saihan Chen",
            "Kaiyan Chang",
            "Yuan Ge",
            "Tong Xiao",
            "Jingbo Zhu"
        ],
        "url": "https://arxiv.org/abs/2510.07718v1",
        "Abstract": "Abstract Graph Retrieval-Augmented Generation (Graph RAG) is effective at building a knowledge graph (KG) to connect disparate facts across a wide document corpus.\nHowever, this broad-view approach often lacks the deep structured reasoning required for complex multi-hop question answering (QA), leading to incomplete evidence and error accumulation. To address these issues, we propose SubQRAG 1 1 1 https://github.com/ljy1228/SubQRAG , a sub-question driven framework that enhances reasoning depth. SubQRAG decomposes a complex question into an ordered chain of verifiable sub-questions. For each sub-question, it retrieves relevant triples from the graph. A key feature is its ability to adapt: if the original graph is insufficient, the system retrieves information from source documents, extracts new triples, and dynamically updates the graph in real time. Finally, all triples used in the reasoning process are aggregated into a “graph memory”, providing a structured and traceable evidence path for the final answer generation. Experiments on three multi-hop QA benchmarks show that SubQRAG consistently achieves significant improvements, particularly in Exact Match scores.",
        "Main": {
            "1 Introduction": "Large Language Models (LLMs) have demonstrated remarkable language understanding and generation capabilities on downstream tasks. However, when solving question answering (QA) tasks that require deeper reasoning and broader coverage, LLMs still face the constraints. Owing to the high cost of re-training, the internal knowledge of LLMs cannot be updated in real time and thus remains limited in coverage, which results in failures to incorporate the latest information and increases the risk of hallucinations in downstream QA tasks [ 1 ] . Retrieval-augmented generation (RAG) [ 2 ] addresses these challenges by retrieving knowledge from an external knowledge corpus. The standard RAG follows a single-step retrieval and generation framework. For simple QA tasks, this works well. However, this strategy is fragile for multi-hop QA because single-step retrieval can cause the retriever to narrowly focus on one dominant topic. Document-level knowledge often exhausts the limited retrieval window with evidence for a single hop, crowding out the distinct information needed to complete the full reasoning chain [ 3 ] . To advance beyond retrieving redundant document spans and to enable a more robust reasoning process, Graph RAG utilizes a knowledge graph (KG) structure. It compresses document-level knowledge into triples as ⟨ e n t i t y , r e l a t i o n , e n t i t y ⟩ \\langle entity,relation,entity\\rangle to describe relationships between entities [ 4 ] . This allows a guided traversal through the knowledge base, such as in GraphRAG [ 5 ] , facilitating the integration of multiple sources for complex QA. Although graph RAG has advanced, it still faces limitations. A primary issue is its continued reliance on single-step retrieval, where the entire complex question is used to perform a single-step search for entry points into the graph. This approach fails to break down the reasoning process, making it difficult to dynamically adjust the retrieval focus for each logical hop [ 6 ] . This problem is further exacerbated by the static nature of the pre-constructed KG. Its structure is fixed and often incomplete, with partial coverage and coarse granularity weakening the reasoning support [ 7 ] . This inflexibility leads to a third major challenge: error accumulation. Because the initial entry nodes are determined by a single, holistic question, any imprecision can derail the entire reasoning chain. Subsequent pathfinding or node expansion inherits and amplifies these initial errors [ 8 ] , causing irrelevant exploration of the graph and ultimate failure [ 9 ] . Motivated by these challenges, we propose SubQRAG, a framework designed to equip Graph RAG with two crucial capabilities: multi-step reasoning and real-time dynamic updating. To enable multi-step reasoning, SubQRAG first decomposes the original question into an ordered chain of sub-questions. It then addresses each sub-question sequentially, performing a focused retrieval of relevant triples at each step. To facilitate dynamic updating, if the pre-constructed graph lacks the necessary information, SubQRAG falls back to source documents. It retrieves relevant text, extracts new triples, and integrates them into the graph. Finally, all triples used in this adaptive reasoning process are aggregated into a coherent “graph memory”. This memory provides a structured and traceable evidence path for the generator, mitigating the error accumulation inherent in undirected graph traversal. Our contributions are summarized as follows: • Decompose sub-questions for dynamic retrieval . We present SubQRAG, which decomposes the original question into sub-questions and retrieves relevant triples, helping dynamically adjust the retrieval process and provide richer evidence. • Dynamic graph updating . When the pre-constructed graph cannot answer the sub-questions, sub-questions drive KG updates by retrieving source documents. • Graph memory as structured guidance . We integrate the triples supporting each sub-question into a subgraph and provide it to the generator. In this way, SubQRAG produces “graph memory” as reasoning trajectory, which reduces the limitations of the context window and improves QA performance.",
            "2 Method": "As shown in Figure 1 , our proposed SubQRAG, a sub-question-driven dynamic Graph RAG framework, operates in four main stages. The process begins with (I) Offline Indexing, where LLMs pre-construct a KG by extracting structured knowledge triples from raw text. Next, during (II) Question Decomposition and Rewriting, a complex original question is broken down into a series of simpler, verifiable sub-questions; to maintain coherence, subsequent sub-questions are then rewritten to incorporate answers from preceding ones. According to simpler sub-questions, SubQRAG is able to perform (III) Retrieval and Dynamic Graph Updating, where for each sub-question, the framework attempts to retrieve supporting triples from the KG. Once the existing graph lacks the necessary information, SubQRAG will dynamically update the original graph by extracting new triples from the source documents. Finally, in the (IV) Answer Generation stage, all supporting triples gathered throughout the intermediate reasoning process are assembled into a graph-based memory, which is then provided to an LLM to synthesize a comprehensive final answer to the original question.",
            "4 Conclusion": "In conclusion, we propose SubQRAG, a sub-question driven graph RAG framework which consists of four stages. Based on pre-construct KG, it first decomposes complex questions into verifiable sub-questions, then dynamically updates the KG when necessary for fine-grained retrieval and finally provides structured “graph memory” to the generator. Through leveraging multi-step reasoning and real-time graph updating, SubQRAG addresses incompleteness and error accumulation in the static graph RAG. Experiments on multi-hop QA benchmarks show that SubQRAG consistently surpasses existing graph RAG approaches, demonstrating its effectiveness in multi-hop QA tasks and achieving balance between the reasoning depth and knowledge base breadth."
        },
        "Tuples": [
            [
                "1 Introduction",
                "Large Language Models (LLMs) have demonstrated remarkable language understanding and generation capabilities on downstream tasks. However, when solving question answering (QA) tasks that require deeper reasoning and broader coverage, LLMs still face the constraints. Owing to the high cost of re-training, the internal knowledge of LLMs cannot be updated in real time and thus remains limited in coverage, which results in failures to incorporate the latest information and increases the risk of hallucinations in downstream QA tasks [ 1 ] . Retrieval-augmented generation (RAG) [ 2 ] addresses these challenges by retrieving knowledge from an external knowledge corpus. The standard RAG follows a single-step retrieval and generation framework. For simple QA tasks, this works well. However, this strategy is fragile for multi-hop QA because single-step retrieval can cause the retriever to narrowly focus on one dominant topic. Document-level knowledge often exhausts the limited retrieval window with evidence for a single hop, crowding out the distinct information needed to complete the full reasoning chain [ 3 ] . To advance beyond retrieving redundant document spans and to enable a more robust reasoning process, Graph RAG utilizes a knowledge graph (KG) structure. It compresses document-level knowledge into triples as ⟨ e n t i t y , r e l a t i o n , e n t i t y ⟩ \\langle entity,relation,entity\\rangle to describe relationships between entities [ 4 ] . This allows a guided traversal through the knowledge base, such as in GraphRAG [ 5 ] , facilitating the integration of multiple sources for complex QA. Although graph RAG has advanced, it still faces limitations. A primary issue is its continued reliance on single-step retrieval, where the entire complex question is used to perform a single-step search for entry points into the graph. This approach fails to break down the reasoning process, making it difficult to dynamically adjust the retrieval focus for each logical hop [ 6 ] . This problem is further exacerbated by the static nature of the pre-constructed KG. Its structure is fixed and often incomplete, with partial coverage and coarse granularity weakening the reasoning support [ 7 ] . This inflexibility leads to a third major challenge: error accumulation. Because the initial entry nodes are determined by a single, holistic question, any imprecision can derail the entire reasoning chain. Subsequent pathfinding or node expansion inherits and amplifies these initial errors [ 8 ] , causing irrelevant exploration of the graph and ultimate failure [ 9 ] . Motivated by these challenges, we propose SubQRAG, a framework designed to equip Graph RAG with two crucial capabilities: multi-step reasoning and real-time dynamic updating. To enable multi-step reasoning, SubQRAG first decomposes the original question into an ordered chain of sub-questions. It then addresses each sub-question sequentially, performing a focused retrieval of relevant triples at each step. To facilitate dynamic updating, if the pre-constructed graph lacks the necessary information, SubQRAG falls back to source documents. It retrieves relevant text, extracts new triples, and integrates them into the graph. Finally, all triples used in this adaptive reasoning process are aggregated into a coherent “graph memory”. This memory provides a structured and traceable evidence path for the generator, mitigating the error accumulation inherent in undirected graph traversal. Our contributions are summarized as follows: • Decompose sub-questions for dynamic retrieval . We present SubQRAG, which decomposes the original question into sub-questions and retrieves relevant triples, helping dynamically adjust the retrieval process and provide richer evidence. • Dynamic graph updating . When the pre-constructed graph cannot answer the sub-questions, sub-questions drive KG updates by retrieving source documents. • Graph memory as structured guidance . We integrate the triples supporting each sub-question into a subgraph and provide it to the generator. In this way, SubQRAG produces “graph memory” as reasoning trajectory, which reduces the limitations of the context window and improves QA performance."
            ],
            [
                "2 Method",
                "As shown in Figure 1 , our proposed SubQRAG, a sub-question-driven dynamic Graph RAG framework, operates in four main stages. The process begins with (I) Offline Indexing, where LLMs pre-construct a KG by extracting structured knowledge triples from raw text. Next, during (II) Question Decomposition and Rewriting, a complex original question is broken down into a series of simpler, verifiable sub-questions; to maintain coherence, subsequent sub-questions are then rewritten to incorporate answers from preceding ones. According to simpler sub-questions, SubQRAG is able to perform (III) Retrieval and Dynamic Graph Updating, where for each sub-question, the framework attempts to retrieve supporting triples from the KG. Once the existing graph lacks the necessary information, SubQRAG will dynamically update the original graph by extracting new triples from the source documents. Finally, in the (IV) Answer Generation stage, all supporting triples gathered throughout the intermediate reasoning process are assembled into a graph-based memory, which is then provided to an LLM to synthesize a comprehensive final answer to the original question."
            ],
            [
                "4 Conclusion",
                "In conclusion, we propose SubQRAG, a sub-question driven graph RAG framework which consists of four stages. Based on pre-construct KG, it first decomposes complex questions into verifiable sub-questions, then dynamically updates the KG when necessary for fine-grained retrieval and finally provides structured “graph memory” to the generator. Through leveraging multi-step reasoning and real-time graph updating, SubQRAG addresses incompleteness and error accumulation in the static graph RAG. Experiments on multi-hop QA benchmarks show that SubQRAG consistently surpasses existing graph RAG approaches, demonstrating its effectiveness in multi-hop QA tasks and achieving balance between the reasoning depth and knowledge base breadth."
            ]
        ],
        "section_summaries": [
            [
                "1 Introduction",
                "Large Language Models (LLMs) have demonstrated remarkable language understanding and generation capabilities on downstream tasks. However, when solving question answering (QA) tasks that require deeper reasoning and broader coverage, LLMs still face the constraints. Owing to the high cost of re-training, the internal knowledge of LLMs cannot be updated in real time and thus remains limited in coverage, which results in failures to incorporate the latest information and increases the risk of hallucinations in downstream QA tasks [ 1 ] . Retrieval-augmented generation (RAG) [ 2 ] addresses these challenges by retrieving knowledge from an external knowledge corpus. The standard RAG follows a single-step retrieval and generation framework. For simple QA tasks, this works well. However, this strategy is fragile for multi-hop QA because single-step retrieval can cause the retriever to narrowly focus on one dominant topic. Document-level knowledge often exhausts the limited retrieval window with evidence for a single hop, crowding out the distinct information needed to complete the full reasoning chain [ 3 ] . To advance beyond retrieving redundant document spans and to enable a more robust reasoning process, Graph RAG utilizes a knowledge graph (KG) structure. It compresses document-level knowledge into triples as ⟨ e n t i t y , r e l a t i o n , e n t i t y ⟩ \\langle entity,relation,entity\\rangle to describe relationships between entities [ 4 ] . This allows a guided traversal through the knowledge base, such as in GraphRAG [ 5 ] , facilitating the integration of multiple sources for complex QA. Although graph RAG has advanced, it still faces limitations. A primary issue is its continued reliance on single-step retrieval, where the entire complex question is used to perform a single-step search for entry points into the graph. This approach fails to break down the reasoning process, making it difficult to dynamically adjust the retrieval focus for each logical hop [ 6 ] . This problem is further exacerbated by the static nature of the pre-constructed KG. Its structure is fixed and often incomplete, with partial coverage and coarse granularity weakening the reasoning support [ 7 ] . This inflexibility leads to a third major challenge: error accumulation. Because the initial entry nodes are determined by a single, holistic question, any imprecision can derail the entire reasoning chain. Subsequent pathfinding or node expansion inherits and amplifies these initial errors [ 8 ] , causing irrelevant exploration of the graph and ultimate failure [ 9 ] . Motivated by these challenges, we propose SubQRAG, a framework designed to equip Graph RAG with two crucial capabilities: multi-step reasoning and real-time dynamic updating. To enable multi-step reasoning, SubQRAG first decomposes the original question into an ordered chain of sub-questions. It then addresses each sub-question sequentially, performing a focused retrieval of relevant triples at each step. To facilitate dynamic updating, if the pre-constructed graph lacks the necessary information, SubQRAG falls back to source documents. It retrieves relevant text, extracts new triples, and integrates them into the graph. Finally, all triples used in this adaptive reasoning process are aggregated into a coherent “graph memory”. This memory provides a structured and traceable evidence path for the generator, mitigating the error accumulation inherent in undirected graph traversal. Our contributions are summarized as follows: • Decompose sub-questions for dynamic retrieval . We present SubQRAG, which decomposes the original question into sub-questions and retrieves relevant triples, helping dynamically adjust the retrieval process and provide richer evidence. • Dynamic graph updating . When the pre-constructed graph cannot answer the sub-questions, sub-questions drive KG updates by retrieving source documents. • Graph memory as structured guidance . We integrate the triples supporting each sub-question into a subgraph and provide it to the generator. In this way, SubQRAG produces “graph memory” as reasoning trajectory, which reduces the limitations of the context window and improves QA performance.",
                "Large Language Models (LLMs) face constraints on question answering tasks due to limited internal knowledge coverage. \nRetrieval-augmented generation (RAG) addresses this with single-step retrieval, but is fragile for multi-hop QA. \nGraph RAG utilizes a knowledge graph structure and has limitations in relying on single-step retrieval and static KG structure.\nThe SubQRAG framework proposes two capabilities: multi-step reasoning by decomposing questions into sub-questions, and dynamic updating of the graph through source document retrieval.\nSubQRAG aggregates triples used in this process into \"graph memory\", providing structured evidence for the generator."
            ],
            [
                "2 Method",
                "As shown in Figure 1 , our proposed SubQRAG, a sub-question-driven dynamic Graph RAG framework, operates in four main stages. The process begins with (I) Offline Indexing, where LLMs pre-construct a KG by extracting structured knowledge triples from raw text. Next, during (II) Question Decomposition and Rewriting, a complex original question is broken down into a series of simpler, verifiable sub-questions; to maintain coherence, subsequent sub-questions are then rewritten to incorporate answers from preceding ones. According to simpler sub-questions, SubQRAG is able to perform (III) Retrieval and Dynamic Graph Updating, where for each sub-question, the framework attempts to retrieve supporting triples from the KG. Once the existing graph lacks the necessary information, SubQRAG will dynamically update the original graph by extracting new triples from the source documents. Finally, in the (IV) Answer Generation stage, all supporting triples gathered throughout the intermediate reasoning process are assembled into a graph-based memory, which is then provided to an LLM to synthesize a comprehensive final answer to the original question.",
                "LLMs pre-construct a KG through Offline Indexing. SubQRAG decomposes complex questions into simpler sub-questions and rewrites them for coherence. It then performs Retrieval and Dynamic Graph Updating based on sub-questions, updating the graph if necessary. Finally, it assembles supporting triples to synthesize a comprehensive final answer.\n\nExtracted information:\n- 4 main stages\n- Figure 1 \n- LLMs: Large Language Models\n- KG: Knowledge Graph"
            ],
            [
                "4 Conclusion",
                "In conclusion, we propose SubQRAG, a sub-question driven graph RAG framework which consists of four stages. Based on pre-construct KG, it first decomposes complex questions into verifiable sub-questions, then dynamically updates the KG when necessary for fine-grained retrieval and finally provides structured “graph memory” to the generator. Through leveraging multi-step reasoning and real-time graph updating, SubQRAG addresses incompleteness and error accumulation in the static graph RAG. Experiments on multi-hop QA benchmarks show that SubQRAG consistently surpasses existing graph RAG approaches, demonstrating its effectiveness in multi-hop QA tasks and achieving balance between the reasoning depth and knowledge base breadth.",
                "SubQRAG framework consists of 4 stages: decomposing complex questions into sub-questions, dynamically updating KG for retrieval, providing graph memory to the generator. It surpasses existing graph RAG approaches on multi-hop QA benchmarks."
            ]
        ],
        "general_summary": "**Research Overview**\nThe paper addresses the limitations of Graph Retrieval-Augmented Generation (Graph RAG) in handling complex multi-hop question answering tasks by proposing a sub-question driven framework called SubQRAG.\n\n**Methodology & Approach**\nSubQRAG decomposes complex questions into ordered chains of verifiable sub-questions, retrieves relevant triples from the graph, and dynamically updates the graph if necessary. The system aggregates all triples used in reasoning into a \"graph memory\" for structured evidence. Technical components include offline indexing by Large Language Models (LLMs), retrieval, dynamic graph updating, and assembly of supporting triples.\n\n**Key Contributions & Findings**\nSubQRAG achieves significant improvements on multi-hop QA benchmarks, particularly in Exact Match scores. The framework's ability to adapt through dynamic graph updates enables it to surpass existing Graph RAG approaches.\n\n**Technical Details**\nThe system utilizes offline indexing by LLMs to pre-construct a Knowledge Graph (KG) structure. Specific techniques include sub-question decomposition, retrieval, and dynamic updating using source document retrieval.\n\n**Limitations & Future Work**\nCurrent limitations include the reliance on LLMs for KG construction and potential biases in sub-question decomposition. Future work could focus on improving these aspects or exploring alternative methods for KG construction.\n\n**Practical Implications**\nSubQRAG's ability to dynamically update the graph through source document retrieval has significant implications for complex multi-hop question answering tasks, enabling more accurate and structured reasoning."
    },
    {
        "id": "2510.03521v1",
        "title": "Identifying Financial Risk Information Using RAG with a Contrastive\n  Insight",
        "authors": [
            "Ali Elahi"
        ],
        "url": "https://arxiv.org/abs/2510.03521v1",
        "Abstract": "Abstract In specialized domains, humans often compare new problems against similar examples, highlight nuances, and draw conclusions instead of analyzing information in isolation. When applying reasoning in specialized contexts with LLMs on top of a RAG, the pipeline can capture contextually relevant information, but it is not designed to retrieve comparable cases or related problems. While RAG is effective at extracting factual information, its outputs in specialized reasoning tasks often remain generic, reflecting broad facts rather than context-specific insights. In finance, it results in generic risks that are true for the majority of companies. To address this limitation, we propose a peer-aware comparative inference layer on top of RAG. Our contrastive approach outperforms baseline RAG in text generation metrics such as ROUGE and BERTScore in comparison with human-generated equity research and risk.",
        "Main": {
            "1 Introduction": "Retrieval-based systems access external documents to provide relevant information for downstream tasks lewis2021retrievalaugmentedgenerationknowledgeintensivenlp . Retrieval-Augmented Generation (RAG) combines a similarity-based retrieval with a language model inference layer to refine and synthesize retrieved content, providing a robust framework for summarization, question answering, and information extraction. Limitation arises from RAG’s performance in retrieving critical information in specialized domains such as finance, particularly when extracting company risk factors from lengthy filings. A contextually rich passage detected by cosine similarity can still be irrelevant or uninformative, while a less rich section can convey more important details. In other words, the salience of information does not necessarily align with the semantic similarity index. We propose a contrastive approach: retrieving a broader set of relevant information for both the target firm and comparable peers, then prompting LLMs to generate a contrastive analysis that highlights nuances and distinctions.",
            "2 Methodology": "We propose an additional stage on top of RAG to identify a company’s most significant and distinctive risks. First, the system extracts a broad set of potential risks from filings and earnings call transcripts. It then compares these with risk factors from peer companies in the same industry. By contrasting common versus unique or unusually emphasized risks, the system highlights those most critical in context. Figure 1 summarizes the proposed risk identification pipeline. In the first stage, text chunks most relevant to the risk query (Appendix A – Risk Query) are extracted using cosine similarity of text embeddings. Each chunk is then processed by an LLM with a prompt (Appendix A – Risk Information Extraction Prompt) to retrieve relevant risk information. A subsequent prompt (Appendix A – Risk Aggregation Prompt) aggregates information across chunks, removes duplicates, and categorizes content into distinct risk topics. Finally, a contrastive prompt (Appendix A – Contrastive Risk Identification Prompt) compares the aggregated risk information for the target company and its peers to identify risks that are most specific or significant for the target company. In the baseline approach, after the aggregation method, we prompt the LLM to generate the most important risks for the company.",
            "3 Experiments and Results": "In our analysis, we generated risks for S&P 500 companies using the latest 10-Ks, 2025 10-Qs, and earnings call transcripts, which are all publicly available data. Relevant text chunks from these documents were extracted as input for the LLMs, and the output is a ranked list of risks.",
            "4 Literature Review": "Traditionally, investment banks spent significant time extracting details from filings, earnings call transcripts, and corporate presentations before synthesizing them into reports. NLP methods dramatically accelerate this process, and various methodologies are proposed to identify risk and summarize financial information using NLP pei2024modeling ; zhou2024finrobotaiagentequity ; mahfouz2021framework . Beyond supporting investment decisions, the extracted and summarized information can also serve as input to downstream LLM pipelines, such as those designed to assess stock valuations given risk and other factors zhao2025alphaagentslargelanguagemodel ; 10825449 . Recent work has combined RAG and reasoning components for general or specialized tasks. C-RAG ranaldi2024eliciting introduces a contrastive framework in which retrieved documents are used to generate explanatory arguments, and teacher-model explanations serve as demonstrations for student models, blending retrieval with contrastive few-shot reasoning in general QA tasks. RAFT proposes a training scenario that strengthens in-domain RAG by teaching models to identify and cite relevant evidence while ignoring “distractor” documents, coupling retrieval with chain-of-thought reasoning to improve factual grounding in domains such as biomedicine zhang2024raft . RankRAG yu2024rankrag integrates retrieval ranking and answer generation within a single instruction-tuned LLM, showing that incorporating ranking signals improves both context selection and downstream reasoning, often outperforming expert rankers and strong generation baselines across multiple benchmarks. Finally, RAG+ explicitly incorporates application-aware reasoning by retrieving not only factual knowledge but also aligned application examples, enabling structured, goal-directed inference; this design yields consistent improvements across domains such as mathematics, law, and medicine wang2025rag .",
            "5 Limitations": "One limitation of our pipeline is its lack of temporal awareness. The methodology does not account for the timeline of ongoing company events, meaning emerging developments are not captured, even though such events may materially affect risk exposure. As a result, the system may overlook dynamic changes that are crucial for timely and accurate risk assessment. Another limitation lies in evaluation. Current assessments rely primarily on NLP-based text generation metrics, which measure surface-level similarity but fail to capture whether the extracted risks are truly aligned with a company’s actual exposures. A stronger evaluation framework would incorporate expert human judgment as well as reasoning-based methods that go beyond lexical overlap to assess factual accuracy and contextual relevance.",
            "Acknowledgments": "This research was conducted at Surlamer Investments. The work, including all findings and results presented in this paper, is the property of Surlamer Investments."
        },
        "Tuples": [
            [
                "1 Introduction",
                "Retrieval-based systems access external documents to provide relevant information for downstream tasks lewis2021retrievalaugmentedgenerationknowledgeintensivenlp . Retrieval-Augmented Generation (RAG) combines a similarity-based retrieval with a language model inference layer to refine and synthesize retrieved content, providing a robust framework for summarization, question answering, and information extraction. Limitation arises from RAG’s performance in retrieving critical information in specialized domains such as finance, particularly when extracting company risk factors from lengthy filings. A contextually rich passage detected by cosine similarity can still be irrelevant or uninformative, while a less rich section can convey more important details. In other words, the salience of information does not necessarily align with the semantic similarity index. We propose a contrastive approach: retrieving a broader set of relevant information for both the target firm and comparable peers, then prompting LLMs to generate a contrastive analysis that highlights nuances and distinctions."
            ],
            [
                "2 Methodology",
                "We propose an additional stage on top of RAG to identify a company’s most significant and distinctive risks. First, the system extracts a broad set of potential risks from filings and earnings call transcripts. It then compares these with risk factors from peer companies in the same industry. By contrasting common versus unique or unusually emphasized risks, the system highlights those most critical in context. Figure 1 summarizes the proposed risk identification pipeline. In the first stage, text chunks most relevant to the risk query (Appendix A – Risk Query) are extracted using cosine similarity of text embeddings. Each chunk is then processed by an LLM with a prompt (Appendix A – Risk Information Extraction Prompt) to retrieve relevant risk information. A subsequent prompt (Appendix A – Risk Aggregation Prompt) aggregates information across chunks, removes duplicates, and categorizes content into distinct risk topics. Finally, a contrastive prompt (Appendix A – Contrastive Risk Identification Prompt) compares the aggregated risk information for the target company and its peers to identify risks that are most specific or significant for the target company. In the baseline approach, after the aggregation method, we prompt the LLM to generate the most important risks for the company."
            ],
            [
                "3 Experiments and Results",
                "In our analysis, we generated risks for S&P 500 companies using the latest 10-Ks, 2025 10-Qs, and earnings call transcripts, which are all publicly available data. Relevant text chunks from these documents were extracted as input for the LLMs, and the output is a ranked list of risks."
            ],
            [
                "4 Literature Review",
                "Traditionally, investment banks spent significant time extracting details from filings, earnings call transcripts, and corporate presentations before synthesizing them into reports. NLP methods dramatically accelerate this process, and various methodologies are proposed to identify risk and summarize financial information using NLP pei2024modeling ; zhou2024finrobotaiagentequity ; mahfouz2021framework . Beyond supporting investment decisions, the extracted and summarized information can also serve as input to downstream LLM pipelines, such as those designed to assess stock valuations given risk and other factors zhao2025alphaagentslargelanguagemodel ; 10825449 . Recent work has combined RAG and reasoning components for general or specialized tasks. C-RAG ranaldi2024eliciting introduces a contrastive framework in which retrieved documents are used to generate explanatory arguments, and teacher-model explanations serve as demonstrations for student models, blending retrieval with contrastive few-shot reasoning in general QA tasks. RAFT proposes a training scenario that strengthens in-domain RAG by teaching models to identify and cite relevant evidence while ignoring “distractor” documents, coupling retrieval with chain-of-thought reasoning to improve factual grounding in domains such as biomedicine zhang2024raft . RankRAG yu2024rankrag integrates retrieval ranking and answer generation within a single instruction-tuned LLM, showing that incorporating ranking signals improves both context selection and downstream reasoning, often outperforming expert rankers and strong generation baselines across multiple benchmarks. Finally, RAG+ explicitly incorporates application-aware reasoning by retrieving not only factual knowledge but also aligned application examples, enabling structured, goal-directed inference; this design yields consistent improvements across domains such as mathematics, law, and medicine wang2025rag ."
            ],
            [
                "5 Limitations",
                "One limitation of our pipeline is its lack of temporal awareness. The methodology does not account for the timeline of ongoing company events, meaning emerging developments are not captured, even though such events may materially affect risk exposure. As a result, the system may overlook dynamic changes that are crucial for timely and accurate risk assessment. Another limitation lies in evaluation. Current assessments rely primarily on NLP-based text generation metrics, which measure surface-level similarity but fail to capture whether the extracted risks are truly aligned with a company’s actual exposures. A stronger evaluation framework would incorporate expert human judgment as well as reasoning-based methods that go beyond lexical overlap to assess factual accuracy and contextual relevance."
            ],
            [
                "Acknowledgments",
                "This research was conducted at Surlamer Investments. The work, including all findings and results presented in this paper, is the property of Surlamer Investments."
            ]
        ],
        "section_summaries": [
            [
                "1 Introduction",
                "Retrieval-based systems access external documents to provide relevant information for downstream tasks lewis2021retrievalaugmentedgenerationknowledgeintensivenlp . Retrieval-Augmented Generation (RAG) combines a similarity-based retrieval with a language model inference layer to refine and synthesize retrieved content, providing a robust framework for summarization, question answering, and information extraction. Limitation arises from RAG’s performance in retrieving critical information in specialized domains such as finance, particularly when extracting company risk factors from lengthy filings. A contextually rich passage detected by cosine similarity can still be irrelevant or uninformative, while a less rich section can convey more important details. In other words, the salience of information does not necessarily align with the semantic similarity index. We propose a contrastive approach: retrieving a broader set of relevant information for both the target firm and comparable peers, then prompting LLMs to generate a contrastive analysis that highlights nuances and distinctions.",
                "Lewis proposes RAG to combine retrieval and language model inference. Limitations arise in specialized domains like finance, where salience does not align with semantic similarity. A contrastive approach is proposed: retrieving broad information for a target firm and peers, then prompting LLMs for a contrastive analysis. \n\nSummary of extracted numbers, dates, names, abbreviations:\n- lewis2021: publication year (2021)\n- RAG: Retrieval-Augmented Generation\n- LLMs: Large Language Models"
            ],
            [
                "2 Methodology",
                "We propose an additional stage on top of RAG to identify a company’s most significant and distinctive risks. First, the system extracts a broad set of potential risks from filings and earnings call transcripts. It then compares these with risk factors from peer companies in the same industry. By contrasting common versus unique or unusually emphasized risks, the system highlights those most critical in context. Figure 1 summarizes the proposed risk identification pipeline. In the first stage, text chunks most relevant to the risk query (Appendix A – Risk Query) are extracted using cosine similarity of text embeddings. Each chunk is then processed by an LLM with a prompt (Appendix A – Risk Information Extraction Prompt) to retrieve relevant risk information. A subsequent prompt (Appendix A – Risk Aggregation Prompt) aggregates information across chunks, removes duplicates, and categorizes content into distinct risk topics. Finally, a contrastive prompt (Appendix A – Contrastive Risk Identification Prompt) compares the aggregated risk information for the target company and its peers to identify risks that are most specific or significant for the target company. In the baseline approach, after the aggregation method, we prompt the LLM to generate the most important risks for the company.",
                "RAG is modified with an additional stage to identify a company's most distinctive risks by comparing potential risks from filings and peer companies. The pipeline involves 4 stages: \n1) Extracting relevant text chunks using cosine similarity, \n2) Processing each chunk with an LLM to retrieve risk information,\n3) Aggregating information and categorizing content into distinct risk topics, and\n4) Comparing aggregated risk information for target company vs peers.\nThe baseline approach prompts the LLM to generate top risks after aggregation."
            ],
            [
                "3 Experiments and Results",
                "In our analysis, we generated risks for S&P 500 companies using the latest 10-Ks, 2025 10-Qs, and earnings call transcripts, which are all publicly available data. Relevant text chunks from these documents were extracted as input for the LLMs, and the output is a ranked list of risks.",
                "S&P 500 companies' risks generated using 2025 10-Ks, 10-Qs, and earnings call transcripts."
            ],
            [
                "4 Literature Review",
                "Traditionally, investment banks spent significant time extracting details from filings, earnings call transcripts, and corporate presentations before synthesizing them into reports. NLP methods dramatically accelerate this process, and various methodologies are proposed to identify risk and summarize financial information using NLP pei2024modeling ; zhou2024finrobotaiagentequity ; mahfouz2021framework . Beyond supporting investment decisions, the extracted and summarized information can also serve as input to downstream LLM pipelines, such as those designed to assess stock valuations given risk and other factors zhao2025alphaagentslargelanguagemodel ; 10825449 . Recent work has combined RAG and reasoning components for general or specialized tasks. C-RAG ranaldi2024eliciting introduces a contrastive framework in which retrieved documents are used to generate explanatory arguments, and teacher-model explanations serve as demonstrations for student models, blending retrieval with contrastive few-shot reasoning in general QA tasks. RAFT proposes a training scenario that strengthens in-domain RAG by teaching models to identify and cite relevant evidence while ignoring “distractor” documents, coupling retrieval with chain-of-thought reasoning to improve factual grounding in domains such as biomedicine zhang2024raft . RankRAG yu2024rankrag integrates retrieval ranking and answer generation within a single instruction-tuned LLM, showing that incorporating ranking signals improves both context selection and downstream reasoning, often outperforming expert rankers and strong generation baselines across multiple benchmarks. Finally, RAG+ explicitly incorporates application-aware reasoning by retrieving not only factual knowledge but also aligned application examples, enabling structured, goal-directed inference; this design yields consistent improvements across domains such as mathematics, law, and medicine wang2025rag .",
                "pei2024modeling, zhou2024finrobotaiagentequity, mahfouz2021framework, zhao2025alphaagentslargelanguagemodel, 10825449, ranaldi2024eliciting, C-RAG, RAFT, zhang2024raft, yu2024rankrag, RankRAG, RAG+, wang2025rag"
            ],
            [
                "5 Limitations",
                "One limitation of our pipeline is its lack of temporal awareness. The methodology does not account for the timeline of ongoing company events, meaning emerging developments are not captured, even though such events may materially affect risk exposure. As a result, the system may overlook dynamic changes that are crucial for timely and accurate risk assessment. Another limitation lies in evaluation. Current assessments rely primarily on NLP-based text generation metrics, which measure surface-level similarity but fail to capture whether the extracted risks are truly aligned with a company’s actual exposures. A stronger evaluation framework would incorporate expert human judgment as well as reasoning-based methods that go beyond lexical overlap to assess factual accuracy and contextual relevance.",
                "Lack of temporal awareness in pipeline methodology, overlooking timeline of ongoing events. \nNLP-based metrics for evaluation rely on surface-level similarity, failing to capture actual exposures."
            ],
            [
                "Acknowledgments",
                "This research was conducted at Surlamer Investments. The work, including all findings and results presented in this paper, is the property of Surlamer Investments.",
                "Surlamer Investments conducted the research."
            ]
        ],
        "general_summary": "**Research Overview**\n\nThis research addresses the limitation of Retrieval-Augmented Generation (RAG) in specialized domains like finance, where it generates generic risks that are true for most companies rather than context-specific insights. A contrastive approach is proposed to improve RAG's performance by comparing a target company with its peers.\n\n**Methodology & Approach**\n\nThe modified RAG pipeline involves four stages: extracting relevant text chunks using cosine similarity, processing each chunk with Large Language Models (LLMs) to retrieve risk information, aggregating information and categorizing content into distinct risk topics, and comparing aggregated risk information for the target company vs. its peers. The proposed contrastive approach outperforms baseline RAG in text generation metrics such as ROUGE and BERTScore.\n\n**Key Contributions & Findings**\n\nThe research proposes a peer-aware comparative inference layer on top of RAG, which outperforms baseline RAG in comparison with human-generated equity research and risk. This novel contribution can improve the accuracy of financial risk information retrieval.\n\n**Technical Details**\n\nThe pipeline uses cosine similarity for text chunk extraction, LLMs for retrieving risk information, and aggregation/categorization techniques to compare risk information between target company and peers. The specific algorithms used are not mentioned in detail.\n\n**Limitations & Future Work**\n\nThe current limitations include lack of temporal awareness in the pipeline methodology and NLP-based metrics that rely on surface-level similarity rather than actual exposures. Future research should address these issues and explore other techniques for improving RAG's performance in specialized domains.\n\n**Practical Implications**\n\nThis research can be applied to improve financial risk information retrieval by using a contrastive approach with peer-aware comparative inference layer on top of RAG. The proposed method can have significant impact on the field of finance by providing more accurate and context-specific insights for investors and analysts."
    },
    {
        "id": "2510.02657v2",
        "title": "Less LLM, More Documents: Searching for Improved RAG",
        "authors": [
            "Jingjie Ning",
            "Yibo Kong",
            "Yunfan Long",
            "Jamie Callan"
        ],
        "url": "https://arxiv.org/abs/2510.02657v2",
        "Abstract": "Abstract Retrieval-Augmented Generation (RAG) couples document retrieval with large language models (LLMs). While scaling generators improves accuracy, it also raises cost and limits deployability. We explore an orthogonal axis: enlarging the retriever’s corpus to reduce reliance on large LLMs. Experimental results show that corpus scaling consistently strengthens RAG and can often serve as a substitute for increasing model size, though with diminishing returns at larger scales. Small- and mid-sized generators paired with larger corpora often rival much larger models with smaller corpora; mid-sized models tend to gain the most, while tiny and large models benefit less. Our analysis shows that improvements arise primarily from increased coverage of answer-bearing passages, while utilization efficiency remains largely unchanged. These findings establish a principled corpus–generator trade-off: investing in larger corpora offers an effective path to stronger RAG, often comparable to enlarging the LLM itself.",
        "Main": {
            "1 Introduction": "Retrieval-Augmented Generation (RAG) [ 6 , 17 ] combines document retrieval with large language models (LLMs), and has become a popular paradigm for open-domain question answering (QA) [ 8 , 28 , 30 ] . Most prior work has focused on scaling up the generator, which indeed improves accuracy but requires very large and expensive LLMs [ 7 , 19 , 21 ] . In contrast, the retriever controls the external evidence supplied to the generator, thereby directly influencing factuality and mitigating hallucinations [ 17 , 25 ] . However, the relationship between retriever capacity and generator size remains underexplored. In particular, it is not well understood whether enlarging the retrieval corpus can reduce reliance on larger generators, which is an important question for practical deployment, where smaller LLMs are easier to serve [ 31 , 34 , 37 ] . To address this gap, we conduct a systematic study of the trade-off between corpus scale and generator size by combining randomly-shardable retrieval over ClueWeb22 [ 20 ] with open-source Qwen3 models [ 35 ] of different scales. Our experiments on multiple QA benchmarks [ 1 , 13 , 16 ] reveal that enlarging the retrieval corpus not only improves the performance of smaller LLMs, but also enables them to rival or even surpass larger counterparts. For example, a 1.7B-parameter model with a 4 × 4\\times larger corpus outperforms a 4B model, and a 4B model with only a 2 × 2\\times larger corpus consistently outperforms an 8B model. These findings highlight a practical trade-off: scaling the retrieval corpus can partially substitute for scaling the generator. This insight suggests an efficient and deployable RAG design, where expanding the corpus offers a promising alternative to enlarging the LLM itself.",
            "2 Related Work": "A substantial body of research has focused on enhancing the intrinsic capabilities of LLMs. Instruction tuning [ 32 , 36 ] and prompt engineering [ 9 , 22 ] improve alignment with user queries, while scaling model size generally yields higher accuracy across diverse tasks [ 3 , 14 ] . However, ever-larger LLMs (e.g., PaLM with 540B parameters [ 4 ] ) incur prohibitive computational costs, which limits their practicality in many settings. In parallel, retrieval-augmented models have emerged as an alternative path to scaling. Retrieval-augmented language models (RALMs) such as RETRO [ 2 ] and Atlas [ 12 ] demonstrate that enlarging inference-time datastores consistently improves performance: relatively small generators paired with massive retrieval memory can outperform much larger LM-only baselines. Shao et al. [ 24 ] further confirmed this monotonic trend. Unlike modular RAG, which decouples retriever and generator, RALMs integrate retrieval through pretraining-time vector memories, requiring retriever-generator co-training. Modular RAG instead relies on external corpora that can be scaled independently of the generator. This line of work has progressed from Dense Passage Retrieval (DPR) [ 15 ] to efficiency-oriented methods such as ANCE [ 33 ] and Contriever [ 11 ] , which make retrieval over very large corpora feasible. These advances enabled a shift from early Wikipedia-only setups to broader and more diverse corpora such as LoTTE [ 23 ] and BEIR [ 27 ] . However, while the community has implicitly moved toward increasingly larger corpora, the direct impact of corpus growth itself has not yet been systematically and comprehensively examined. More recently, studies have begun to explore broader factors influencing RAG, including model size, corpus scale, and context size [ 18 , 29 ] . However, these analyses remain fragmented and primarily descriptive, typically isolating single variables. Importantly, they do not provide a principled understanding of how corpus size and LLM size interact, leaving the corpus-generator trade-off essentially unexplored. In particular, prior studies have not jointly examined retrieval corpus expansion and LLM size, leaving open the fundamental question of how corpus-generator trade-offs shape overall system performance.",
            "3 Methodology": "To address this gap, we present a systematic framework for analyzing corpus–generator trade-offs in RAG. Our approach evaluates whether, and under what conditions, scaling the retrieval corpus can compensate for smaller LLMs. We organize the methodology around two complementary dimensions. First, we study corpus scaling as compensation : does enlarging the retriever corpus enable smaller generators to rival or surpass larger models by leveraging broader retrieval evidence? Second, we investigate differential effects across LLMs : how do models of different sizes benefit from corpus expansion, and are there consistent patterns in how scaling interacts with model capacity? These two dimensions form the backbone of our experimental design and subsequent analysis.",
            "4 Experiment": "Building on the methodology outlined in Section 3 , we conducted a series of controlled experiments across different corpus scales to systematically evaluate our research questions.",
            "6 Conclusion": "In this paper, we asked whether scaling the retrieval corpus can often substitute for scaling the generator in RAG, and how corpus size interacts with model size under a fixed evidence budget. Using controlled evaluations on NQ, TriviaQA, and WebQ with standardized prompting and context formatting, we characterize the corpus–generator trade-off while holding the presented evidence constant. Our results show that corpus scaling can often offset model downsizing. Across datasets, enlarging the corpus is a reliable lever: smaller or mid-sized generators paired with larger corpora frequently surpass larger models under the same evidence budget. In several settings, moving up corpus tiers closed the gap of one to two model-size tiers, demonstrating that “more documents” can often substitute for “more parameters” when inference resources are constrained. A consistent mechanism explains these gains: performance improvements are driven by relevant-document coverage, not by utilization efficiency . As the corpus grows, the likelihood that retrieved passages contain the gold answer increases consistently, whereas the model’s context-utilization ratio remains roughly stable across shard counts and model sizes. Thus, scaling primarily works by raising the hit rate of answer-bearing evidence rather than by altering how effectively models exploit the context. At the same time, performance gains from corpus growth saturate after about a 5 5 – 6 × 6\\times increase, showing clear diminishing returns. Practically, when resources constrain generator size, it is often better to expand the corpus . Pairing mid-sized generators with larger corpora effectively converts coverage into end-task gains, whereas very large models offer little additional benefit, and very small ones require steep corpus expansions. Although we mainly focus on the Qwen3 family due to the lack of other open-source LLM series with homogeneous, wide-ranging variants, we hope to extend the analysis to additional model families once they become available. Notably, mid-sized models sometimes exploit retrieved context more efficiently than the largest models, consistent with our utilization analysis. Tracking gold-answer coverage and the Utilization Ratio provides practical diagnostics to guide budgeting between retriever and generator. In short: Less LLM, More Documents ."
        },
        "Tuples": [
            [
                "1 Introduction",
                "Retrieval-Augmented Generation (RAG) [ 6 , 17 ] combines document retrieval with large language models (LLMs), and has become a popular paradigm for open-domain question answering (QA) [ 8 , 28 , 30 ] . Most prior work has focused on scaling up the generator, which indeed improves accuracy but requires very large and expensive LLMs [ 7 , 19 , 21 ] . In contrast, the retriever controls the external evidence supplied to the generator, thereby directly influencing factuality and mitigating hallucinations [ 17 , 25 ] . However, the relationship between retriever capacity and generator size remains underexplored. In particular, it is not well understood whether enlarging the retrieval corpus can reduce reliance on larger generators, which is an important question for practical deployment, where smaller LLMs are easier to serve [ 31 , 34 , 37 ] . To address this gap, we conduct a systematic study of the trade-off between corpus scale and generator size by combining randomly-shardable retrieval over ClueWeb22 [ 20 ] with open-source Qwen3 models [ 35 ] of different scales. Our experiments on multiple QA benchmarks [ 1 , 13 , 16 ] reveal that enlarging the retrieval corpus not only improves the performance of smaller LLMs, but also enables them to rival or even surpass larger counterparts. For example, a 1.7B-parameter model with a 4 × 4\\times larger corpus outperforms a 4B model, and a 4B model with only a 2 × 2\\times larger corpus consistently outperforms an 8B model. These findings highlight a practical trade-off: scaling the retrieval corpus can partially substitute for scaling the generator. This insight suggests an efficient and deployable RAG design, where expanding the corpus offers a promising alternative to enlarging the LLM itself."
            ],
            [
                "2 Related Work",
                "A substantial body of research has focused on enhancing the intrinsic capabilities of LLMs. Instruction tuning [ 32 , 36 ] and prompt engineering [ 9 , 22 ] improve alignment with user queries, while scaling model size generally yields higher accuracy across diverse tasks [ 3 , 14 ] . However, ever-larger LLMs (e.g., PaLM with 540B parameters [ 4 ] ) incur prohibitive computational costs, which limits their practicality in many settings. In parallel, retrieval-augmented models have emerged as an alternative path to scaling. Retrieval-augmented language models (RALMs) such as RETRO [ 2 ] and Atlas [ 12 ] demonstrate that enlarging inference-time datastores consistently improves performance: relatively small generators paired with massive retrieval memory can outperform much larger LM-only baselines. Shao et al. [ 24 ] further confirmed this monotonic trend. Unlike modular RAG, which decouples retriever and generator, RALMs integrate retrieval through pretraining-time vector memories, requiring retriever-generator co-training. Modular RAG instead relies on external corpora that can be scaled independently of the generator. This line of work has progressed from Dense Passage Retrieval (DPR) [ 15 ] to efficiency-oriented methods such as ANCE [ 33 ] and Contriever [ 11 ] , which make retrieval over very large corpora feasible. These advances enabled a shift from early Wikipedia-only setups to broader and more diverse corpora such as LoTTE [ 23 ] and BEIR [ 27 ] . However, while the community has implicitly moved toward increasingly larger corpora, the direct impact of corpus growth itself has not yet been systematically and comprehensively examined. More recently, studies have begun to explore broader factors influencing RAG, including model size, corpus scale, and context size [ 18 , 29 ] . However, these analyses remain fragmented and primarily descriptive, typically isolating single variables. Importantly, they do not provide a principled understanding of how corpus size and LLM size interact, leaving the corpus-generator trade-off essentially unexplored. In particular, prior studies have not jointly examined retrieval corpus expansion and LLM size, leaving open the fundamental question of how corpus-generator trade-offs shape overall system performance."
            ],
            [
                "3 Methodology",
                "To address this gap, we present a systematic framework for analyzing corpus–generator trade-offs in RAG. Our approach evaluates whether, and under what conditions, scaling the retrieval corpus can compensate for smaller LLMs. We organize the methodology around two complementary dimensions. First, we study corpus scaling as compensation : does enlarging the retriever corpus enable smaller generators to rival or surpass larger models by leveraging broader retrieval evidence? Second, we investigate differential effects across LLMs : how do models of different sizes benefit from corpus expansion, and are there consistent patterns in how scaling interacts with model capacity? These two dimensions form the backbone of our experimental design and subsequent analysis."
            ],
            [
                "4 Experiment",
                "Building on the methodology outlined in Section 3 , we conducted a series of controlled experiments across different corpus scales to systematically evaluate our research questions."
            ],
            [
                "6 Conclusion",
                "In this paper, we asked whether scaling the retrieval corpus can often substitute for scaling the generator in RAG, and how corpus size interacts with model size under a fixed evidence budget. Using controlled evaluations on NQ, TriviaQA, and WebQ with standardized prompting and context formatting, we characterize the corpus–generator trade-off while holding the presented evidence constant. Our results show that corpus scaling can often offset model downsizing. Across datasets, enlarging the corpus is a reliable lever: smaller or mid-sized generators paired with larger corpora frequently surpass larger models under the same evidence budget. In several settings, moving up corpus tiers closed the gap of one to two model-size tiers, demonstrating that “more documents” can often substitute for “more parameters” when inference resources are constrained. A consistent mechanism explains these gains: performance improvements are driven by relevant-document coverage, not by utilization efficiency . As the corpus grows, the likelihood that retrieved passages contain the gold answer increases consistently, whereas the model’s context-utilization ratio remains roughly stable across shard counts and model sizes. Thus, scaling primarily works by raising the hit rate of answer-bearing evidence rather than by altering how effectively models exploit the context. At the same time, performance gains from corpus growth saturate after about a 5 5 – 6 × 6\\times increase, showing clear diminishing returns. Practically, when resources constrain generator size, it is often better to expand the corpus . Pairing mid-sized generators with larger corpora effectively converts coverage into end-task gains, whereas very large models offer little additional benefit, and very small ones require steep corpus expansions. Although we mainly focus on the Qwen3 family due to the lack of other open-source LLM series with homogeneous, wide-ranging variants, we hope to extend the analysis to additional model families once they become available. Notably, mid-sized models sometimes exploit retrieved context more efficiently than the largest models, consistent with our utilization analysis. Tracking gold-answer coverage and the Utilization Ratio provides practical diagnostics to guide budgeting between retriever and generator. In short: Less LLM, More Documents ."
            ]
        ],
        "section_summaries": [
            [
                "1 Introduction",
                "Retrieval-Augmented Generation (RAG) [ 6 , 17 ] combines document retrieval with large language models (LLMs), and has become a popular paradigm for open-domain question answering (QA) [ 8 , 28 , 30 ] . Most prior work has focused on scaling up the generator, which indeed improves accuracy but requires very large and expensive LLMs [ 7 , 19 , 21 ] . In contrast, the retriever controls the external evidence supplied to the generator, thereby directly influencing factuality and mitigating hallucinations [ 17 , 25 ] . However, the relationship between retriever capacity and generator size remains underexplored. In particular, it is not well understood whether enlarging the retrieval corpus can reduce reliance on larger generators, which is an important question for practical deployment, where smaller LLMs are easier to serve [ 31 , 34 , 37 ] . To address this gap, we conduct a systematic study of the trade-off between corpus scale and generator size by combining randomly-shardable retrieval over ClueWeb22 [ 20 ] with open-source Qwen3 models [ 35 ] of different scales. Our experiments on multiple QA benchmarks [ 1 , 13 , 16 ] reveal that enlarging the retrieval corpus not only improves the performance of smaller LLMs, but also enables them to rival or even surpass larger counterparts. For example, a 1.7B-parameter model with a 4 × 4\\times larger corpus outperforms a 4B model, and a 4B model with only a 2 × 2\\times larger corpus consistently outperforms an 8B model. These findings highlight a practical trade-off: scaling the retrieval corpus can partially substitute for scaling the generator. This insight suggests an efficient and deployable RAG design, where expanding the corpus offers a promising alternative to enlarging the LLM itself.",
                "RAG combines document retrieval with large language models (LLMs) for open-domain question answering (QA). The retriever's capacity influences factuality and mitigates hallucinations. Enlarging the retrieval corpus can reduce reliance on larger generators, which is important for practical deployment. Experiments combining ClueWeb22 with Qwen3 models reveal that enlarging the corpus improves smaller LLMs' performance, making them rival or surpass larger counterparts.\n\n* Combined: RAG [6, 17], QA [8, 28, 30]\n* Generators sizes: 1.7B, 4B, 8B\n* Corpus scales: 2x, 4x\n* Datasets used: ClueWeb22, Qwen3 models\n* Benchmarks used: multiple QA benchmarks [1, 13, 16]"
            ],
            [
                "2 Related Work",
                "A substantial body of research has focused on enhancing the intrinsic capabilities of LLMs. Instruction tuning [ 32 , 36 ] and prompt engineering [ 9 , 22 ] improve alignment with user queries, while scaling model size generally yields higher accuracy across diverse tasks [ 3 , 14 ] . However, ever-larger LLMs (e.g., PaLM with 540B parameters [ 4 ] ) incur prohibitive computational costs, which limits their practicality in many settings. In parallel, retrieval-augmented models have emerged as an alternative path to scaling. Retrieval-augmented language models (RALMs) such as RETRO [ 2 ] and Atlas [ 12 ] demonstrate that enlarging inference-time datastores consistently improves performance: relatively small generators paired with massive retrieval memory can outperform much larger LM-only baselines. Shao et al. [ 24 ] further confirmed this monotonic trend. Unlike modular RAG, which decouples retriever and generator, RALMs integrate retrieval through pretraining-time vector memories, requiring retriever-generator co-training. Modular RAG instead relies on external corpora that can be scaled independently of the generator. This line of work has progressed from Dense Passage Retrieval (DPR) [ 15 ] to efficiency-oriented methods such as ANCE [ 33 ] and Contriever [ 11 ] , which make retrieval over very large corpora feasible. These advances enabled a shift from early Wikipedia-only setups to broader and more diverse corpora such as LoTTE [ 23 ] and BEIR [ 27 ] . However, while the community has implicitly moved toward increasingly larger corpora, the direct impact of corpus growth itself has not yet been systematically and comprehensively examined. More recently, studies have begun to explore broader factors influencing RAG, including model size, corpus scale, and context size [ 18 , 29 ] . However, these analyses remain fragmented and primarily descriptive, typically isolating single variables. Importantly, they do not provide a principled understanding of how corpus size and LLM size interact, leaving the corpus-generator trade-off essentially unexplored. In particular, prior studies have not jointly examined retrieval corpus expansion and LLM size, leaving open the fundamental question of how corpus-generator trade-offs shape overall system performance.",
                "Research has focused on enhancing Large Language Models (LLMs) with methods like instruction tuning and prompt engineering. However, larger models incur high computational costs, prompting an alternative path in retrieval-augmented models (RALMs). RALMs demonstrate improved performance by enlarging inference-time datastores and integrating retrieval through pretraining-vector memories.\n\nResearch has progressed from Dense Passage Retrieval to efficiency-oriented methods and expanded corpora such as LoTTE and BEIR. However, the impact of corpus growth on RAG remains unexamined. Studies have explored broader factors influencing RAG but lack a principled understanding of how corpus size and LLM size interact.\n\nStudies have not jointly examined retrieval corpus expansion and LLM size, leaving open the question of how corpus-generator trade-offs shape overall system performance.\n\nKey figures:\n- 32, 36: Instruction tuning references\n- 9, 22: Prompt engineering references\n- 3, 14: Scaling model size references\n- 4: PaLM model with 540B parameters\n- 2: RETRO RALM reference\n- 12: Atlas RALM reference\n- 24: Shao et al. study reference\n- 15: Dense Passage Retrieval (DPR) reference\n- 33: ANCE efficiency-oriented method reference\n- 11: Contriever efficiency-oriented method reference\n- 23: LoTTE corpus reference\n- 27: BEIR corpus reference\n- 18, 29: Model size and context size studies"
            ],
            [
                "3 Methodology",
                "To address this gap, we present a systematic framework for analyzing corpus–generator trade-offs in RAG. Our approach evaluates whether, and under what conditions, scaling the retrieval corpus can compensate for smaller LLMs. We organize the methodology around two complementary dimensions. First, we study corpus scaling as compensation : does enlarging the retriever corpus enable smaller generators to rival or surpass larger models by leveraging broader retrieval evidence? Second, we investigate differential effects across LLMs : how do models of different sizes benefit from corpus expansion, and are there consistent patterns in how scaling interacts with model capacity? These two dimensions form the backbone of our experimental design and subsequent analysis.",
                "Our approach evaluates whether scaling a retrieval corpus can compensate for smaller Large Language Models (LLMs). We study two complementary dimensions: corpus scaling as compensation, and differential effects across LLMs."
            ],
            [
                "4 Experiment",
                "Building on the methodology outlined in Section 3 , we conducted a series of controlled experiments across different corpus scales to systematically evaluate our research questions.",
                "Experiments were conducted, building on Section 3 methodology."
            ],
            [
                "6 Conclusion",
                "In this paper, we asked whether scaling the retrieval corpus can often substitute for scaling the generator in RAG, and how corpus size interacts with model size under a fixed evidence budget. Using controlled evaluations on NQ, TriviaQA, and WebQ with standardized prompting and context formatting, we characterize the corpus–generator trade-off while holding the presented evidence constant. Our results show that corpus scaling can often offset model downsizing. Across datasets, enlarging the corpus is a reliable lever: smaller or mid-sized generators paired with larger corpora frequently surpass larger models under the same evidence budget. In several settings, moving up corpus tiers closed the gap of one to two model-size tiers, demonstrating that “more documents” can often substitute for “more parameters” when inference resources are constrained. A consistent mechanism explains these gains: performance improvements are driven by relevant-document coverage, not by utilization efficiency . As the corpus grows, the likelihood that retrieved passages contain the gold answer increases consistently, whereas the model’s context-utilization ratio remains roughly stable across shard counts and model sizes. Thus, scaling primarily works by raising the hit rate of answer-bearing evidence rather than by altering how effectively models exploit the context. At the same time, performance gains from corpus growth saturate after about a 5 5 – 6 × 6\\times increase, showing clear diminishing returns. Practically, when resources constrain generator size, it is often better to expand the corpus . Pairing mid-sized generators with larger corpora effectively converts coverage into end-task gains, whereas very large models offer little additional benefit, and very small ones require steep corpus expansions. Although we mainly focus on the Qwen3 family due to the lack of other open-source LLM series with homogeneous, wide-ranging variants, we hope to extend the analysis to additional model families once they become available. Notably, mid-sized models sometimes exploit retrieved context more efficiently than the largest models, consistent with our utilization analysis. Tracking gold-answer coverage and the Utilization Ratio provides practical diagnostics to guide budgeting between retriever and generator. In short: Less LLM, More Documents .",
                "NQ, TriviaQA, WebQ datasets; 5.55-6x corpus increase; Qwen3 family; LLM series variants; Utilization Ratio; retriever-generator budgeting."
            ]
        ],
        "general_summary": "**Research Overview**\n\nThis research addresses the problem of improving Retrieval-Augmented Generation (RAG) models for open-domain question answering, focusing on reducing reliance on large language models (LLMs) by scaling up the retrieval corpus. This is important because it can make RAG more practical and deployable.\n\n**Methodology & Approach**\n\nThe proposed solution involves exploring the impact of enlarging the retrieval corpus on RAG performance, using ClueWeb22 dataset in combination with Qwen3 models. The study examines the effect of corpus scaling as compensation for smaller LLMs and investigates differential effects across various generator sizes. The authors employ multiple QA benchmarks to evaluate system performance.\n\n**Key Contributions & Findings**\n\nThe experimental results show that enlarging the retrieval corpus consistently strengthens RAG, often rivaling or surpassing larger models with smaller corpora. Mid-sized generators paired with larger corpora tend to gain the most, while tiny and large models benefit less. The analysis reveals that improvements arise primarily from increased coverage of answer-bearing passages.\n\n**Technical Details**\n\nThe study uses Qwen3 family models (1.7B, 4B, 8B parameters) combined with ClueWeb22 corpus, which is expanded up to 5.55-6x in size. The authors employ multiple QA benchmarks and evaluate system performance using utilization ratio metrics.\n\n**Limitations & Future Work**\n\nThe study acknowledges that the findings are limited by the specific datasets and models used, suggesting further investigation of RAG with larger corpora on diverse tasks and settings.\n\n**Practical Implications**\n\nThis research demonstrates that investing in larger retrieval corpora can be an effective path to stronger RAG, offering a practical alternative to enlarging LLMs. The results have significant implications for the design of real-world QA systems, enabling them to operate with smaller generators while maintaining high performance."
    },
    {
        "id": "2510.00586v1",
        "title": "Eyes-on-Me: Scalable RAG Poisoning through Transferable\n  Attention-Steering Attractors",
        "authors": [
            "Yen-Shan Chen",
            "Sian-Yao Huang",
            "Cheng-Lin Yang",
            "Yun-Nung Chen"
        ],
        "url": "https://arxiv.org/abs/2510.00586v1",
        "Abstract": "Abstract Existing data poisoning attacks on retrieval-augmented generation (RAG) systems scale poorly because they require costly optimization of poisoned documents for each target phrase. We introduce Eyes-on-Me , a modular attack that decomposes an adversarial document into reusable Attention Attractors and Focus Regions . Attractors are optimized to direct attention to the Focus Region. Attackers can then insert semantic baits for the retriever or malicious instructions for the generator, adapting to new targets at near zero cost. This is achieved by steering a small subset of attention heads that we empirically identify as strongly correlated with attack success. Across 18 end-to-end RAG settings (3 datasets × \\times 2 retrievers × \\times 3 generators), Eyes-on-Me raises average attack success rates from 21.9 to 57.8 (+35.9 points, 2.6 × \\times over prior work). A single optimized attractor transfers to unseen black box retrievers and generators without retraining. Our findings establish a scalable paradigm for RAG data poisoning and show that modular, reusable components pose a practical threat to modern AI systems. They also reveal a strong link between attention concentration and model outputs, informing interpretability research.",
        "Main": {
            "1 Introduction": "Retrieval-augmented generation (RAG) (Lewis et al., 2020 ) is a common strategy to reduce hallucinations by grounding large language models (LLMs) in external knowledge. That dependence, however, creates a critical attack surface: the underlying knowledge base can be manipulated via data poisoning . Early work studied query-specific poisoning , where an adversarial document is crafted to manipulate a single, complete user query string (Zou et al., 2025 ; Zhang et al., 2024d ) (illustrated in Fig. 1 ). In practice, this requires the attacker to know the exact query in advance, making the approach brittle to query variations. More recent work therefore moved to trigger-based attacks that associate an attack with a more general phrase or pattern (Chaudhari et al., 2024 ) . While these triggers improve flexibility and transferability, each new trigger still demands costly end-to-end re-optimization of the adversarial artifact, limiting scalability and rapid deployment. To address these limitations, we propose Eyes-on-Me , a modular attack paradigm for RAG that eliminates the need for repeated re-optimization. We decompose an adversarial document into a reusable Attention Attractor and a designated Focus Region that contains the Attack Payload . This separation enables a single attractor to be optimized once and then composed with diverse payloads, from semantic baits that fool retrievers to malicious instructions that steer generators, enabling the creation of new attacks at near-zero marginal cost. The architecture is enabled by an attention-guided proxy objective. Rather than brittle end-to-end optimization, we tune attractor tokens to steer a small, empirically identified subset of influential attention heads toward the Focus Region. By optimizing attention, the attractor amplifies the influence of any content placed in that region, supporting transfer across both the retriever and the generator. We evaluate Eyes-on-Me across 18 end-to-end RAG settings, covering 3 QA datasets (e.g., Natural Questions (Kwiatkowski et al., 2019 ) and MS MARCO (Nguyen et al., 2025 ) ), 2 retrievers (e.g., Qwen3-0.6-Embedding (Zhang et al., 2025b ) ), and 3 instruction-tuned LLMs (e.g., Qwen2.5-0.5B-Instruct (Team, 2024 ) ). The threat model is strict and realistic: a single poisoned document is inserted into a 1,000-document corpus ( ≤ 0.1 \\leq 0.1 %), the trigger phrase \\lily (e.g., president ) must appear in the queries, and the poisoned document competes with other trigger-relevant documents. Training uses no user queries; at test time, queries are LLM-generated and semantically related to the trigger. Under this setup, an optimized attractor paired with an LLM-generated payload attains an average attack success rate (ASR) of 57.8%, compared to 21.9% for state-of-the-art optimization-based methods (+35.9 pts; 2.6×). All methods use the same poisoned-document length budget, which ensures fairness. The modular design also transfers across retrievers, generators, and triggers, composes with diverse payloads, and enables reusable, low-cost attacks without retraining. Contributions. (1) We introduce Eyes-on-Me , a modular RAG-poisoning framework that decouples the attack into a reusable Attention Attractor and a swappable payload within a Focus Region, enabling new attacks without retraining. (2) We propose an attention-guided proxy objective that steers a subset of influential attention heads to that region, thereby amplifying any content placed within for both retrieval and generation. (3) Under a strict and realistic threat model, our method achieves 57.8% ASR across 18 RAG settings, substantially outperforming the 21.9% of prior work, with strong transfer across retrievers, generators, and triggers.",
            "3 Threat Model and Problem Formulation": "\\eric System and Attacker Setup. We consider a RAG system consisting of a document corpus 𝒟 = { d 1 , d 2 , … , d | 𝒟 | } \\mathcal{D}=\\{d_{1},d_{2},\\dots,d_{|\\mathcal{D}|}\\} ( d i d_{i} represents the i i -th document), a retriever R R , and a generator G G . Following prior work on knowledge poisoning Zou et al. ( 2025 ); Zhang et al. ( 2025a ) , we assume an attacker who can inject a small set of malicious documents 𝒟 mal \\mathcal{D}_{\\text{mal}} into the corpus, forming an augmented corpus 𝒟 ′ = 𝒟 ∪ 𝒟 mal \\mathcal{D}^{\\prime}=\\mathcal{D}\\cup\\mathcal{D}_{\\text{mal}} , where | 𝒟 mal | ≪ | 𝒟 | |\\mathcal{D}_{\\text{mal}}|\\ll|\\mathcal{D}| . This can be done via edits to user-editable sources (e.g., Wikipedia or internal KBs). We assume a white-box setting with full access to the retriever and generator (architectures, parameters, gradients); Sec. 5.3 relaxes this to evaluate transfer to black-box models. \\eric At inference, given a query q q , the retriever returns the top- k k set ℛ = R ( q , k , 𝒟 ′ ) ⊆ 𝒟 ′ \\mathcal{R}=R(q,k,\\mathcal{D}^{\\prime})\\subseteq\\mathcal{D^{\\prime}} ranked by a similarity score sim ⁡ ( q , d ) \\operatorname{sim}(q,d) (e.g., dot/cosine over embeddings). The generator then outputs a final response r = G ( q , ℛ ) r=G(q,\\mathcal{R}) conditioned on q q and the retrieved context. \\eric Attack Trigger and Scope. To activate the attack, the adversary defines a trigger phrase t t (e.g., “climate change”), which serves as the optimization anchor for crafting the malicious documents. The attack is activated for any query that the retriever deems semantically related to t t (not only exact matches). We denote this set of user queries as 𝒬 t \\mathcal{Q}_{t} and refer to them as targeted queries . This approach is practical as it does not require foreknowledge of specific user queries; the attacker only needs to target a general phrase expected to appear in natural language. \\lily To keep the threat model realistic, we require that each trigger appears in at least α % \\alpha\\% of benign queries, ensuring that attackers target naturally frequent user inputs rather than rare phrases. Moreover, we verify that these triggers also appear in benign documents; this way, malicious documents must outcompete many relevant benign ones, yielding a stricter and more realistic threat model. \\eric Attack Success Criteria. The attacker crafts 𝒟 mal \\mathcal{D}_{\\text{mal}} to achieve two primary goals: (i) be retrieved when a targeted query q ∈ 𝒬 t q\\in\\mathcal{Q}_{t} is issued; (ii) influence the output of generator to attacker-specified. \\eric A retrieval-phase attack is successful for a targeted query q ∈ 𝒬 t q\\in\\mathcal{Q}_{t} if and only if: ∃ d m ∈ 𝒟 mal such that d m ∈ R ( q , k , 𝒟 ′ ) \\exists d_{m}\\in\\mathcal{D}_{\\text{mal}}\\text{ such that }d_{m}\\in R(q,k,\\mathcal{D}^{\\prime}) (1) where 𝒞 mal ( r ) \\mathcal{C}_{\\text{mal}}(r) returns 1 1 when r r exhibits the attacker-specified malicious behavior (e.g., executing a forbidden instruction, leaking sensitive data, targeted disinformation). \\eric A generation-phase attack is successful for a targeted query q ∈ 𝒬 t q\\in\\mathcal{Q}_{t} if and only if: 𝒞 mal ( G ( q , R ( q , k , 𝒟 ′ ) ) ) = 1 and ∃ d m ∈ 𝒟 mal : d m ∈ R ( q , k , 𝒟 ′ ) . \\mathcal{C}_{\\text{mal}}\\!\\big(G(q,R(q,k,\\mathcal{D}^{\\prime}))\\big)=1\\ \\text{and}\\ \\exists d_{m}\\in\\mathcal{D}_{\\text{mal}}\\colon d_{m}\\in R(q,k,\\mathcal{D}^{\\prime}). (2)",
            "4 Methodology": "We (i) decompose each malicious document into a reusable Attention Attractor and a swappable payload placed in a designated Focus Region (Sec. 4.1 ); (ii) optimize the attractor with an attention-guided proxy to concentrate impactful heads on that Focus Region (Sec. 4.2 ); and (iii) instantiate the attractor via HotFlip under a fluency constraint (Sec. 4.3 ). See Figure 2 for a framework overview.",
            "6 Ablation Studies and Analyses": "\\eric In this section, we analyze our attack’s sensitivity to document variables (Sec. 6.1 ) and hyperparameters (Sec. 6.2 ), as well as its robustness against SOTA defenses(Sec. 6.3 ).",
            "7 Conclusions": "\\eric We propose Eyes-on-Me , a scalable and modular RAG poisoning framework. By decoupling adversarial documents into reusable Attention Attractors and Focus Regions , our method strategically steers model attention across retriever and generator components, shaping both retrieval ranking and generation outcomes. Experiments across 18 RAG configurations show that Eyes-on-Me improves end-to-end attack success rates by up to 2.6 × \\times over optimization-based baselines, while maintaining constant optimization cost and resilience against practical defenses. Beyond these empirical findings, our study highlights two insights. First, realistic retrieval distributions with frequent benign triggers are essential for evaluating attack effectiveness, exposing the weakness of prior optimization-based methods. Second, attention concentration in specific heads strongly shapes model behavior, highlighting opportunities for mechanistic interpretability or defense design. \\eric Limitations. Although our method generalizes across diverse RAG settings and maintains strong robustness, it is less effective for highly complex malicious instructions (e.g., URL-style payloads), and its influence may be weakened under retrievers that aggregate token representations (e.g., mean pooling). Moreover, its interaction with rerankers in practical RAG systems is unexplored. Addressing these cases requires more fine-grained attention steering, which we leave for future work.",
            "Ethics Statement": "\\lily This work investigates a database poisoning attack in RAG systems. While our methods reveal ways to manipulate model behavior, the intention of this research is strictly to advance understanding of LLM safety and to motivate the development of more robust defenses. \\lily Disclosure of LLM Usage. LLMs were used as an assistive writing tool, a generator of synthetic data (see Section 5.1 ), and a coding agent to help implement some straightforward algorithms. All scientific contributions, experimental designs, and analysis were performed by the authors. All final content has been critically reviewed and verified by the authors to ensure accuracy.",
            "Reproducibility Statement": "\\lily To ensure reproducibility, we provide detailed descriptions of our experimental setup and also release code, scripts, and configuration files to enable others to replicate and extend our work. Random seeds were fixed where possible. However, while we make strong efforts to ensure reproducibility, ASR outcomes may still vary depending on attack objectives, trigger selection, corpus composition, query selection, and other hyperparameters."
        },
        "Tuples": [
            [
                "1 Introduction",
                "Retrieval-augmented generation (RAG) (Lewis et al., 2020 ) is a common strategy to reduce hallucinations by grounding large language models (LLMs) in external knowledge. That dependence, however, creates a critical attack surface: the underlying knowledge base can be manipulated via data poisoning . Early work studied query-specific poisoning , where an adversarial document is crafted to manipulate a single, complete user query string (Zou et al., 2025 ; Zhang et al., 2024d ) (illustrated in Fig. 1 ). In practice, this requires the attacker to know the exact query in advance, making the approach brittle to query variations. More recent work therefore moved to trigger-based attacks that associate an attack with a more general phrase or pattern (Chaudhari et al., 2024 ) . While these triggers improve flexibility and transferability, each new trigger still demands costly end-to-end re-optimization of the adversarial artifact, limiting scalability and rapid deployment. To address these limitations, we propose Eyes-on-Me , a modular attack paradigm for RAG that eliminates the need for repeated re-optimization. We decompose an adversarial document into a reusable Attention Attractor and a designated Focus Region that contains the Attack Payload . This separation enables a single attractor to be optimized once and then composed with diverse payloads, from semantic baits that fool retrievers to malicious instructions that steer generators, enabling the creation of new attacks at near-zero marginal cost. The architecture is enabled by an attention-guided proxy objective. Rather than brittle end-to-end optimization, we tune attractor tokens to steer a small, empirically identified subset of influential attention heads toward the Focus Region. By optimizing attention, the attractor amplifies the influence of any content placed in that region, supporting transfer across both the retriever and the generator. We evaluate Eyes-on-Me across 18 end-to-end RAG settings, covering 3 QA datasets (e.g., Natural Questions (Kwiatkowski et al., 2019 ) and MS MARCO (Nguyen et al., 2025 ) ), 2 retrievers (e.g., Qwen3-0.6-Embedding (Zhang et al., 2025b ) ), and 3 instruction-tuned LLMs (e.g., Qwen2.5-0.5B-Instruct (Team, 2024 ) ). The threat model is strict and realistic: a single poisoned document is inserted into a 1,000-document corpus ( ≤ 0.1 \\leq 0.1 %), the trigger phrase \\lily (e.g., president ) must appear in the queries, and the poisoned document competes with other trigger-relevant documents. Training uses no user queries; at test time, queries are LLM-generated and semantically related to the trigger. Under this setup, an optimized attractor paired with an LLM-generated payload attains an average attack success rate (ASR) of 57.8%, compared to 21.9% for state-of-the-art optimization-based methods (+35.9 pts; 2.6×). All methods use the same poisoned-document length budget, which ensures fairness. The modular design also transfers across retrievers, generators, and triggers, composes with diverse payloads, and enables reusable, low-cost attacks without retraining. Contributions. (1) We introduce Eyes-on-Me , a modular RAG-poisoning framework that decouples the attack into a reusable Attention Attractor and a swappable payload within a Focus Region, enabling new attacks without retraining. (2) We propose an attention-guided proxy objective that steers a subset of influential attention heads to that region, thereby amplifying any content placed within for both retrieval and generation. (3) Under a strict and realistic threat model, our method achieves 57.8% ASR across 18 RAG settings, substantially outperforming the 21.9% of prior work, with strong transfer across retrievers, generators, and triggers."
            ],
            [
                "3 Threat Model and Problem Formulation",
                "\\eric System and Attacker Setup. We consider a RAG system consisting of a document corpus 𝒟 = { d 1 , d 2 , … , d | 𝒟 | } \\mathcal{D}=\\{d_{1},d_{2},\\dots,d_{|\\mathcal{D}|}\\} ( d i d_{i} represents the i i -th document), a retriever R R , and a generator G G . Following prior work on knowledge poisoning Zou et al. ( 2025 ); Zhang et al. ( 2025a ) , we assume an attacker who can inject a small set of malicious documents 𝒟 mal \\mathcal{D}_{\\text{mal}} into the corpus, forming an augmented corpus 𝒟 ′ = 𝒟 ∪ 𝒟 mal \\mathcal{D}^{\\prime}=\\mathcal{D}\\cup\\mathcal{D}_{\\text{mal}} , where | 𝒟 mal | ≪ | 𝒟 | |\\mathcal{D}_{\\text{mal}}|\\ll|\\mathcal{D}| . This can be done via edits to user-editable sources (e.g., Wikipedia or internal KBs). We assume a white-box setting with full access to the retriever and generator (architectures, parameters, gradients); Sec. 5.3 relaxes this to evaluate transfer to black-box models. \\eric At inference, given a query q q , the retriever returns the top- k k set ℛ = R ( q , k , 𝒟 ′ ) ⊆ 𝒟 ′ \\mathcal{R}=R(q,k,\\mathcal{D}^{\\prime})\\subseteq\\mathcal{D^{\\prime}} ranked by a similarity score sim ⁡ ( q , d ) \\operatorname{sim}(q,d) (e.g., dot/cosine over embeddings). The generator then outputs a final response r = G ( q , ℛ ) r=G(q,\\mathcal{R}) conditioned on q q and the retrieved context. \\eric Attack Trigger and Scope. To activate the attack, the adversary defines a trigger phrase t t (e.g., “climate change”), which serves as the optimization anchor for crafting the malicious documents. The attack is activated for any query that the retriever deems semantically related to t t (not only exact matches). We denote this set of user queries as 𝒬 t \\mathcal{Q}_{t} and refer to them as targeted queries . This approach is practical as it does not require foreknowledge of specific user queries; the attacker only needs to target a general phrase expected to appear in natural language. \\lily To keep the threat model realistic, we require that each trigger appears in at least α % \\alpha\\% of benign queries, ensuring that attackers target naturally frequent user inputs rather than rare phrases. Moreover, we verify that these triggers also appear in benign documents; this way, malicious documents must outcompete many relevant benign ones, yielding a stricter and more realistic threat model. \\eric Attack Success Criteria. The attacker crafts 𝒟 mal \\mathcal{D}_{\\text{mal}} to achieve two primary goals: (i) be retrieved when a targeted query q ∈ 𝒬 t q\\in\\mathcal{Q}_{t} is issued; (ii) influence the output of generator to attacker-specified. \\eric A retrieval-phase attack is successful for a targeted query q ∈ 𝒬 t q\\in\\mathcal{Q}_{t} if and only if: ∃ d m ∈ 𝒟 mal such that d m ∈ R ( q , k , 𝒟 ′ ) \\exists d_{m}\\in\\mathcal{D}_{\\text{mal}}\\text{ such that }d_{m}\\in R(q,k,\\mathcal{D}^{\\prime}) (1) where 𝒞 mal ( r ) \\mathcal{C}_{\\text{mal}}(r) returns 1 1 when r r exhibits the attacker-specified malicious behavior (e.g., executing a forbidden instruction, leaking sensitive data, targeted disinformation). \\eric A generation-phase attack is successful for a targeted query q ∈ 𝒬 t q\\in\\mathcal{Q}_{t} if and only if: 𝒞 mal ( G ( q , R ( q , k , 𝒟 ′ ) ) ) = 1 and ∃ d m ∈ 𝒟 mal : d m ∈ R ( q , k , 𝒟 ′ ) . \\mathcal{C}_{\\text{mal}}\\!\\big(G(q,R(q,k,\\mathcal{D}^{\\prime}))\\big)=1\\ \\text{and}\\ \\exists d_{m}\\in\\mathcal{D}_{\\text{mal}}\\colon d_{m}\\in R(q,k,\\mathcal{D}^{\\prime}). (2)"
            ],
            [
                "4 Methodology",
                "We (i) decompose each malicious document into a reusable Attention Attractor and a swappable payload placed in a designated Focus Region (Sec. 4.1 ); (ii) optimize the attractor with an attention-guided proxy to concentrate impactful heads on that Focus Region (Sec. 4.2 ); and (iii) instantiate the attractor via HotFlip under a fluency constraint (Sec. 4.3 ). See Figure 2 for a framework overview."
            ],
            [
                "6 Ablation Studies and Analyses",
                "\\eric In this section, we analyze our attack’s sensitivity to document variables (Sec. 6.1 ) and hyperparameters (Sec. 6.2 ), as well as its robustness against SOTA defenses(Sec. 6.3 )."
            ],
            [
                "7 Conclusions",
                "\\eric We propose Eyes-on-Me , a scalable and modular RAG poisoning framework. By decoupling adversarial documents into reusable Attention Attractors and Focus Regions , our method strategically steers model attention across retriever and generator components, shaping both retrieval ranking and generation outcomes. Experiments across 18 RAG configurations show that Eyes-on-Me improves end-to-end attack success rates by up to 2.6 × \\times over optimization-based baselines, while maintaining constant optimization cost and resilience against practical defenses. Beyond these empirical findings, our study highlights two insights. First, realistic retrieval distributions with frequent benign triggers are essential for evaluating attack effectiveness, exposing the weakness of prior optimization-based methods. Second, attention concentration in specific heads strongly shapes model behavior, highlighting opportunities for mechanistic interpretability or defense design. \\eric Limitations. Although our method generalizes across diverse RAG settings and maintains strong robustness, it is less effective for highly complex malicious instructions (e.g., URL-style payloads), and its influence may be weakened under retrievers that aggregate token representations (e.g., mean pooling). Moreover, its interaction with rerankers in practical RAG systems is unexplored. Addressing these cases requires more fine-grained attention steering, which we leave for future work."
            ],
            [
                "Ethics Statement",
                "\\lily This work investigates a database poisoning attack in RAG systems. While our methods reveal ways to manipulate model behavior, the intention of this research is strictly to advance understanding of LLM safety and to motivate the development of more robust defenses. \\lily Disclosure of LLM Usage. LLMs were used as an assistive writing tool, a generator of synthetic data (see Section 5.1 ), and a coding agent to help implement some straightforward algorithms. All scientific contributions, experimental designs, and analysis were performed by the authors. All final content has been critically reviewed and verified by the authors to ensure accuracy."
            ],
            [
                "Reproducibility Statement",
                "\\lily To ensure reproducibility, we provide detailed descriptions of our experimental setup and also release code, scripts, and configuration files to enable others to replicate and extend our work. Random seeds were fixed where possible. However, while we make strong efforts to ensure reproducibility, ASR outcomes may still vary depending on attack objectives, trigger selection, corpus composition, query selection, and other hyperparameters."
            ]
        ],
        "section_summaries": [
            [
                "1 Introduction",
                "Retrieval-augmented generation (RAG) (Lewis et al., 2020 ) is a common strategy to reduce hallucinations by grounding large language models (LLMs) in external knowledge. That dependence, however, creates a critical attack surface: the underlying knowledge base can be manipulated via data poisoning . Early work studied query-specific poisoning , where an adversarial document is crafted to manipulate a single, complete user query string (Zou et al., 2025 ; Zhang et al., 2024d ) (illustrated in Fig. 1 ). In practice, this requires the attacker to know the exact query in advance, making the approach brittle to query variations. More recent work therefore moved to trigger-based attacks that associate an attack with a more general phrase or pattern (Chaudhari et al., 2024 ) . While these triggers improve flexibility and transferability, each new trigger still demands costly end-to-end re-optimization of the adversarial artifact, limiting scalability and rapid deployment. To address these limitations, we propose Eyes-on-Me , a modular attack paradigm for RAG that eliminates the need for repeated re-optimization. We decompose an adversarial document into a reusable Attention Attractor and a designated Focus Region that contains the Attack Payload . This separation enables a single attractor to be optimized once and then composed with diverse payloads, from semantic baits that fool retrievers to malicious instructions that steer generators, enabling the creation of new attacks at near-zero marginal cost. The architecture is enabled by an attention-guided proxy objective. Rather than brittle end-to-end optimization, we tune attractor tokens to steer a small, empirically identified subset of influential attention heads toward the Focus Region. By optimizing attention, the attractor amplifies the influence of any content placed in that region, supporting transfer across both the retriever and the generator. We evaluate Eyes-on-Me across 18 end-to-end RAG settings, covering 3 QA datasets (e.g., Natural Questions (Kwiatkowski et al., 2019 ) and MS MARCO (Nguyen et al., 2025 ) ), 2 retrievers (e.g., Qwen3-0.6-Embedding (Zhang et al., 2025b ) ), and 3 instruction-tuned LLMs (e.g., Qwen2.5-0.5B-Instruct (Team, 2024 ) ). The threat model is strict and realistic: a single poisoned document is inserted into a 1,000-document corpus ( ≤ 0.1 \\leq 0.1 %), the trigger phrase \\lily (e.g., president ) must appear in the queries, and the poisoned document competes with other trigger-relevant documents. Training uses no user queries; at test time, queries are LLM-generated and semantically related to the trigger. Under this setup, an optimized attractor paired with an LLM-generated payload attains an average attack success rate (ASR) of 57.8%, compared to 21.9% for state-of-the-art optimization-based methods (+35.9 pts; 2.6×). All methods use the same poisoned-document length budget, which ensures fairness. The modular design also transfers across retrievers, generators, and triggers, composes with diverse payloads, and enables reusable, low-cost attacks without retraining. Contributions. (1) We introduce Eyes-on-Me , a modular RAG-poisoning framework that decouples the attack into a reusable Attention Attractor and a swappable payload within a Focus Region, enabling new attacks without retraining. (2) We propose an attention-guided proxy objective that steers a subset of influential attention heads to that region, thereby amplifying any content placed within for both retrieval and generation. (3) Under a strict and realistic threat model, our method achieves 57.8% ASR across 18 RAG settings, substantially outperforming the 21.9% of prior work, with strong transfer across retrievers, generators, and triggers.",
                "Lewis et al. (2020), Zou et al. (2025), Zhang et al. (2024d), Chaudhari et al. (2024) \nKnowledge base manipulation via data poisoning is a threat to RAG models, particularly query-specific poisoning and trigger-based attacks.\nEyes-on-Me, a modular attack paradigm for RAG, eliminates re-optimization by decomposing an adversarial document into Attention Attractor and Focus Region.\n18 end-to-end RAG settings were evaluated across 3 QA datasets (Natural Questions, MS MARCO), 2 retrievers (Qwen3-0.6-Embedding) and 3 instruction-tuned LLMs (Qwen2.5-0.5B-Instruct).\nOptimized attractor paired with an LLM-generated payload attains 57.8% average attack success rate, compared to 21.9% for state-of-the-art optimization-based methods.\nContribution: Eyes-on-Me introduces a modular RAG-poisoning framework that decouples the attack and enables new attacks without retraining."
            ],
            [
                "3 Threat Model and Problem Formulation",
                "\\eric System and Attacker Setup. We consider a RAG system consisting of a document corpus 𝒟 = { d 1 , d 2 , … , d | 𝒟 | } \\mathcal{D}=\\{d_{1},d_{2},\\dots,d_{|\\mathcal{D}|}\\} ( d i d_{i} represents the i i -th document), a retriever R R , and a generator G G . Following prior work on knowledge poisoning Zou et al. ( 2025 ); Zhang et al. ( 2025a ) , we assume an attacker who can inject a small set of malicious documents 𝒟 mal \\mathcal{D}_{\\text{mal}} into the corpus, forming an augmented corpus 𝒟 ′ = 𝒟 ∪ 𝒟 mal \\mathcal{D}^{\\prime}=\\mathcal{D}\\cup\\mathcal{D}_{\\text{mal}} , where | 𝒟 mal | ≪ | 𝒟 | |\\mathcal{D}_{\\text{mal}}|\\ll|\\mathcal{D}| . This can be done via edits to user-editable sources (e.g., Wikipedia or internal KBs). We assume a white-box setting with full access to the retriever and generator (architectures, parameters, gradients); Sec. 5.3 relaxes this to evaluate transfer to black-box models. \\eric At inference, given a query q q , the retriever returns the top- k k set ℛ = R ( q , k , 𝒟 ′ ) ⊆ 𝒟 ′ \\mathcal{R}=R(q,k,\\mathcal{D}^{\\prime})\\subseteq\\mathcal{D^{\\prime}} ranked by a similarity score sim ⁡ ( q , d ) \\operatorname{sim}(q,d) (e.g., dot/cosine over embeddings). The generator then outputs a final response r = G ( q , ℛ ) r=G(q,\\mathcal{R}) conditioned on q q and the retrieved context. \\eric Attack Trigger and Scope. To activate the attack, the adversary defines a trigger phrase t t (e.g., “climate change”), which serves as the optimization anchor for crafting the malicious documents. The attack is activated for any query that the retriever deems semantically related to t t (not only exact matches). We denote this set of user queries as 𝒬 t \\mathcal{Q}_{t} and refer to them as targeted queries . This approach is practical as it does not require foreknowledge of specific user queries; the attacker only needs to target a general phrase expected to appear in natural language. \\lily To keep the threat model realistic, we require that each trigger appears in at least α % \\alpha\\% of benign queries, ensuring that attackers target naturally frequent user inputs rather than rare phrases. Moreover, we verify that these triggers also appear in benign documents; this way, malicious documents must outcompete many relevant benign ones, yielding a stricter and more realistic threat model. \\eric Attack Success Criteria. The attacker crafts 𝒟 mal \\mathcal{D}_{\\text{mal}} to achieve two primary goals: (i) be retrieved when a targeted query q ∈ 𝒬 t q\\in\\mathcal{Q}_{t} is issued; (ii) influence the output of generator to attacker-specified. \\eric A retrieval-phase attack is successful for a targeted query q ∈ 𝒬 t q\\in\\mathcal{Q}_{t} if and only if: ∃ d m ∈ 𝒟 mal such that d m ∈ R ( q , k , 𝒟 ′ ) \\exists d_{m}\\in\\mathcal{D}_{\\text{mal}}\\text{ such that }d_{m}\\in R(q,k,\\mathcal{D}^{\\prime}) (1) where 𝒞 mal ( r ) \\mathcal{C}_{\\text{mal}}(r) returns 1 1 when r r exhibits the attacker-specified malicious behavior (e.g., executing a forbidden instruction, leaking sensitive data, targeted disinformation). \\eric A generation-phase attack is successful for a targeted query q ∈ 𝒬 t q\\in\\mathcal{Q}_{t} if and only if: 𝒞 mal ( G ( q , R ( q , k , 𝒟 ′ ) ) ) = 1 and ∃ d m ∈ 𝒟 mal : d m ∈ R ( q , k , 𝒟 ′ ) . \\mathcal{C}_{\\text{mal}}\\!\\big(G(q,R(q,k,\\mathcal{D}^{\\prime}))\\big)=1\\ \\text{and}\\ \\exists d_{m}\\in\\mathcal{D}_{\\text{mal}}\\colon d_{m}\\in R(q,k,\\mathcal{D}^{\\prime}). (2)",
                "𝒟 = {d1, d2, … , d|𝒟|}, a retriever R and a generator G\nAn attacker injects 𝒟mal into 𝒟, forming 𝒟′ with |𝒟mal| ≪ |𝒟|\nThe attacker uses a white-box setting to access the retriever and generator architectures, parameters, and gradients\nAt inference, the retriever returns the top-k set ℛ = R(q, k, 𝒟′) ⊆ 𝒟′\nThe generator outputs r = G(q, ℛ)\nThe attack is triggered by a phrase t (e.g., “climate change”) in user queries 𝒬t\nThe attacker requires that each trigger appears in at least α% of benign queries and also appears in benign documents\nThe attacker crafts 𝒟mal to achieve two primary goals: \n(i) be retrieved when a targeted query q ∈ 𝒬t is issued \n(ii) influence the output of generator to attacker-specified behavior\nA retrieval-phase attack is successful if ∃ dm ∈ 𝒟mal such that dm ∈ R(q, k, 𝒟′)\nA generation-phase attack is successful if 𝒞mal(G(q, R(q, k, 𝒟′))) = 1 and ∃ dm ∈ 𝒟mal: dm ∈ R(q, k, 𝒟′)"
            ],
            [
                "4 Methodology",
                "We (i) decompose each malicious document into a reusable Attention Attractor and a swappable payload placed in a designated Focus Region (Sec. 4.1 ); (ii) optimize the attractor with an attention-guided proxy to concentrate impactful heads on that Focus Region (Sec. 4.2 ); and (iii) instantiate the attractor via HotFlip under a fluency constraint (Sec. 4.3 ). See Figure 2 for a framework overview.",
                "We decompose malicious documents into reusable Attention Attractor and swappable payload, optimize with attention-guided proxy, and instantiate via HotFlip under fluency constraint."
            ],
            [
                "6 Ablation Studies and Analyses",
                "\\eric In this section, we analyze our attack’s sensitivity to document variables (Sec. 6.1 ) and hyperparameters (Sec. 6.2 ), as well as its robustness against SOTA defenses(Sec. 6.3 ).",
                "Sensitivity analysis on document variables, hyperparameters, and SOTA defenses is conducted in Sec. 6.1, Sec. 6.2, and Sec. 6.3, respectively."
            ],
            [
                "7 Conclusions",
                "\\eric We propose Eyes-on-Me , a scalable and modular RAG poisoning framework. By decoupling adversarial documents into reusable Attention Attractors and Focus Regions , our method strategically steers model attention across retriever and generator components, shaping both retrieval ranking and generation outcomes. Experiments across 18 RAG configurations show that Eyes-on-Me improves end-to-end attack success rates by up to 2.6 × \\times over optimization-based baselines, while maintaining constant optimization cost and resilience against practical defenses. Beyond these empirical findings, our study highlights two insights. First, realistic retrieval distributions with frequent benign triggers are essential for evaluating attack effectiveness, exposing the weakness of prior optimization-based methods. Second, attention concentration in specific heads strongly shapes model behavior, highlighting opportunities for mechanistic interpretability or defense design. \\eric Limitations. Although our method generalizes across diverse RAG settings and maintains strong robustness, it is less effective for highly complex malicious instructions (e.g., URL-style payloads), and its influence may be weakened under retrievers that aggregate token representations (e.g., mean pooling). Moreover, its interaction with rerankers in practical RAG systems is unexplored. Addressing these cases requires more fine-grained attention steering, which we leave for future work.",
                "Eyes-on-Me improves attack success rates by up to 2.6 × over baselines across 18 RAG configurations. It highlights the importance of realistic retrieval distributions and attention concentration in specific heads. Limitations include reduced effectiveness for complex malicious instructions and interactions with rerankers, requiring further investigation."
            ],
            [
                "Ethics Statement",
                "\\lily This work investigates a database poisoning attack in RAG systems. While our methods reveal ways to manipulate model behavior, the intention of this research is strictly to advance understanding of LLM safety and to motivate the development of more robust defenses. \\lily Disclosure of LLM Usage. LLMs were used as an assistive writing tool, a generator of synthetic data (see Section 5.1 ), and a coding agent to help implement some straightforward algorithms. All scientific contributions, experimental designs, and analysis were performed by the authors. All final content has been critically reviewed and verified by the authors to ensure accuracy.",
                "This work investigates database poisoning attacks in RAG systems, aims to advance LLM safety, and promote robust defenses development. \nLLMs were used as an assistive tool for writing and generating synthetic data (Section 5.1). \nAll scientific contributions, experimental designs, and analysis were performed by the authors."
            ],
            [
                "Reproducibility Statement",
                "\\lily To ensure reproducibility, we provide detailed descriptions of our experimental setup and also release code, scripts, and configuration files to enable others to replicate and extend our work. Random seeds were fixed where possible. However, while we make strong efforts to ensure reproducibility, ASR outcomes may still vary depending on attack objectives, trigger selection, corpus composition, query selection, and other hyperparameters.",
                "Lily, experimental setup description released with code and scripts, random seeds fixed. Reproducibility efforts made but ASR outcomes may vary due to factors like attack objectives, trigger selection, corpus composition, query selection, and hyperparameters."
            ]
        ],
        "general_summary": "**Research Overview**\n\nThis research addresses the problem of data poisoning attacks on retrieval-augmented generation (RAG) systems, which pose a threat to modern AI systems. The study introduces Eyes-on-Me , a modular attack paradigm that decomposes an adversarial document into reusable Attention Attractors and Focus Regions, enabling scalable and efficient attacks.\n\n**Methodology & Approach**\n\nThe proposed solution involves decomposing malicious documents into two components: Attention Attractors, which are optimized to direct attention to the Focus Region, and a swappable payload. The approach uses an attention-guided proxy for optimization and instantiates the attack via HotFlip under fluency constraints. This modular design allows attackers to adapt to new targets at near zero cost.\n\n**Key Contributions & Findings**\n\nThe study evaluates Eyes-on-Me across 18 end-to-end RAG settings, achieving average attack success rates of 57.8%, a significant improvement over prior work (21.9%). A single optimized Attractor transfers to unseen black-box retrievers and generators without retraining. The findings establish a scalable paradigm for RAG data poisoning and highlight the importance of realistic retrieval distributions and attention concentration.\n\n**Technical Details**\n\nThe study employs an attention-guided proxy for optimization, which steers a small subset of attention heads strongly correlated with attack success. The approach uses HotFlip under fluency constraints to instantiate the attack. This modular design decouples the attack from the payload, enabling efficient adaptation to new targets.\n\n**Limitations & Future Work**\n\nThe study notes that Eyes-on-Me is less effective for complex malicious instructions and interactions with rerankers, requiring further investigation. Additionally, ASR outcomes may vary due to factors such as attack objectives, trigger selection, corpus composition, query selection, and hyperparameters.\n\n**Practical Implications**\n\nThis research has significant implications for the development of robust defenses against RAG data poisoning attacks. The findings highlight the importance of realistic retrieval distributions and attention concentration in specific heads, informing interpretability research and advancing LLM safety."
    },
    {
        "id": "2510.05310v1",
        "title": "RAG Makes Guardrails Unsafe? Investigating Robustness of Guardrails\n  under RAG-style Contexts",
        "authors": [
            "Yining She",
            "Daniel W. Peterson",
            "Marianne Menglin Liu",
            "Vikas Upadhyay",
            "Mohammad Hossein Chaghazardi",
            "Eunsuk Kang",
            "Dan Roth"
        ],
        "url": "https://arxiv.org/abs/2510.05310v1",
        "Abstract": "Abstract With the increasing adoption of large language models (LLMs), ensuring the safety of LLM systems has become a pressing concern.\nExternal LLM-based guardrail models have emerged as a popular solution to screen unsafe inputs and outputs, but they are themselves fine-tuned or prompt-engineered LLMs that are vulnerable to data distribution shifts.\nIn this paper, taking Retrieval Augmentation Generation (RAG) as a case study, we investigated how robust LLM-based guardrails are against additional information embedded in the context.\nThrough a systematic evaluation of 3 Llama Guards and 2 GPT-oss models, we confirmed that inserting benign documents into the guardrail context alters the judgments of input and output guardrails in around 11% and 8% of cases , making them unreliable.\nWe separately analyzed the effect of each component in the augmented context: retrieved documents, user query, and LLM-generated response.\nThe two mitigation methods we tested only bring minor improvements.\nThese results expose a context-robustness gap in current guardrails and motivate training and evaluation protocols that are robust to retrieval and query composition.",
        "Main": {
            "1 Introduction": "Large language models (LLMs) have rapidly become a central component of modern AI systems, powering applications from conversational assistants to code generation (Brown et al., 2020 ; Jiang et al., 2024 ) . Their ability to generalize across domains and tasks has made them widely adopted in real-world deployments (Hadi et al., 2023 ) . However, the same flexibility that enables their success also raises serious concerns about safety. LLMs are known to occasionally produce harmful, biased, or otherwise unsafe outputs, which poses significant risks when these models are used by millions of end users (Bai et al., 2022b ; Ganguli et al., 2022 ; Gallegos et al., 2024 ; She et al., 2025 ; Guo et al., 2025 ) . To mitigate such risks, the research community and industry have invested heavily in methods for aligning LLMs with human safety preferences. Two main strategies have emerged: direct safety fine-tuning of base models (Ouyang et al., 2022 ) and the use of external guardrails (Rebedea et al., 2023 ) . Guardrail models serve as dedicated safety filters layered on top of generation, offering flexibility and modularity. They can be updated independently of the base model, deployed as both input and output filters, and integrated into existing systems without retraining (Hurst et al., 2024 ) . Most guardrails are themselves LLM-based (e.g., Llama Guard (Inan et al., 2023 ) ). Leveraging the expressive power of LLMs allows guardrails to handle nuanced, context-dependent safety decisions. However, this also exposes them to the same vulnerabilities as the models that they are meant to protect. Prior work (Liu et al., 2024 ) has shown that LLMs are sensitive to the information contained in their context, and even benign additions can cause shifts in their behavior. For example, a recent work (An et al., 2025 ) suggests that Retrieval-Augmented Generation (RAG) may also increase the risk of unsafe or malicious generations, since safety alignment methods such as RLHF are typically applied in non-RAG settings. This raises an important but underexplored question: do LLM-based guardrails, when provided with richer contexts, alter their safety judgments? To address this question, we take RAG as a case study to investigate the robustness of guardrails under such conditions, since RAG is a widely adopted paradigm for improving the factuality and relevance of LLM outputs (Gao et al., 2023 ) . We consider two settings as shown in Fig. 1 : (1) normal RAG setting, where guardrails check user query or the query-response pair without exposure to retrieved documents, (2) RAG-style query setting, where the query has been augmented with retrieved documents and would be passed in guardrails as a whole. In this work, we conducted a systematic evaluation study of the robustness of LLM-based guardrails under RAG-style context . We introduce a novel implementation of robustness metric, Flip Rate . This metric measures the frequency with which guardrail judgments change between a vanilla and a RAG-augmented setting, and can be computed without ground-truth labeling. Using Flip Rate , we comprehensively evaluated three Llama Guard models and two GPT-oss models. We posed the following three research questions: RQ1: How does RAG-style context affect LLM-based guardrail judgment? We assessed 5 popular LLM-based guardrails on over 6,000 harmful queries and the responses to them generated by 8 LLMs with non-RAG-style and RAG-style context separately. We found that RAG-style context leads the guardrails to flip their judgments in both input guardrail and output guardrail settings. For example, well-aligned models like GPT-oss-20B give opposite judgments in around 15.0 % 15.0\\% cases when used as an output guardrail. RQ2: How does each component of RAG-style context affect the robustness of guardrails? We isolated each component of RAG-style context and examined its individual effect on robustness. Our results show that (1) the relevance between retrieved documents and user query exacerbates the vulnerability, while the number of documents have minor effects, (2) guardrails flip safety judgments due to context shifts, regardless of whether the query is safe or unsafe, (3) responses generated by different LLMs affects the guardrail differently. RQ3: Can general LLM enhancements mitigate this safety concern? We explored two potential general-purpose mitigations: high-reasoning-effort mode, and RAG-style-context-aware prompting. Both improved robustness by lowering flip rate, but neither solved the issue completely, highlighting the need for future research on guardrail techniques specifically tailored to RAG-style contexts.",
            "2 Related Works": "Guardrail models. LLMs have become increasingly powerful and widely deployed, but their open-ended generation abilities also introduce new safety challenges. To mitigate these risks, guardrails, the external defense layers that monitor and control LLM interactions, have emerged as a crucial solution (Inan et al., 2023 ; Markov et al., 2023 ; Wang et al., 2024 ; Han et al., 2024 ; Kang & Li, 2025 ; Ghosh et al., 2024 ) . These mechanisms offer a distinct advantage over internal alignment techniques like RLHF (Ouyang et al., 2022 ; Bai et al., 2022a ) by effectively filtering malicious inputs and outputs without compromising the core integrity of the base LLM (Dong et al., 2024 ) . Existing guardrail evaluations focus on plain inputs or output checks (Mazeika et al., 2024 ; Zou et al., 2023 ; Radharapu et al., 2023 ; Bhardwaj & Poria, 2023 ; Shaikh et al., 2023 ; Bhardwaj et al., 2024 ; Deng et al., 2023 ; Bassani & Sanchez, 2024 ; Lin et al., 2023 ) , while our study targets a blind spot where the content under classification contains retrieved documents (RAG-style context). Safety of RAG. RAG introduces unique security challenges beyond vanilla LLM generation, as the integration of external knowledge corpora creates novel attack surfaces. A growing body of work demonstrates that adversaries can poison indices, implant backdoors, or craft retrieval-optimized injections that steer models toward unsafe behavior (Xue et al., 2024 ; Zou et al., 2025 ; Cheng et al., 2024 ) . Liang et al. ( 2025 ) and Ni et al. ( 2025 ) conducted benchmarks and surveys to further catalog these threats and showed that vulnerabilities span indexing, retrieval, filtering, and generation stages. Furthermore, beyond malicious content, the inherent properties of the corpus can lead to unwanted responses in other ways. For example, Wu et al. ( 2025 ) showed that demographic biases present in the retrieval data can persist or even be amplified by the RAG pipeline, while Zeng et al. ( 2024 ) found that RAG can leak proprietary retrieval database. Our work examines how benign context shifts affect guardrails, diverging from the poisoned-corpus threat model. This parallels An et al. ( 2025 ) , which provided the first comprehensive analysis of RAG’s impact on LLM safety. They found that incorporating retrieval often makes LLM less safe and alters its safety profile even if the RAG corpus is secured. However, their evaluation focused exclusively on safety-aligned LLMs, without considering external guardrail models. In contrast, our work centered on evaluating guardrail models in the RAG settings.",
            "3 Problem Setup and Robustness Metric": "In this section, we formalize the concepts underlying our study and introduce the robustness metric that we will use throughout the experiments.",
            "4 RQ1: How does RAG-style context affect guardrail judgment?": "We first investigate whether RAG-style context perturbs the safety judgments of guardrail models.",
            "5 RQ2: How does each component of RAG-style context affect the robustness of guardrails?": "In this section, we isolate each component of the RAG-style context and examine its individual effect on robustness. A RAG-style context consists of (i) retrieved documents, (ii) the user query, and (iii) the LLM-generated responses (for output guardrails only). We discuss them one by one.",
            "6 RQ3: Can general LLM enhancements mitigate this?": "Having established that guardrails are susceptible to perturbations from RAG-style context, we next investigate whether general LLM enhancement techniques can alleviate this issue. Our goal here is not to provide a comprehensive solution, but rather to conduct a preliminary exploration of whether such general-purpose methods can reduce the observed vulnerability and to identify potential directions for future research. We focus on two representative strategies: (1) employing models with high reasoning effort, and (2) modifying prompts to explicitly account for retrieved documents in the context.",
            "7 Discussion and Limitations": "Our analysis relies primarily on the proposed Flip Rate metric, which captures changes in guardrail judgments between normal and RAG-style settings. The computation of Flip Rate doesn’t require labeling, providing a scalable robustness metric to better understand guardrails’ properties in addition to accuracy metric which requires annotator and is subjective. It could be easily integrated into any existing guardrail evaluation pipeline in production. However, robustness alone does not fully characterize safety performance. Future work could measure complementary metrics such as precision/recall against human labels or utility-safety trade-offs to provide a more holistic evaluation. In Sec. 5.3 we found that guardrail robustness varies with the LLM that generates candidate responses. A deeper investigation into how response features shape guardrail judgments could inform more resilient guardrail designs. Our exploration of high-reasoning-effort model and prompt engineering (Sec. 6 ), showed only limited improvements, suggesting that generic techniques provide only partial robustness. Future work should explore training-time interventions, hybrid symbolic–neural guardrails, and uncertainty-aware methods that explicitly detect contextual shifts. Our study covered five strong and popular guardrails, but this limited diversity leaves open the possibility that other guardrails are more robust. While we followed An et al. ( 2025 ) in using BM25, we observed that relevant documents amplify instability (Sec. 5.1.2 ). Different retrievers may yield different outcomes, calling for systematic evaluation across retriever quality and adversarial strength.",
            "8 Conclusion": "In this work, we demonstrated that LLM-based guardrails are vulnerable to contextual perturbations such as retrieval augmentation, leading to nontrivial rates of judgment flips. By systematically evaluating five guardrails across diverse settings, we revealed that once one enriches the context to the guardrail, even with only one benign and irrelevant document, the quality of this safety mechanism drops significantly. Our findings underscore an overlooked but critical limitation in current guardrails. We hope this study motivates deeper inquiry into guardrail robustness and inspires the development of safer, more reliable alignment techniques for real-world LLM system deployments.",
            "9 Ethics Statements": "This work aims to investigate the robustness of LLM-based guardrails under RAG-style contexts. While our findings reveal vulnerabilities that could be exploited to bypass existing safety mechanisms, the primary goal of this research is to strengthen evaluation practices and inform the design of more resilient guardrail models. We believe that disclosing these limitations contributes to the responsible development and deployment of LLM systems, ultimately advancing their safe and trustworthy use in real-world applications.",
            "10 Reproducibility statement": "We provide a detailed description of our experimental setup in Sections 4 , 5 , and 6 . Additional implementation details, including dataset statistics, are presented in Appendix B . The prompts used for input and output guardrails are listed in Appendix C , while the prompt for RAG response generation is provided in Appendix D . The enhanced guardrail prompts employed in Section 6.2 are included in Appendix E ."
        },
        "Tuples": [
            [
                "1 Introduction",
                "Large language models (LLMs) have rapidly become a central component of modern AI systems, powering applications from conversational assistants to code generation (Brown et al., 2020 ; Jiang et al., 2024 ) . Their ability to generalize across domains and tasks has made them widely adopted in real-world deployments (Hadi et al., 2023 ) . However, the same flexibility that enables their success also raises serious concerns about safety. LLMs are known to occasionally produce harmful, biased, or otherwise unsafe outputs, which poses significant risks when these models are used by millions of end users (Bai et al., 2022b ; Ganguli et al., 2022 ; Gallegos et al., 2024 ; She et al., 2025 ; Guo et al., 2025 ) . To mitigate such risks, the research community and industry have invested heavily in methods for aligning LLMs with human safety preferences. Two main strategies have emerged: direct safety fine-tuning of base models (Ouyang et al., 2022 ) and the use of external guardrails (Rebedea et al., 2023 ) . Guardrail models serve as dedicated safety filters layered on top of generation, offering flexibility and modularity. They can be updated independently of the base model, deployed as both input and output filters, and integrated into existing systems without retraining (Hurst et al., 2024 ) . Most guardrails are themselves LLM-based (e.g., Llama Guard (Inan et al., 2023 ) ). Leveraging the expressive power of LLMs allows guardrails to handle nuanced, context-dependent safety decisions. However, this also exposes them to the same vulnerabilities as the models that they are meant to protect. Prior work (Liu et al., 2024 ) has shown that LLMs are sensitive to the information contained in their context, and even benign additions can cause shifts in their behavior. For example, a recent work (An et al., 2025 ) suggests that Retrieval-Augmented Generation (RAG) may also increase the risk of unsafe or malicious generations, since safety alignment methods such as RLHF are typically applied in non-RAG settings. This raises an important but underexplored question: do LLM-based guardrails, when provided with richer contexts, alter their safety judgments? To address this question, we take RAG as a case study to investigate the robustness of guardrails under such conditions, since RAG is a widely adopted paradigm for improving the factuality and relevance of LLM outputs (Gao et al., 2023 ) . We consider two settings as shown in Fig. 1 : (1) normal RAG setting, where guardrails check user query or the query-response pair without exposure to retrieved documents, (2) RAG-style query setting, where the query has been augmented with retrieved documents and would be passed in guardrails as a whole. In this work, we conducted a systematic evaluation study of the robustness of LLM-based guardrails under RAG-style context . We introduce a novel implementation of robustness metric, Flip Rate . This metric measures the frequency with which guardrail judgments change between a vanilla and a RAG-augmented setting, and can be computed without ground-truth labeling. Using Flip Rate , we comprehensively evaluated three Llama Guard models and two GPT-oss models. We posed the following three research questions: RQ1: How does RAG-style context affect LLM-based guardrail judgment? We assessed 5 popular LLM-based guardrails on over 6,000 harmful queries and the responses to them generated by 8 LLMs with non-RAG-style and RAG-style context separately. We found that RAG-style context leads the guardrails to flip their judgments in both input guardrail and output guardrail settings. For example, well-aligned models like GPT-oss-20B give opposite judgments in around 15.0 % 15.0\\% cases when used as an output guardrail. RQ2: How does each component of RAG-style context affect the robustness of guardrails? We isolated each component of RAG-style context and examined its individual effect on robustness. Our results show that (1) the relevance between retrieved documents and user query exacerbates the vulnerability, while the number of documents have minor effects, (2) guardrails flip safety judgments due to context shifts, regardless of whether the query is safe or unsafe, (3) responses generated by different LLMs affects the guardrail differently. RQ3: Can general LLM enhancements mitigate this safety concern? We explored two potential general-purpose mitigations: high-reasoning-effort mode, and RAG-style-context-aware prompting. Both improved robustness by lowering flip rate, but neither solved the issue completely, highlighting the need for future research on guardrail techniques specifically tailored to RAG-style contexts."
            ],
            [
                "2 Related Works",
                "Guardrail models. LLMs have become increasingly powerful and widely deployed, but their open-ended generation abilities also introduce new safety challenges. To mitigate these risks, guardrails, the external defense layers that monitor and control LLM interactions, have emerged as a crucial solution (Inan et al., 2023 ; Markov et al., 2023 ; Wang et al., 2024 ; Han et al., 2024 ; Kang & Li, 2025 ; Ghosh et al., 2024 ) . These mechanisms offer a distinct advantage over internal alignment techniques like RLHF (Ouyang et al., 2022 ; Bai et al., 2022a ) by effectively filtering malicious inputs and outputs without compromising the core integrity of the base LLM (Dong et al., 2024 ) . Existing guardrail evaluations focus on plain inputs or output checks (Mazeika et al., 2024 ; Zou et al., 2023 ; Radharapu et al., 2023 ; Bhardwaj & Poria, 2023 ; Shaikh et al., 2023 ; Bhardwaj et al., 2024 ; Deng et al., 2023 ; Bassani & Sanchez, 2024 ; Lin et al., 2023 ) , while our study targets a blind spot where the content under classification contains retrieved documents (RAG-style context). Safety of RAG. RAG introduces unique security challenges beyond vanilla LLM generation, as the integration of external knowledge corpora creates novel attack surfaces. A growing body of work demonstrates that adversaries can poison indices, implant backdoors, or craft retrieval-optimized injections that steer models toward unsafe behavior (Xue et al., 2024 ; Zou et al., 2025 ; Cheng et al., 2024 ) . Liang et al. ( 2025 ) and Ni et al. ( 2025 ) conducted benchmarks and surveys to further catalog these threats and showed that vulnerabilities span indexing, retrieval, filtering, and generation stages. Furthermore, beyond malicious content, the inherent properties of the corpus can lead to unwanted responses in other ways. For example, Wu et al. ( 2025 ) showed that demographic biases present in the retrieval data can persist or even be amplified by the RAG pipeline, while Zeng et al. ( 2024 ) found that RAG can leak proprietary retrieval database. Our work examines how benign context shifts affect guardrails, diverging from the poisoned-corpus threat model. This parallels An et al. ( 2025 ) , which provided the first comprehensive analysis of RAG’s impact on LLM safety. They found that incorporating retrieval often makes LLM less safe and alters its safety profile even if the RAG corpus is secured. However, their evaluation focused exclusively on safety-aligned LLMs, without considering external guardrail models. In contrast, our work centered on evaluating guardrail models in the RAG settings."
            ],
            [
                "3 Problem Setup and Robustness Metric",
                "In this section, we formalize the concepts underlying our study and introduce the robustness metric that we will use throughout the experiments."
            ],
            [
                "4 RQ1: How does RAG-style context affect guardrail judgment?",
                "We first investigate whether RAG-style context perturbs the safety judgments of guardrail models."
            ],
            [
                "5 RQ2: How does each component of RAG-style context affect the robustness of guardrails?",
                "In this section, we isolate each component of the RAG-style context and examine its individual effect on robustness. A RAG-style context consists of (i) retrieved documents, (ii) the user query, and (iii) the LLM-generated responses (for output guardrails only). We discuss them one by one."
            ],
            [
                "6 RQ3: Can general LLM enhancements mitigate this?",
                "Having established that guardrails are susceptible to perturbations from RAG-style context, we next investigate whether general LLM enhancement techniques can alleviate this issue. Our goal here is not to provide a comprehensive solution, but rather to conduct a preliminary exploration of whether such general-purpose methods can reduce the observed vulnerability and to identify potential directions for future research. We focus on two representative strategies: (1) employing models with high reasoning effort, and (2) modifying prompts to explicitly account for retrieved documents in the context."
            ],
            [
                "7 Discussion and Limitations",
                "Our analysis relies primarily on the proposed Flip Rate metric, which captures changes in guardrail judgments between normal and RAG-style settings. The computation of Flip Rate doesn’t require labeling, providing a scalable robustness metric to better understand guardrails’ properties in addition to accuracy metric which requires annotator and is subjective. It could be easily integrated into any existing guardrail evaluation pipeline in production. However, robustness alone does not fully characterize safety performance. Future work could measure complementary metrics such as precision/recall against human labels or utility-safety trade-offs to provide a more holistic evaluation. In Sec. 5.3 we found that guardrail robustness varies with the LLM that generates candidate responses. A deeper investigation into how response features shape guardrail judgments could inform more resilient guardrail designs. Our exploration of high-reasoning-effort model and prompt engineering (Sec. 6 ), showed only limited improvements, suggesting that generic techniques provide only partial robustness. Future work should explore training-time interventions, hybrid symbolic–neural guardrails, and uncertainty-aware methods that explicitly detect contextual shifts. Our study covered five strong and popular guardrails, but this limited diversity leaves open the possibility that other guardrails are more robust. While we followed An et al. ( 2025 ) in using BM25, we observed that relevant documents amplify instability (Sec. 5.1.2 ). Different retrievers may yield different outcomes, calling for systematic evaluation across retriever quality and adversarial strength."
            ],
            [
                "8 Conclusion",
                "In this work, we demonstrated that LLM-based guardrails are vulnerable to contextual perturbations such as retrieval augmentation, leading to nontrivial rates of judgment flips. By systematically evaluating five guardrails across diverse settings, we revealed that once one enriches the context to the guardrail, even with only one benign and irrelevant document, the quality of this safety mechanism drops significantly. Our findings underscore an overlooked but critical limitation in current guardrails. We hope this study motivates deeper inquiry into guardrail robustness and inspires the development of safer, more reliable alignment techniques for real-world LLM system deployments."
            ],
            [
                "9 Ethics Statements",
                "This work aims to investigate the robustness of LLM-based guardrails under RAG-style contexts. While our findings reveal vulnerabilities that could be exploited to bypass existing safety mechanisms, the primary goal of this research is to strengthen evaluation practices and inform the design of more resilient guardrail models. We believe that disclosing these limitations contributes to the responsible development and deployment of LLM systems, ultimately advancing their safe and trustworthy use in real-world applications."
            ],
            [
                "10 Reproducibility statement",
                "We provide a detailed description of our experimental setup in Sections 4 , 5 , and 6 . Additional implementation details, including dataset statistics, are presented in Appendix B . The prompts used for input and output guardrails are listed in Appendix C , while the prompt for RAG response generation is provided in Appendix D . The enhanced guardrail prompts employed in Section 6.2 are included in Appendix E ."
            ]
        ],
        "section_summaries": [
            [
                "1 Introduction",
                "Large language models (LLMs) have rapidly become a central component of modern AI systems, powering applications from conversational assistants to code generation (Brown et al., 2020 ; Jiang et al., 2024 ) . Their ability to generalize across domains and tasks has made them widely adopted in real-world deployments (Hadi et al., 2023 ) . However, the same flexibility that enables their success also raises serious concerns about safety. LLMs are known to occasionally produce harmful, biased, or otherwise unsafe outputs, which poses significant risks when these models are used by millions of end users (Bai et al., 2022b ; Ganguli et al., 2022 ; Gallegos et al., 2024 ; She et al., 2025 ; Guo et al., 2025 ) . To mitigate such risks, the research community and industry have invested heavily in methods for aligning LLMs with human safety preferences. Two main strategies have emerged: direct safety fine-tuning of base models (Ouyang et al., 2022 ) and the use of external guardrails (Rebedea et al., 2023 ) . Guardrail models serve as dedicated safety filters layered on top of generation, offering flexibility and modularity. They can be updated independently of the base model, deployed as both input and output filters, and integrated into existing systems without retraining (Hurst et al., 2024 ) . Most guardrails are themselves LLM-based (e.g., Llama Guard (Inan et al., 2023 ) ). Leveraging the expressive power of LLMs allows guardrails to handle nuanced, context-dependent safety decisions. However, this also exposes them to the same vulnerabilities as the models that they are meant to protect. Prior work (Liu et al., 2024 ) has shown that LLMs are sensitive to the information contained in their context, and even benign additions can cause shifts in their behavior. For example, a recent work (An et al., 2025 ) suggests that Retrieval-Augmented Generation (RAG) may also increase the risk of unsafe or malicious generations, since safety alignment methods such as RLHF are typically applied in non-RAG settings. This raises an important but underexplored question: do LLM-based guardrails, when provided with richer contexts, alter their safety judgments? To address this question, we take RAG as a case study to investigate the robustness of guardrails under such conditions, since RAG is a widely adopted paradigm for improving the factuality and relevance of LLM outputs (Gao et al., 2023 ) . We consider two settings as shown in Fig. 1 : (1) normal RAG setting, where guardrails check user query or the query-response pair without exposure to retrieved documents, (2) RAG-style query setting, where the query has been augmented with retrieved documents and would be passed in guardrails as a whole. In this work, we conducted a systematic evaluation study of the robustness of LLM-based guardrails under RAG-style context . We introduce a novel implementation of robustness metric, Flip Rate . This metric measures the frequency with which guardrail judgments change between a vanilla and a RAG-augmented setting, and can be computed without ground-truth labeling. Using Flip Rate , we comprehensively evaluated three Llama Guard models and two GPT-oss models. We posed the following three research questions: RQ1: How does RAG-style context affect LLM-based guardrail judgment? We assessed 5 popular LLM-based guardrails on over 6,000 harmful queries and the responses to them generated by 8 LLMs with non-RAG-style and RAG-style context separately. We found that RAG-style context leads the guardrails to flip their judgments in both input guardrail and output guardrail settings. For example, well-aligned models like GPT-oss-20B give opposite judgments in around 15.0 % 15.0\\% cases when used as an output guardrail. RQ2: How does each component of RAG-style context affect the robustness of guardrails? We isolated each component of RAG-style context and examined its individual effect on robustness. Our results show that (1) the relevance between retrieved documents and user query exacerbates the vulnerability, while the number of documents have minor effects, (2) guardrails flip safety judgments due to context shifts, regardless of whether the query is safe or unsafe, (3) responses generated by different LLMs affects the guardrail differently. RQ3: Can general LLM enhancements mitigate this safety concern? We explored two potential general-purpose mitigations: high-reasoning-effort mode, and RAG-style-context-aware prompting. Both improved robustness by lowering flip rate, but neither solved the issue completely, highlighting the need for future research on guardrail techniques specifically tailored to RAG-style contexts.",
                "Brown et al. (2020), Jiang et al. (2024), Hadi et al. (2023), Bai et al. (2022b), Ganguli et al. (2022), Gallegos et al. (2024), She et al. (2025), Guo et al. (2025), Ouyang et al. (2022), Rebedea et al. (2023), Hurst et al. (2024), Inan et al. (2023), Liu et al. (2024), An et al. (2025), Gao et al. (2023) \nLarge language models are widely adopted but raise safety concerns, as they can produce harmful or biased outputs. To mitigate these risks, researchers use methods like direct safety fine-tuning and external guardrails, which can be updated independently of the base model. Guardrail models are themselves LLM-based and leverage their expressive power to handle nuanced safety decisions, but this also exposes them to vulnerabilities. A recent study suggests that Retrieval-Augmented Generation (RAG) may increase the risk of unsafe or malicious generations.\nA systematic evaluation study of the robustness of LLM-based guardrails under RAG-style context is conducted. The study introduces a novel implementation of robustness metric, Flip Rate, which measures the frequency with which guardrail judgments change between a vanilla and a RAG-augmented setting. Three Llama Guard models and two GPT-oss models are evaluated using Flip Rate.\nThe evaluation shows that RAG-style context leads the guardrails to flip their judgments in both input guardrail and output guardrail settings, with well-aligned models like GPT-oss-20B giving opposite judgments in around 15.0% cases when used as an output guardrail. The relevance between retrieved documents and user query exacerbates the vulnerability, while the number of documents has minor effects. Responses generated by different LLMs affect the guardrail differently.\nGeneral LLM enhancements such as high-reasoning-effort mode and RAG-style-context-aware prompting improve robustness but do not completely solve the issue, highlighting the need for future research on guardrail techniques specifically tailored to RAG-style contexts."
            ],
            [
                "2 Related Works",
                "Guardrail models. LLMs have become increasingly powerful and widely deployed, but their open-ended generation abilities also introduce new safety challenges. To mitigate these risks, guardrails, the external defense layers that monitor and control LLM interactions, have emerged as a crucial solution (Inan et al., 2023 ; Markov et al., 2023 ; Wang et al., 2024 ; Han et al., 2024 ; Kang & Li, 2025 ; Ghosh et al., 2024 ) . These mechanisms offer a distinct advantage over internal alignment techniques like RLHF (Ouyang et al., 2022 ; Bai et al., 2022a ) by effectively filtering malicious inputs and outputs without compromising the core integrity of the base LLM (Dong et al., 2024 ) . Existing guardrail evaluations focus on plain inputs or output checks (Mazeika et al., 2024 ; Zou et al., 2023 ; Radharapu et al., 2023 ; Bhardwaj & Poria, 2023 ; Shaikh et al., 2023 ; Bhardwaj et al., 2024 ; Deng et al., 2023 ; Bassani & Sanchez, 2024 ; Lin et al., 2023 ) , while our study targets a blind spot where the content under classification contains retrieved documents (RAG-style context). Safety of RAG. RAG introduces unique security challenges beyond vanilla LLM generation, as the integration of external knowledge corpora creates novel attack surfaces. A growing body of work demonstrates that adversaries can poison indices, implant backdoors, or craft retrieval-optimized injections that steer models toward unsafe behavior (Xue et al., 2024 ; Zou et al., 2025 ; Cheng et al., 2024 ) . Liang et al. ( 2025 ) and Ni et al. ( 2025 ) conducted benchmarks and surveys to further catalog these threats and showed that vulnerabilities span indexing, retrieval, filtering, and generation stages. Furthermore, beyond malicious content, the inherent properties of the corpus can lead to unwanted responses in other ways. For example, Wu et al. ( 2025 ) showed that demographic biases present in the retrieval data can persist or even be amplified by the RAG pipeline, while Zeng et al. ( 2024 ) found that RAG can leak proprietary retrieval database. Our work examines how benign context shifts affect guardrails, diverging from the poisoned-corpus threat model. This parallels An et al. ( 2025 ) , which provided the first comprehensive analysis of RAG’s impact on LLM safety. They found that incorporating retrieval often makes LLM less safe and alters its safety profile even if the RAG corpus is secured. However, their evaluation focused exclusively on safety-aligned LLMs, without considering external guardrail models. In contrast, our work centered on evaluating guardrail models in the RAG settings.",
                "Inan et al., 2023; Markov et al., 2023; Wang et al., 2024; Han et al., 2024; Kang & Li, 2025; Ghosh et al., 2024 introduce guardrails to mitigate LLM safety risks. Existing evaluations focus on plain inputs and outputs. Our study targets RAG-style context where content under classification contains retrieved documents. A growing body of work demonstrates that adversaries can poison indices, implant backdoors or craft retrieval-optimized injections in RAG. Liang et al., 2025 and Ni et al., 2025 conducted benchmarks and surveys to further catalog these threats. Wu et al., 2025 showed that demographic biases present in the retrieval data can persist or be amplified by RAG. Zeng et al., 2024 found that RAG can leak proprietary retrieval database. Our work examines how benign context shifts affect guardrails, diverging from the poisoned-corpus threat model."
            ],
            [
                "3 Problem Setup and Robustness Metric",
                "In this section, we formalize the concepts underlying our study and introduce the robustness metric that we will use throughout the experiments.",
                "We formalize concepts underlying our study. \n\n(Note: There are no numbers, dates, names, abbreviations mentioned in this paragraph)"
            ],
            [
                "4 RQ1: How does RAG-style context affect guardrail judgment?",
                "We first investigate whether RAG-style context perturbs the safety judgments of guardrail models.",
                "RAG-style context affects safety judgments of guardrail models."
            ],
            [
                "5 RQ2: How does each component of RAG-style context affect the robustness of guardrails?",
                "In this section, we isolate each component of the RAG-style context and examine its individual effect on robustness. A RAG-style context consists of (i) retrieved documents, (ii) the user query, and (iii) the LLM-generated responses (for output guardrails only). We discuss them one by one.",
                "A RAG-style context consists of: \n(i) retrieved documents, \n(ii) user query, \nand (iii) LLM-generated responses."
            ],
            [
                "6 RQ3: Can general LLM enhancements mitigate this?",
                "Having established that guardrails are susceptible to perturbations from RAG-style context, we next investigate whether general LLM enhancement techniques can alleviate this issue. Our goal here is not to provide a comprehensive solution, but rather to conduct a preliminary exploration of whether such general-purpose methods can reduce the observed vulnerability and to identify potential directions for future research. We focus on two representative strategies: (1) employing models with high reasoning effort, and (2) modifying prompts to explicitly account for retrieved documents in the context.",
                "RAG-style context perturbs guardrails; LLM enhancements (high reasoning effort, modified prompts) explored."
            ],
            [
                "7 Discussion and Limitations",
                "Our analysis relies primarily on the proposed Flip Rate metric, which captures changes in guardrail judgments between normal and RAG-style settings. The computation of Flip Rate doesn’t require labeling, providing a scalable robustness metric to better understand guardrails’ properties in addition to accuracy metric which requires annotator and is subjective. It could be easily integrated into any existing guardrail evaluation pipeline in production. However, robustness alone does not fully characterize safety performance. Future work could measure complementary metrics such as precision/recall against human labels or utility-safety trade-offs to provide a more holistic evaluation. In Sec. 5.3 we found that guardrail robustness varies with the LLM that generates candidate responses. A deeper investigation into how response features shape guardrail judgments could inform more resilient guardrail designs. Our exploration of high-reasoning-effort model and prompt engineering (Sec. 6 ), showed only limited improvements, suggesting that generic techniques provide only partial robustness. Future work should explore training-time interventions, hybrid symbolic–neural guardrails, and uncertainty-aware methods that explicitly detect contextual shifts. Our study covered five strong and popular guardrails, but this limited diversity leaves open the possibility that other guardrails are more robust. While we followed An et al. ( 2025 ) in using BM25, we observed that relevant documents amplify instability (Sec. 5.1.2 ). Different retrievers may yield different outcomes, calling for systematic evaluation across retriever quality and adversarial strength.",
                "The analysis relies on Flip Rate metric to capture guardrail judgments. It's scalable, robust, and doesn't require labeling. However, it doesn't fully characterize safety performance. Future work could measure metrics like precision/recall or utility-safety trade-offs. \n\nFive popular guardrails were studied but limited diversity is noted. Different retrievers may yield different outcomes, requiring systematic evaluation.\n\nLLM response features shape guardrail judgments and training-time interventions are suggested for improvement."
            ],
            [
                "8 Conclusion",
                "In this work, we demonstrated that LLM-based guardrails are vulnerable to contextual perturbations such as retrieval augmentation, leading to nontrivial rates of judgment flips. By systematically evaluating five guardrails across diverse settings, we revealed that once one enriches the context to the guardrail, even with only one benign and irrelevant document, the quality of this safety mechanism drops significantly. Our findings underscore an overlooked but critical limitation in current guardrails. We hope this study motivates deeper inquiry into guardrail robustness and inspires the development of safer, more reliable alignment techniques for real-world LLM system deployments.",
                "LLM-based guardrails are vulnerable to contextual perturbations, leading to 5 guardrails failing in diverse settings. A single benign document can cause a significant drop in their quality."
            ],
            [
                "9 Ethics Statements",
                "This work aims to investigate the robustness of LLM-based guardrails under RAG-style contexts. While our findings reveal vulnerabilities that could be exploited to bypass existing safety mechanisms, the primary goal of this research is to strengthen evaluation practices and inform the design of more resilient guardrail models. We believe that disclosing these limitations contributes to the responsible development and deployment of LLM systems, ultimately advancing their safe and trustworthy use in real-world applications.",
                "LLM-based guardrails are vulnerable to bypass under RAG-style contexts, revealing weaknesses in safety mechanisms. The research aims to strengthen evaluation practices for more resilient models, promoting responsible development and deployment of LLM systems."
            ],
            [
                "10 Reproducibility statement",
                "We provide a detailed description of our experimental setup in Sections 4 , 5 , and 6 . Additional implementation details, including dataset statistics, are presented in Appendix B . The prompts used for input and output guardrails are listed in Appendix C , while the prompt for RAG response generation is provided in Appendix D . The enhanced guardrail prompts employed in Section 6.2 are included in Appendix E .",
                "Sections 4, 5, and 6 describe experimental setup; details in Appendices B-E."
            ]
        ],
        "general_summary": "**1. Research Overview**\n\nThis study investigates the robustness of large language model (LLM)-based guardrails under Retrieval-Augmentation Generation (RAG)-style contexts. Guardrails are used to mitigate LLM safety risks, but they are themselves vulnerable to data distribution shifts and contextual perturbations.\n\n**2. Methodology & Approach**\n\nThe authors conducted a systematic evaluation of 3 Llama Guards and 2 GPT-oss models using a novel robustness metric called Flip Rate, which measures the frequency with which guardrail judgments change between vanilla and RAG-augmented settings. The study examines how different components of the augmented context affect guardrail robustness.\n\n**3. Key Contributions & Findings**\n\nThe study found that inserting benign documents into the guardrail context alters judgments in around 11% and 8% of cases, making them unreliable. The two mitigation methods tested only brought minor improvements. The analysis suggests that current guardrails are vulnerable to contextual perturbations and requires more robust evaluation protocols.\n\n**4. Technical Details**\n\nThe study used a Flip Rate metric to evaluate the robustness of guardrails, which measures the frequency with which judgments change between vanilla and RAG-augmented settings. The authors tested general LLM enhancements such as high-reasoning-effort mode and RAG-style-context-aware prompting to improve robustness.\n\n**5. Limitations & Future Work**\n\nThe analysis relies on Flip Rate metric, which does not fully characterize safety performance. Future work could measure metrics like precision/recall or utility-safety trade-offs. The study also highlights the need for more systematic evaluation of guardrail techniques tailored to RAG-style contexts.\n\n**6. Practical Implications**\n\nThis research has implications for the development and deployment of LLM systems, highlighting the need for more robust evaluation protocols and training methods that can mitigate contextual perturbations. The findings suggest that current guardrails are vulnerable to bypass under RAG-style contexts, revealing weaknesses in safety mechanisms.\n\nThe authors provide detailed information on experimental setup in Sections 4-6 and Appendices B-E, allowing for reproducibility of the results."
    },
    {
        "id": "2510.07233v1",
        "title": "LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document\n  Understanding",
        "authors": [
            "Zhivar Sourati",
            "Zheng Wang",
            "Marianne Menglin Liu",
            "Yazhe Hu",
            "Mengqing Guo",
            "Sujeeth Bharadwaj",
            "Kyu Han",
            "Tao Sheng",
            "Sujith Ravi",
            "Morteza Dehghani",
            "Dan Roth"
        ],
        "url": "https://arxiv.org/abs/2510.07233v1",
        "Abstract": "Question answering over visually rich documents (VRDs) requires reasoning not\nonly over isolated content but also over documents' structural organization and\ncross-page dependencies. However, conventional retrieval-augmented generation\n(RAG) methods encode content in isolated chunks during ingestion, losing\nstructural and cross-page dependencies, and retrieve a fixed number of pages at\ninference, regardless of the specific demands of the question or context. This\noften results in incomplete evidence retrieval and degraded answer quality for\nmulti-page reasoning tasks. To address these limitations, we propose LAD-RAG, a\nnovel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs\na symbolic document graph that captures layout structure and cross-page\ndependencies, adding it alongside standard neural embeddings to yield a more\nholistic representation of the document. During inference, an LLM agent\ndynamically interacts with the neural and symbolic indices to adaptively\nretrieve the necessary evidence based on the query. Experiments on\nMMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG\nimproves retrieval, achieving over 90% perfect recall on average without any\ntop-k tuning, and outperforming baseline retrievers by up to 20% in recall at\ncomparable noise levels, yielding higher QA accuracy with minimal latency.",
        "Main": "",
        "Tuples": [
            [
                "Summary",
                "Question answering over visually rich documents (VRDs) requires reasoning not\nonly over isolated content but also over documents' structural organization and\ncross-page dependencies. However, conventional retrieval-augmented generation\n(RAG) methods encode content in isolated chunks during ingestion, losing\nstructural and cross-page dependencies, and retrieve a fixed number of pages at\ninference, regardless of the specific demands of the question or context. This\noften results in incomplete evidence retrieval and degraded answer quality for\nmulti-page reasoning tasks. To address these limitations, we propose LAD-RAG, a\nnovel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs\na symbolic document graph that captures layout structure and cross-page\ndependencies, adding it alongside standard neural embeddings to yield a more\nholistic representation of the document. During inference, an LLM agent\ndynamically interacts with the neural and symbolic indices to adaptively\nretrieve the necessary evidence based on the query. Experiments on\nMMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG\nimproves retrieval, achieving over 90% perfect recall on average without any\ntop-k tuning, and outperforming baseline retrievers by up to 20% in recall at\ncomparable noise levels, yielding higher QA accuracy with minimal latency."
            ]
        ],
        "section_summaries": [
            [
                "Summary",
                "Question answering over visually rich documents (VRDs) requires reasoning not\nonly over isolated content but also over documents' structural organization and\ncross-page dependencies. However, conventional retrieval-augmented generation\n(RAG) methods encode content in isolated chunks during ingestion, losing\nstructural and cross-page dependencies, and retrieve a fixed number of pages at\ninference, regardless of the specific demands of the question or context. This\noften results in incomplete evidence retrieval and degraded answer quality for\nmulti-page reasoning tasks. To address these limitations, we propose LAD-RAG, a\nnovel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs\na symbolic document graph that captures layout structure and cross-page\ndependencies, adding it alongside standard neural embeddings to yield a more\nholistic representation of the document. During inference, an LLM agent\ndynamically interacts with the neural and symbolic indices to adaptively\nretrieve the necessary evidence based on the query. Experiments on\nMMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG\nimproves retrieval, achieving over 90% perfect recall on average without any\ntop-k tuning, and outperforming baseline retrievers by up to 20% in recall at\ncomparable noise levels, yielding higher QA accuracy with minimal latency.",
                "LAD-RAG constructs a symbolic document graph during ingestion, capturing layout structure and cross-page dependencies. During inference, an LLM agent adaptsively retrieves evidence based on the query, improving retrieval by 20% in recall and achieving over 90% perfect recall without top-k tuning."
            ]
        ],
        "general_summary": "**Research Overview**\nThe research addresses the limitations of conventional retrieval-augmented generation (RAG) methods for question answering over visually rich documents (VRDs), which fail to capture structural organization and cross-page dependencies.\n\n**Methodology & Approach**\nLAD-RAG proposes a novel Layout-Aware Dynamic RAG framework that constructs a symbolic document graph during ingestion, capturing layout structure and cross-page dependencies. This graph is combined with standard neural embeddings to yield a more holistic representation of the document. During inference, an LLM agent dynamically interacts with the neural and symbolic indices to adaptively retrieve necessary evidence based on the query.\n\n**Key Contributions & Findings**\nLAD-RAG achieves over 90% perfect recall on average without any top-k tuning, outperforming baseline retrievers by up to 20% in recall at comparable noise levels. This improvement leads to higher QA accuracy with minimal latency. The framework's adaptive retrieval mechanism enables it to dynamically retrieve the necessary evidence based on the query.\n\n**Technical Details**\nThe symbolic document graph is constructed during ingestion using a combination of layout analysis and cross-page dependency modeling techniques. The LLM agent interacts with both neural and symbolic indices through a dynamic retrieval mechanism, enabling adaptive evidence retrieval.\n\n**Limitations & Future Work**\nCurrent limitations include the computational overhead of constructing the symbolic document graph and the potential for noise in the layout analysis process. Future work may focus on optimizing the construction process and improving the robustness of the layout analysis.\n\n**Practical Implications**\nLAD-RAG has the potential to improve question answering accuracy over visually rich documents by capturing structural organization and cross-page dependencies, enabling more accurate evidence retrieval and higher QA accuracy with minimal latency. This research can be applied in various fields that involve question answering over complex documents, such as law, finance, and medicine."
    },
    {
        "id": "2510.02549v1",
        "title": "Knowledge-Graph Based RAG System Evaluation Framework",
        "authors": [
            "Sicheng Dong",
            "Vahid Zolfaghari",
            "Nenad Petrovic",
            "Alois Knoll"
        ],
        "url": "https://arxiv.org/abs/2510.02549v1",
        "Abstract": "Abstract Large language models (LLMs) has become a significant research focus and is utilized in various fields, such as text generation and dialog systems. One of the most essential applications of LLM is Retrieval Augmented Generation (RAG), which greatly enhances generated content’s reliability and relevance. However, evaluating RAG systems remains a challenging task. Traditional evaluation metrics struggle to effectively capture the key features of modern LLM-generated content that often exhibits high fluency and naturalness. Inspired by the RAGAS tool, a well-known RAG evaluation framework, we extended this framework into a KG-based evaluation paradigm, enabling multi-hop reasoning and semantic community clustering to derive more comprehensive scoring metrics. By incorporating these comprehensive evaluation criteria, we gain a deeper understanding of RAG systems and a more nuanced perspective on their performance. To validate the effectiveness of our approach, we compare its performance with RAGAS scores and construct a human-annotated subset to assess the correlation between human judgments and automated metrics. In addition, we conduct targeted experiments to demonstrate that our KG-based evaluation method is more sensitive to subtle semantic differences in generated outputs. Finally, we discuss the key challenges in evaluating RAG systems and highlight potential directions for future research.",
        "Main": {
            "1 Introduction": "Large Language Models (LLMs) are one of the hottest research topics in artificial intelligence today, and they have proven to be extremely powerful in a variety of fields, including healthcare and education. [ 8 ] [ 27 ] Despite their strong performance, LLMs nevertheless have a number of serious drawbacks. For example, they frequently lack the knowledge required to respond to domain-specific queries. [ 23 ] Furthermore, LLM databases eventually become out of date and cannot address today’s issues. [ 4 ] Researchers have taken two primary approaches to solving these issues: fine-tuning the model using domain-specific data and connecting the model to additional external information sources . [ 20 ] Although fine-tuning is a straightforward and effective approach, it has some obvious drawbacks, such as the scarcity of high-quality domain data and the high computational cost of the training process. [ 10 ] As a result, the second strategy, known as the Retrieval Augmented Generation (RAG) system , is increasingly being used in research. By accessing external data sources, this approach can search for domain-specific data in real time without the need for extensive training. [ 16 ] In addition, RAG is also regarded an effective structure to solve the problem of the system to generate inaccurate or misleading information (hallucination). [ 19 ] A RAG system consists of two key components: a retriever and a generator . The retriever will fetch relevant information based on the given input, and the generator then utilizes the information from the retriever to produce the final output. [ 1 ] Although baseline RAG has demonstrated strong information retrieval capabilities in certain tasks, it still faces several key challenges, particularly its limited ability to integrate multiple information sources. When answering a question requires synthesizing information from multiple, distinct sources, baseline RAG often struggles to effectively combine these pieces into a coherent and accurate response. [ 5 ] To address this issue, recent research has explored the integration of Knowledge Graphs (KG) [ 9 ] with RAG, leading to the development of the GraphRAG architecture. [ 5 ] This approach leverages LLMs for entity recognition and relation extraction to construct KGs, which will then be integrated with graph machine learning metrics to capture a more completed structure of the retrieved information, resulting in high response quality and accuracy. Building on this foundation, the latest research proposed lightRAG , another KG-based RAG system. [ 7 ] It leverages KG to improve retriever capability and optimizes the overall system architecture to provide a more efficient and lighter generation of response. However, when deploying such systems in real-world applications, it becomes critical to understand the reliability and effectiveness of RAG systems. The recently widely adopted evaluation framework, RAGAS, leverages large language models and techniques such as atomic facts to provide a more comprehensive assessment. Although atomic facts are very effective, they still face challenges when dealing with complex documents or when finer-grained evaluation is required. Therefore, we use a knowledge graph here to enhance the evaluation capability in this aspect. In this experiment, inspired by RAGAS , we extended this to a KG-based approach, aiming to provide a more precise evaluation system capable of handling complex, multi-fact relationships.",
            "2 Related Work": "RAG systems have attracted widespread attention across various fields, as researchers use them to enhance models’ ability to leverage external knowledge. However, when it comes to dynamic knowledge and other complex structures, evaluating these systems has remained a major research challenge. The whole evaluation process not only includes assessing the quality of the final generated output but also analyzing the retriever’s ability to fetch relevant information and examine the interaction between the retriever and generator components.Traditional evaluation methods, such as word-overlap-based metrics (e.g., BLEU [ 17 ] , ROUGE [ 21 ] ) or pre-trained model-based methods (e.g., BERTScore [ 26 ] ), struggle to effectively capture the semantic richness of modern LLM-generated text and give a perfect evaluation. Therefore, researchers have begun to focus on LLMs as evaluators for assessing RAG systems. For example, Li et al. (2025) define scoring bias and illustrate how perturbations in prompts or answer templates affect judgments. [ 12 ] Shi et al. (2024) specifically study position bias in pairwise comparisons conducted by LLM judges. [ 22 ] Moreover, Li et al. (2025) show that LLM judges are less stable when encountering adversarial manipulations and prompt sensitivities. [ 13 ] Compared to traditional evaluation methods, this kind of LLM-driven approach demonstrates great advantages in both efficiency and accuracy since most of the work can be done by LLMs themselves, reducing manual intervention and enhancing sensitivity to linguistic nuances. [ 28 ] Several well-known evaluation frameworks, such as RAGAS [ 6 ] , have already achieved significant progress in this field. These frameworks implement diverse evaluation metrics. Besides traditional metrics, it also leverages LLMs as evaluators to systematically assess RAG systems. The framework defines a wide range of metrics, some of which are outlined below [ 6 ] : Factual Correctness compares how factually accurate the generated response is compared with the reference. Faithfulness measures the consistency between a response and the retrieved context. Answer Relevancy evaluates how relevant the generated response is to the user input. Context Relevancy evaluates how pertinent the retrieved context is to the user input. Among the methods used in RAGAS are techniques such as splitting sentences into atomic statements and employing embedding models to compute similarity values. The idea behind the scene is is the use of atomic facts . We decompose the original sentence below as an example: “Theron Shan is a man who has given over his life in service to the Republic, using work to try and cope with abandonment issues gained from being hurt too many times by those who were supposed to love him.” can be separated into: • Theron Shan is a man. • He has devoted his life to serving the Republic. • He uses work to cope with abandonment issues. • These abandonment issues stem from being repeatedly hurt by those who were supposed to love him. The definition of atomic facts states that they are the smallest units of information that can stand alone and be evaluated independently. [ 11 ] By segmenting a passage into distinct atomic facts, we can better understand its central meaning. Particularly in question answering and RAG evaluation, methodologies based on atomic facts have achieved significant success. [ 6 ] [ 11 ] [ 15 ] Although atomic facts have been proven highly promising for evaluation, they still face challenges when dealing with complex contexts or long contexts. [ 15 ] Researchers have therefore turned their focus to knowledge graphs, attempting to use graph algorithms to structure and operationalize resources and improve the results. For example, Yan et al. proved that knowledge graph is useful for Atomic Fact Decomposition-based problem. [ 25 ] Li et al. also propose KELDaR framework to enhance atomic facts-based ability by knowledge graph. [ 14 ]",
            "3 Environment Setup": "In this section, we will discuss in detail the specific implementation steps of our experiment environment.",
            "4 Methodology": "The evaluator LLM utilized in the research below is GPT-4o-mini and the embedding model utilized for semantic similarity is all-MiniLM-L6-v2 . Building upon the RAGAS, we introduce a KG-based approach that enables deeper multi-hop reasoning. The KG-based evaluation metrics we adopt are context-agnostic , meaning they can be flexibly applied to various combinations of input components without being tied to a specific retrieval-generation pipeline. Specifically, the following input pairs can be evaluated: Context Relevancy, Factual Correctness, Faithfulness and Answer Relevancy. In the following, we describe the evaluation steps under the assumption that we are calculating context relevancy —i.e., measuring the semantic alignment between the input question and the retrieved context. The whole evaluation process can be separated into three stages: we first introduces the construction of the knowledge graph (Section 4.1 ), and then presents two algorithms implemented on top of the KG (Sections 4.2 and 4.3 ).",
            "6 Limitations": "A major limitation of our evaluation system lies in its scalability. The core bottleneck is the high computational cost of graph construction. In particular, when the input context is large, the time required to build the graph grows significantly, which hinders efficiency and makes scaling to real-world settings challenging.",
            "7 Conclusion and future scope": "This paper proposes an LLM-driven KG-based approach for evaluating RAG systems. By leveraging an LLM as an evaluator and defining multi-dimensional metrics, we conduct an efficient and accurate assessment of RAG systems.We evaluate two KG-based subscores, Multi-Hop Semantic Matching and Community-Based Semantic Overlap , which show moderate-to-high correlation with both human annotations and RAGAS. They complement each other across different metrics, and exhibit higher sensitivity when contrasting highly or non-relevant inputs. Currently, we only focus on the similarity between individual entities. Valuable research directions can be to investigate how to extend the similarity to triplet level and how to find a well-defined hyperparameter to gain a more fine-grained evaluation. Other metrics, such as negative rejection and long-context accuracy, also worth thorough exploration. [ b27 ] [ b28 ] {credits}"
        },
        "Tuples": [
            [
                "1 Introduction",
                "Large Language Models (LLMs) are one of the hottest research topics in artificial intelligence today, and they have proven to be extremely powerful in a variety of fields, including healthcare and education. [ 8 ] [ 27 ] Despite their strong performance, LLMs nevertheless have a number of serious drawbacks. For example, they frequently lack the knowledge required to respond to domain-specific queries. [ 23 ] Furthermore, LLM databases eventually become out of date and cannot address today’s issues. [ 4 ] Researchers have taken two primary approaches to solving these issues: fine-tuning the model using domain-specific data and connecting the model to additional external information sources . [ 20 ] Although fine-tuning is a straightforward and effective approach, it has some obvious drawbacks, such as the scarcity of high-quality domain data and the high computational cost of the training process. [ 10 ] As a result, the second strategy, known as the Retrieval Augmented Generation (RAG) system , is increasingly being used in research. By accessing external data sources, this approach can search for domain-specific data in real time without the need for extensive training. [ 16 ] In addition, RAG is also regarded an effective structure to solve the problem of the system to generate inaccurate or misleading information (hallucination). [ 19 ] A RAG system consists of two key components: a retriever and a generator . The retriever will fetch relevant information based on the given input, and the generator then utilizes the information from the retriever to produce the final output. [ 1 ] Although baseline RAG has demonstrated strong information retrieval capabilities in certain tasks, it still faces several key challenges, particularly its limited ability to integrate multiple information sources. When answering a question requires synthesizing information from multiple, distinct sources, baseline RAG often struggles to effectively combine these pieces into a coherent and accurate response. [ 5 ] To address this issue, recent research has explored the integration of Knowledge Graphs (KG) [ 9 ] with RAG, leading to the development of the GraphRAG architecture. [ 5 ] This approach leverages LLMs for entity recognition and relation extraction to construct KGs, which will then be integrated with graph machine learning metrics to capture a more completed structure of the retrieved information, resulting in high response quality and accuracy. Building on this foundation, the latest research proposed lightRAG , another KG-based RAG system. [ 7 ] It leverages KG to improve retriever capability and optimizes the overall system architecture to provide a more efficient and lighter generation of response. However, when deploying such systems in real-world applications, it becomes critical to understand the reliability and effectiveness of RAG systems. The recently widely adopted evaluation framework, RAGAS, leverages large language models and techniques such as atomic facts to provide a more comprehensive assessment. Although atomic facts are very effective, they still face challenges when dealing with complex documents or when finer-grained evaluation is required. Therefore, we use a knowledge graph here to enhance the evaluation capability in this aspect. In this experiment, inspired by RAGAS , we extended this to a KG-based approach, aiming to provide a more precise evaluation system capable of handling complex, multi-fact relationships."
            ],
            [
                "2 Related Work",
                "RAG systems have attracted widespread attention across various fields, as researchers use them to enhance models’ ability to leverage external knowledge. However, when it comes to dynamic knowledge and other complex structures, evaluating these systems has remained a major research challenge. The whole evaluation process not only includes assessing the quality of the final generated output but also analyzing the retriever’s ability to fetch relevant information and examine the interaction between the retriever and generator components.Traditional evaluation methods, such as word-overlap-based metrics (e.g., BLEU [ 17 ] , ROUGE [ 21 ] ) or pre-trained model-based methods (e.g., BERTScore [ 26 ] ), struggle to effectively capture the semantic richness of modern LLM-generated text and give a perfect evaluation. Therefore, researchers have begun to focus on LLMs as evaluators for assessing RAG systems. For example, Li et al. (2025) define scoring bias and illustrate how perturbations in prompts or answer templates affect judgments. [ 12 ] Shi et al. (2024) specifically study position bias in pairwise comparisons conducted by LLM judges. [ 22 ] Moreover, Li et al. (2025) show that LLM judges are less stable when encountering adversarial manipulations and prompt sensitivities. [ 13 ] Compared to traditional evaluation methods, this kind of LLM-driven approach demonstrates great advantages in both efficiency and accuracy since most of the work can be done by LLMs themselves, reducing manual intervention and enhancing sensitivity to linguistic nuances. [ 28 ] Several well-known evaluation frameworks, such as RAGAS [ 6 ] , have already achieved significant progress in this field. These frameworks implement diverse evaluation metrics. Besides traditional metrics, it also leverages LLMs as evaluators to systematically assess RAG systems. The framework defines a wide range of metrics, some of which are outlined below [ 6 ] : Factual Correctness compares how factually accurate the generated response is compared with the reference. Faithfulness measures the consistency between a response and the retrieved context. Answer Relevancy evaluates how relevant the generated response is to the user input. Context Relevancy evaluates how pertinent the retrieved context is to the user input. Among the methods used in RAGAS are techniques such as splitting sentences into atomic statements and employing embedding models to compute similarity values. The idea behind the scene is is the use of atomic facts . We decompose the original sentence below as an example: “Theron Shan is a man who has given over his life in service to the Republic, using work to try and cope with abandonment issues gained from being hurt too many times by those who were supposed to love him.” can be separated into: • Theron Shan is a man. • He has devoted his life to serving the Republic. • He uses work to cope with abandonment issues. • These abandonment issues stem from being repeatedly hurt by those who were supposed to love him. The definition of atomic facts states that they are the smallest units of information that can stand alone and be evaluated independently. [ 11 ] By segmenting a passage into distinct atomic facts, we can better understand its central meaning. Particularly in question answering and RAG evaluation, methodologies based on atomic facts have achieved significant success. [ 6 ] [ 11 ] [ 15 ] Although atomic facts have been proven highly promising for evaluation, they still face challenges when dealing with complex contexts or long contexts. [ 15 ] Researchers have therefore turned their focus to knowledge graphs, attempting to use graph algorithms to structure and operationalize resources and improve the results. For example, Yan et al. proved that knowledge graph is useful for Atomic Fact Decomposition-based problem. [ 25 ] Li et al. also propose KELDaR framework to enhance atomic facts-based ability by knowledge graph. [ 14 ]"
            ],
            [
                "3 Environment Setup",
                "In this section, we will discuss in detail the specific implementation steps of our experiment environment."
            ],
            [
                "4 Methodology",
                "The evaluator LLM utilized in the research below is GPT-4o-mini and the embedding model utilized for semantic similarity is all-MiniLM-L6-v2 . Building upon the RAGAS, we introduce a KG-based approach that enables deeper multi-hop reasoning. The KG-based evaluation metrics we adopt are context-agnostic , meaning they can be flexibly applied to various combinations of input components without being tied to a specific retrieval-generation pipeline. Specifically, the following input pairs can be evaluated: Context Relevancy, Factual Correctness, Faithfulness and Answer Relevancy. In the following, we describe the evaluation steps under the assumption that we are calculating context relevancy —i.e., measuring the semantic alignment between the input question and the retrieved context. The whole evaluation process can be separated into three stages: we first introduces the construction of the knowledge graph (Section 4.1 ), and then presents two algorithms implemented on top of the KG (Sections 4.2 and 4.3 )."
            ],
            [
                "6 Limitations",
                "A major limitation of our evaluation system lies in its scalability. The core bottleneck is the high computational cost of graph construction. In particular, when the input context is large, the time required to build the graph grows significantly, which hinders efficiency and makes scaling to real-world settings challenging."
            ],
            [
                "7 Conclusion and future scope",
                "This paper proposes an LLM-driven KG-based approach for evaluating RAG systems. By leveraging an LLM as an evaluator and defining multi-dimensional metrics, we conduct an efficient and accurate assessment of RAG systems.We evaluate two KG-based subscores, Multi-Hop Semantic Matching and Community-Based Semantic Overlap , which show moderate-to-high correlation with both human annotations and RAGAS. They complement each other across different metrics, and exhibit higher sensitivity when contrasting highly or non-relevant inputs. Currently, we only focus on the similarity between individual entities. Valuable research directions can be to investigate how to extend the similarity to triplet level and how to find a well-defined hyperparameter to gain a more fine-grained evaluation. Other metrics, such as negative rejection and long-context accuracy, also worth thorough exploration. [ b27 ] [ b28 ] {credits}"
            ]
        ],
        "section_summaries": [
            [
                "1 Introduction",
                "Large Language Models (LLMs) are one of the hottest research topics in artificial intelligence today, and they have proven to be extremely powerful in a variety of fields, including healthcare and education. [ 8 ] [ 27 ] Despite their strong performance, LLMs nevertheless have a number of serious drawbacks. For example, they frequently lack the knowledge required to respond to domain-specific queries. [ 23 ] Furthermore, LLM databases eventually become out of date and cannot address today’s issues. [ 4 ] Researchers have taken two primary approaches to solving these issues: fine-tuning the model using domain-specific data and connecting the model to additional external information sources . [ 20 ] Although fine-tuning is a straightforward and effective approach, it has some obvious drawbacks, such as the scarcity of high-quality domain data and the high computational cost of the training process. [ 10 ] As a result, the second strategy, known as the Retrieval Augmented Generation (RAG) system , is increasingly being used in research. By accessing external data sources, this approach can search for domain-specific data in real time without the need for extensive training. [ 16 ] In addition, RAG is also regarded an effective structure to solve the problem of the system to generate inaccurate or misleading information (hallucination). [ 19 ] A RAG system consists of two key components: a retriever and a generator . The retriever will fetch relevant information based on the given input, and the generator then utilizes the information from the retriever to produce the final output. [ 1 ] Although baseline RAG has demonstrated strong information retrieval capabilities in certain tasks, it still faces several key challenges, particularly its limited ability to integrate multiple information sources. When answering a question requires synthesizing information from multiple, distinct sources, baseline RAG often struggles to effectively combine these pieces into a coherent and accurate response. [ 5 ] To address this issue, recent research has explored the integration of Knowledge Graphs (KG) [ 9 ] with RAG, leading to the development of the GraphRAG architecture. [ 5 ] This approach leverages LLMs for entity recognition and relation extraction to construct KGs, which will then be integrated with graph machine learning metrics to capture a more completed structure of the retrieved information, resulting in high response quality and accuracy. Building on this foundation, the latest research proposed lightRAG , another KG-based RAG system. [ 7 ] It leverages KG to improve retriever capability and optimizes the overall system architecture to provide a more efficient and lighter generation of response. However, when deploying such systems in real-world applications, it becomes critical to understand the reliability and effectiveness of RAG systems. The recently widely adopted evaluation framework, RAGAS, leverages large language models and techniques such as atomic facts to provide a more comprehensive assessment. Although atomic facts are very effective, they still face challenges when dealing with complex documents or when finer-grained evaluation is required. Therefore, we use a knowledge graph here to enhance the evaluation capability in this aspect. In this experiment, inspired by RAGAS , we extended this to a KG-based approach, aiming to provide a more precise evaluation system capable of handling complex, multi-fact relationships.",
                "Large Language Models (LLMs) have limitations such as lacking domain-specific knowledge and outdated databases. Researchers use two approaches: fine-tuning models with domain data or connecting models to external sources via Retrieval Augmented Generation (RAG) systems. RAG consists of a retriever and generator, but faces challenges integrating multiple information sources. The GraphRAG architecture integrates Knowledge Graphs (KG) with RAG, improving response quality and accuracy. The lightRAG system also leverages KG for improved retriever capability. A new evaluation framework, inspired by RAGAS, uses a KG-based approach to provide more precise evaluations of complex relationships."
            ],
            [
                "2 Related Work",
                "RAG systems have attracted widespread attention across various fields, as researchers use them to enhance models’ ability to leverage external knowledge. However, when it comes to dynamic knowledge and other complex structures, evaluating these systems has remained a major research challenge. The whole evaluation process not only includes assessing the quality of the final generated output but also analyzing the retriever’s ability to fetch relevant information and examine the interaction between the retriever and generator components.Traditional evaluation methods, such as word-overlap-based metrics (e.g., BLEU [ 17 ] , ROUGE [ 21 ] ) or pre-trained model-based methods (e.g., BERTScore [ 26 ] ), struggle to effectively capture the semantic richness of modern LLM-generated text and give a perfect evaluation. Therefore, researchers have begun to focus on LLMs as evaluators for assessing RAG systems. For example, Li et al. (2025) define scoring bias and illustrate how perturbations in prompts or answer templates affect judgments. [ 12 ] Shi et al. (2024) specifically study position bias in pairwise comparisons conducted by LLM judges. [ 22 ] Moreover, Li et al. (2025) show that LLM judges are less stable when encountering adversarial manipulations and prompt sensitivities. [ 13 ] Compared to traditional evaluation methods, this kind of LLM-driven approach demonstrates great advantages in both efficiency and accuracy since most of the work can be done by LLMs themselves, reducing manual intervention and enhancing sensitivity to linguistic nuances. [ 28 ] Several well-known evaluation frameworks, such as RAGAS [ 6 ] , have already achieved significant progress in this field. These frameworks implement diverse evaluation metrics. Besides traditional metrics, it also leverages LLMs as evaluators to systematically assess RAG systems. The framework defines a wide range of metrics, some of which are outlined below [ 6 ] : Factual Correctness compares how factually accurate the generated response is compared with the reference. Faithfulness measures the consistency between a response and the retrieved context. Answer Relevancy evaluates how relevant the generated response is to the user input. Context Relevancy evaluates how pertinent the retrieved context is to the user input. Among the methods used in RAGAS are techniques such as splitting sentences into atomic statements and employing embedding models to compute similarity values. The idea behind the scene is is the use of atomic facts . We decompose the original sentence below as an example: “Theron Shan is a man who has given over his life in service to the Republic, using work to try and cope with abandonment issues gained from being hurt too many times by those who were supposed to love him.” can be separated into: • Theron Shan is a man. • He has devoted his life to serving the Republic. • He uses work to cope with abandonment issues. • These abandonment issues stem from being repeatedly hurt by those who were supposed to love him. The definition of atomic facts states that they are the smallest units of information that can stand alone and be evaluated independently. [ 11 ] By segmenting a passage into distinct atomic facts, we can better understand its central meaning. Particularly in question answering and RAG evaluation, methodologies based on atomic facts have achieved significant success. [ 6 ] [ 11 ] [ 15 ] Although atomic facts have been proven highly promising for evaluation, they still face challenges when dealing with complex contexts or long contexts. [ 15 ] Researchers have therefore turned their focus to knowledge graphs, attempting to use graph algorithms to structure and operationalize resources and improve the results. For example, Yan et al. proved that knowledge graph is useful for Atomic Fact Decomposition-based problem. [ 25 ] Li et al. also propose KELDaR framework to enhance atomic facts-based ability by knowledge graph. [ 14 ]",
                "Li et al. (2025) define scoring bias and position bias in RAG systems, and show that LLM judges are less stable when encountering adversarial manipulations. Shi et al. (2024) study position bias in pairwise comparisons conducted by LLM judges. Li et al. (2025) also demonstrate the advantages of using LLMs as evaluators for assessing RAG systems. Several evaluation frameworks, such as RAGAS [ 6 ] , have achieved significant progress in this field and implement diverse evaluation metrics. Techniques used in RAGAS include splitting sentences into atomic statements and employing embedding models to compute similarity values."
            ],
            [
                "3 Environment Setup",
                "In this section, we will discuss in detail the specific implementation steps of our experiment environment.",
                "Environment setup will be discussed in detail."
            ],
            [
                "4 Methodology",
                "The evaluator LLM utilized in the research below is GPT-4o-mini and the embedding model utilized for semantic similarity is all-MiniLM-L6-v2 . Building upon the RAGAS, we introduce a KG-based approach that enables deeper multi-hop reasoning. The KG-based evaluation metrics we adopt are context-agnostic , meaning they can be flexibly applied to various combinations of input components without being tied to a specific retrieval-generation pipeline. Specifically, the following input pairs can be evaluated: Context Relevancy, Factual Correctness, Faithfulness and Answer Relevancy. In the following, we describe the evaluation steps under the assumption that we are calculating context relevancy —i.e., measuring the semantic alignment between the input question and the retrieved context. The whole evaluation process can be separated into three stages: we first introduces the construction of the knowledge graph (Section 4.1 ), and then presents two algorithms implemented on top of the KG (Sections 4.2 and 4.3 ).",
                "GPT-4o-mini LLM, all-MiniLM-L6-v2 embedding model, RAGAS approach, KG-based metrics: Context Relevancy, Factual Correctness, Faithfulness, Answer Relevancy. Three stages of evaluation: knowledge graph construction (Section 4.1), two algorithms on top of the KG (Sections 4.2 and 4.3)."
            ],
            [
                "6 Limitations",
                "A major limitation of our evaluation system lies in its scalability. The core bottleneck is the high computational cost of graph construction. In particular, when the input context is large, the time required to build the graph grows significantly, which hinders efficiency and makes scaling to real-world settings challenging.",
                "Major limitation: scalability; core bottleneck: high computational cost of graph construction; time required to build graph grows with input context size."
            ],
            [
                "7 Conclusion and future scope",
                "This paper proposes an LLM-driven KG-based approach for evaluating RAG systems. By leveraging an LLM as an evaluator and defining multi-dimensional metrics, we conduct an efficient and accurate assessment of RAG systems.We evaluate two KG-based subscores, Multi-Hop Semantic Matching and Community-Based Semantic Overlap , which show moderate-to-high correlation with both human annotations and RAGAS. They complement each other across different metrics, and exhibit higher sensitivity when contrasting highly or non-relevant inputs. Currently, we only focus on the similarity between individual entities. Valuable research directions can be to investigate how to extend the similarity to triplet level and how to find a well-defined hyperparameter to gain a more fine-grained evaluation. Other metrics, such as negative rejection and long-context accuracy, also worth thorough exploration. [ b27 ] [ b28 ] {credits}",
                "LLM-driven KG-based approach evaluates RAG systems with 2 subscores: Multi-Hop Semantic Matching and Community-Based Semantic Overlap. Moderate-to-high correlation with human annotations and RAGAS is shown. Research directions include extending similarity to triplet level and finding a hyperparameter for fine-grained evaluation."
            ]
        ],
        "general_summary": "**Research Overview**\n\nThis research addresses the challenging task of evaluating Retrieval Augmented Generation (RAG) systems, which are essential in various applications of large language models (LLMs). The current traditional evaluation metrics fail to capture the key features of modern LLM-generated content, leading to the development of a Knowledge-Graph (KG) based RAG system evaluation framework. This new approach enables multi-hop reasoning and semantic community clustering, providing more comprehensive scoring metrics.\n\n**Methodology & Approach**\n\nThe proposed solution utilizes a KG-based evaluation paradigm inspired by the RAGAS tool. The key technical components include a GPT-4o-mini LLM, an all-MiniLM-L6-v2 embedding model, and KG-based metrics such as Context Relevancy, Factual Correctness, Faithfulness, and Answer Relevancy. Three stages of evaluation are employed: knowledge graph construction, multi-hop semantic matching, and community-based semantic overlap.\n\n**Key Contributions & Findings**\n\nThe KG-based RAG system evaluation framework demonstrates a moderate-to-high correlation with human annotations and outperforms the traditional RAGAS scores. The results show that the new approach is more sensitive to subtle semantic differences in generated outputs. Additionally, the study highlights the importance of evaluating RAG systems comprehensively using KG-based metrics.\n\n**Technical Details**\n\nThe framework uses three algorithms on top of the constructed knowledge graph: a multi-hop algorithm for semantic matching and a community clustering algorithm for overlap evaluation. The embedding model is used to compute similarity values between atomic statements. The graph construction process is computationally expensive, which limits the scalability of the approach.\n\n**Limitations & Future Work**\n\nThe major limitation of this work is its scalability due to the high computational cost of graph construction, which grows with input context size. To overcome this challenge, future research directions include extending similarity measures to triplet level and fine-tuning hyperparameters for more precise evaluation.\n\n**Practical Implications**\n\nThis study provides a comprehensive evaluation framework for RAG systems, enabling researchers to better assess their performance. The KG-based approach can be applied in various fields where LLM-generated content is used, such as text generation and dialog systems."
    },
    {
        "id": "2510.08958v1",
        "title": "EcphoryRAG: Re-Imagining Knowledge-Graph RAG via Human Associative\n  Memory",
        "authors": [
            "Zirui Liao"
        ],
        "url": "https://arxiv.org/abs/2510.08958v1",
        "Abstract": "Abstract Cognitive neuroscience research indicates that humans leverage cues to activate entity-centered memory traces (engrams) for complex, multi-hop recollection. Inspired by this mechanism, we introduce EcphoryRAG, an entity-centric knowledge graph RAG framework. During indexing, EcphoryRAG extracts and stores only core entities with corresponding metadata, a lightweight approach that reduces token consumption by up to 94% compared to other structured RAG systems. For retrieval, the system first extracts cue entities from queries, then performs a scalable multi-hop associative search across the knowledge graph. Crucially, EcphoryRAG dynamically infers implicit relations between entities to populate context, enabling deep reasoning without exhaustive pre-enumeration of relationships. Extensive evaluations on the 2WikiMultiHop, HotpotQA, and MuSiQue benchmarks demonstrate that EcphoryRAG sets a new state-of-the-art, improving the average Exact Match (EM) score from 0.392 to 0.474 over strong KG-RAG methods like HippoRAG. These results validate the efficacy of the entity-cue-multi-hop retrieval paradigm for complex question answering.",
        "Main": {
            "1 Introduction": "The capacity for lifelong acquisition, integration, and reasoning over ever-expanding knowledge is both a defining trait of human cognition and a pivotal challenge for Large Language Models (LLMs). Despite their impressive parametric memory, these models are inherently static and susceptible to hallucination. Retrieval-Augmented Generation (RAG) has consequently become the de-facto paradigm for grounding LLMs in external, verifiable sources [ 4 ] . The evolution of RAG has progressed from simple dense retrieval towards more structured approaches. For example, methods like ReAct [ 16 ] simulate multi-hop reasoning through iterative LLM calls, but this approach can introduce high latency and propagate errors. Knowledge Graph-Augmented RAG (KG-RAG) offers a more robust solution by explicitly modeling relationships [ 2 ] .However, current KG-RAG implementations present a difficult trade-off. Static, pre-computed graphs offer efficient querying but are costly to update and struggle with unfamiliar queries. In contrast, dynamic graph traversal provides flexibility but often depends on slow, token-by-token LLM guidance for pathfinding [ 11 ] . There is a clear need for a framework that merges the structural efficiency of static graphs with the adaptability of dynamic methods. Cognitive science offers a different model for knowledge retrieval. Human memory, for instance, does not perform an exhaustive search; recall is instead an efficient, cue-driven process. As shown in Figure 1 , a general prompt may yield incomplete information, whereas specific cues can activate targeted memory traces—or engrams 1 1 1 We use engram computationally to denote a structured record (an entity plus its metadata), drawing an analogy to the biological ensemble of neurons without claiming a direct equivalence. —for precise retrieval [ 13 , 3 ] . The principle by which a partial cue triggers the reconstruction of a full memory is known as ecphory . Although recent systems like HippoRAG2 [ 7 ] draw inspiration from human memory, they do not fully replicate this dynamic, cue-based mechanism. Therefore, a practical and scalable computational model of ecphory for RAG has yet to be developed. To address this gap, we introduce EcphoryRAG , a novel framework that instantiates the principle of ecphory for knowledge reasoning. As illustrated in Figure 2 , EcphoryRAG fundamentally redesigns the standard RAG workflow. Instead of direct, monolithic retrieval, our framework first uses an LLM to Extract specific Cues from the user’s query. These cues then trigger a targeted Recall process (Ecphory) from a structured knowledge base of engrams. This design achieves the structural integrity of KG-RAG while maintaining the adaptability of dynamic methods, all inspired by the efficiency of human memory. Our primary contributions are: (1) We propose EcphoryRAG, a new RAG framework that operationalizes the cognitive principle of ecphory through a cue-driven, multi-hop retrieval mechanism designed for complex reasoning. (2) We introduce a hybrid associative search algorithm that combines explicit graph traversal with implicit semantic expansion, enabling the discovery of complex, latent reasoning paths that are missed by other methods. (3) We demonstrate through extensive experiments that EcphoryRAG not only establishes a new state-of-the-art on three challenging multi-hop QA benchmarks, but also achieves this with remarkable efficiency, reducing offline indexing token costs by up to 18x compared to other structured RAG systems.",
            "2 Related Work": "Our work builds upon advancements in Retrieval-Augmented Generation (RAG), drawing from the evolution of structured retrieval and the emerging field of cognitive-inspired memory systems.",
            "3 The EcphoryRAG Framework": "This section details our proposed framework, EcphoryRAG, which is inspired by the cognitive process of memory ecphory—the mechanism by which cues trigger the retrieval of stored memory traces [ 13 ] . Drawing an analogy from human memory, EcphoryRAG computationalizes this process for complex, multi-hop question answering. It comprises two main phases: an offline Memory System Construction phase to index knowledge, and an online Retrieval as Ecphory phase that simulates cue-driven recall. The detailed workflow is illustrated in Figure 3 .",
            "4 Experiments": "We conduct a series of experiments to comprehensively evaluate EcphoryRAG, focusing on its multi-hop reasoning performance, computational efficiency, and the impact of its core architectural components.",
            "6 Conclusion": "In this paper, we introduced EcphoryRAG, a novel framework that operationalizes the cognitive principle of memory ecphory for complex, multi-hop question answering. By integrating a cue-driven associative search with a highly efficient entity-centric indexing strategy, our framework sets a new state-of-the-art on challenging QA benchmarks. The results validate that our cognitively inspired design leads not only to superior reasoning accuracy but also to a more practical and scalable architecture for structured RAG systems."
        },
        "Tuples": [
            [
                "1 Introduction",
                "The capacity for lifelong acquisition, integration, and reasoning over ever-expanding knowledge is both a defining trait of human cognition and a pivotal challenge for Large Language Models (LLMs). Despite their impressive parametric memory, these models are inherently static and susceptible to hallucination. Retrieval-Augmented Generation (RAG) has consequently become the de-facto paradigm for grounding LLMs in external, verifiable sources [ 4 ] . The evolution of RAG has progressed from simple dense retrieval towards more structured approaches. For example, methods like ReAct [ 16 ] simulate multi-hop reasoning through iterative LLM calls, but this approach can introduce high latency and propagate errors. Knowledge Graph-Augmented RAG (KG-RAG) offers a more robust solution by explicitly modeling relationships [ 2 ] .However, current KG-RAG implementations present a difficult trade-off. Static, pre-computed graphs offer efficient querying but are costly to update and struggle with unfamiliar queries. In contrast, dynamic graph traversal provides flexibility but often depends on slow, token-by-token LLM guidance for pathfinding [ 11 ] . There is a clear need for a framework that merges the structural efficiency of static graphs with the adaptability of dynamic methods. Cognitive science offers a different model for knowledge retrieval. Human memory, for instance, does not perform an exhaustive search; recall is instead an efficient, cue-driven process. As shown in Figure 1 , a general prompt may yield incomplete information, whereas specific cues can activate targeted memory traces—or engrams 1 1 1 We use engram computationally to denote a structured record (an entity plus its metadata), drawing an analogy to the biological ensemble of neurons without claiming a direct equivalence. —for precise retrieval [ 13 , 3 ] . The principle by which a partial cue triggers the reconstruction of a full memory is known as ecphory . Although recent systems like HippoRAG2 [ 7 ] draw inspiration from human memory, they do not fully replicate this dynamic, cue-based mechanism. Therefore, a practical and scalable computational model of ecphory for RAG has yet to be developed. To address this gap, we introduce EcphoryRAG , a novel framework that instantiates the principle of ecphory for knowledge reasoning. As illustrated in Figure 2 , EcphoryRAG fundamentally redesigns the standard RAG workflow. Instead of direct, monolithic retrieval, our framework first uses an LLM to Extract specific Cues from the user’s query. These cues then trigger a targeted Recall process (Ecphory) from a structured knowledge base of engrams. This design achieves the structural integrity of KG-RAG while maintaining the adaptability of dynamic methods, all inspired by the efficiency of human memory. Our primary contributions are: (1) We propose EcphoryRAG, a new RAG framework that operationalizes the cognitive principle of ecphory through a cue-driven, multi-hop retrieval mechanism designed for complex reasoning. (2) We introduce a hybrid associative search algorithm that combines explicit graph traversal with implicit semantic expansion, enabling the discovery of complex, latent reasoning paths that are missed by other methods. (3) We demonstrate through extensive experiments that EcphoryRAG not only establishes a new state-of-the-art on three challenging multi-hop QA benchmarks, but also achieves this with remarkable efficiency, reducing offline indexing token costs by up to 18x compared to other structured RAG systems."
            ],
            [
                "2 Related Work",
                "Our work builds upon advancements in Retrieval-Augmented Generation (RAG), drawing from the evolution of structured retrieval and the emerging field of cognitive-inspired memory systems."
            ],
            [
                "3 The EcphoryRAG Framework",
                "This section details our proposed framework, EcphoryRAG, which is inspired by the cognitive process of memory ecphory—the mechanism by which cues trigger the retrieval of stored memory traces [ 13 ] . Drawing an analogy from human memory, EcphoryRAG computationalizes this process for complex, multi-hop question answering. It comprises two main phases: an offline Memory System Construction phase to index knowledge, and an online Retrieval as Ecphory phase that simulates cue-driven recall. The detailed workflow is illustrated in Figure 3 ."
            ],
            [
                "4 Experiments",
                "We conduct a series of experiments to comprehensively evaluate EcphoryRAG, focusing on its multi-hop reasoning performance, computational efficiency, and the impact of its core architectural components."
            ],
            [
                "6 Conclusion",
                "In this paper, we introduced EcphoryRAG, a novel framework that operationalizes the cognitive principle of memory ecphory for complex, multi-hop question answering. By integrating a cue-driven associative search with a highly efficient entity-centric indexing strategy, our framework sets a new state-of-the-art on challenging QA benchmarks. The results validate that our cognitively inspired design leads not only to superior reasoning accuracy but also to a more practical and scalable architecture for structured RAG systems."
            ]
        ],
        "section_summaries": [
            [
                "1 Introduction",
                "The capacity for lifelong acquisition, integration, and reasoning over ever-expanding knowledge is both a defining trait of human cognition and a pivotal challenge for Large Language Models (LLMs). Despite their impressive parametric memory, these models are inherently static and susceptible to hallucination. Retrieval-Augmented Generation (RAG) has consequently become the de-facto paradigm for grounding LLMs in external, verifiable sources [ 4 ] . The evolution of RAG has progressed from simple dense retrieval towards more structured approaches. For example, methods like ReAct [ 16 ] simulate multi-hop reasoning through iterative LLM calls, but this approach can introduce high latency and propagate errors. Knowledge Graph-Augmented RAG (KG-RAG) offers a more robust solution by explicitly modeling relationships [ 2 ] .However, current KG-RAG implementations present a difficult trade-off. Static, pre-computed graphs offer efficient querying but are costly to update and struggle with unfamiliar queries. In contrast, dynamic graph traversal provides flexibility but often depends on slow, token-by-token LLM guidance for pathfinding [ 11 ] . There is a clear need for a framework that merges the structural efficiency of static graphs with the adaptability of dynamic methods. Cognitive science offers a different model for knowledge retrieval. Human memory, for instance, does not perform an exhaustive search; recall is instead an efficient, cue-driven process. As shown in Figure 1 , a general prompt may yield incomplete information, whereas specific cues can activate targeted memory traces—or engrams 1 1 1 We use engram computationally to denote a structured record (an entity plus its metadata), drawing an analogy to the biological ensemble of neurons without claiming a direct equivalence. —for precise retrieval [ 13 , 3 ] . The principle by which a partial cue triggers the reconstruction of a full memory is known as ecphory . Although recent systems like HippoRAG2 [ 7 ] draw inspiration from human memory, they do not fully replicate this dynamic, cue-based mechanism. Therefore, a practical and scalable computational model of ecphory for RAG has yet to be developed. To address this gap, we introduce EcphoryRAG , a novel framework that instantiates the principle of ecphory for knowledge reasoning. As illustrated in Figure 2 , EcphoryRAG fundamentally redesigns the standard RAG workflow. Instead of direct, monolithic retrieval, our framework first uses an LLM to Extract specific Cues from the user’s query. These cues then trigger a targeted Recall process (Ecphory) from a structured knowledge base of engrams. This design achieves the structural integrity of KG-RAG while maintaining the adaptability of dynamic methods, all inspired by the efficiency of human memory. Our primary contributions are: (1) We propose EcphoryRAG, a new RAG framework that operationalizes the cognitive principle of ecphory through a cue-driven, multi-hop retrieval mechanism designed for complex reasoning. (2) We introduce a hybrid associative search algorithm that combines explicit graph traversal with implicit semantic expansion, enabling the discovery of complex, latent reasoning paths that are missed by other methods. (3) We demonstrate through extensive experiments that EcphoryRAG not only establishes a new state-of-the-art on three challenging multi-hop QA benchmarks, but also achieves this with remarkable efficiency, reducing offline indexing token costs by up to 18x compared to other structured RAG systems.",
                "The capacity for lifelong acquisition and reasoning over knowledge is a challenge for Large Language Models (LLMs), which are static and susceptible to hallucination. Retrieval-Augmented Generation (RAG) has become the paradigm, evolving from simple dense retrieval to structured approaches like ReAct and Knowledge Graph-Augmented RAG (KG-RAG). However, KG-RAG implementations present a trade-off between efficiency and adaptability.\n\nHuman memory offers an alternative model for knowledge retrieval, which does not perform exhaustive search. Instead, recall is cue-driven and efficient. EcphoryRAG introduces a framework that instantiates the principle of ecphory for RAG, redesigning the standard workflow to first extract specific cues from user queries and then trigger targeted recall from a structured knowledge base.\n\nThe framework combines the structural integrity of KG-RAG with adaptability through a cue-driven, multi-hop retrieval mechanism. It achieves state-of-the-art results on three challenging multi-hop QA benchmarks while reducing offline indexing token costs by up to 18x compared to other structured RAG systems."
            ],
            [
                "2 Related Work",
                "Our work builds upon advancements in Retrieval-Augmented Generation (RAG), drawing from the evolution of structured retrieval and the emerging field of cognitive-inspired memory systems.",
                "RAG, structured retrieval, and cognitive-inspired memory systems."
            ],
            [
                "3 The EcphoryRAG Framework",
                "This section details our proposed framework, EcphoryRAG, which is inspired by the cognitive process of memory ecphory—the mechanism by which cues trigger the retrieval of stored memory traces [ 13 ] . Drawing an analogy from human memory, EcphoryRAG computationalizes this process for complex, multi-hop question answering. It comprises two main phases: an offline Memory System Construction phase to index knowledge, and an online Retrieval as Ecphory phase that simulates cue-driven recall. The detailed workflow is illustrated in Figure 3 .",
                "EcphoryRAG framework inspired by human memory ecphory, comprising two phases: \n- Offline Memory System Construction to index knowledge\n- Online Retrieval as Ecphory for cue-driven recall\n[13]"
            ],
            [
                "4 Experiments",
                "We conduct a series of experiments to comprehensively evaluate EcphoryRAG, focusing on its multi-hop reasoning performance, computational efficiency, and the impact of its core architectural components.",
                "We conduct 4 experiments."
            ],
            [
                "6 Conclusion",
                "In this paper, we introduced EcphoryRAG, a novel framework that operationalizes the cognitive principle of memory ecphory for complex, multi-hop question answering. By integrating a cue-driven associative search with a highly efficient entity-centric indexing strategy, our framework sets a new state-of-the-art on challenging QA benchmarks. The results validate that our cognitively inspired design leads not only to superior reasoning accuracy but also to a more practical and scalable architecture for structured RAG systems.",
                "EcphoryRAG framework sets new state-of-the-art on challenging QA benchmarks, achieving superior reasoning accuracy and a more practical scalable architecture."
            ]
        ],
        "general_summary": "**Research Overview**\n\nThis research addresses the challenge of large language models' (LLMs) limitations in lifelong knowledge acquisition and reasoning through a novel approach to Retrieval-Augmented Generation (RAG). Inspired by human memory mechanisms, EcphoryRAG proposes an entity-centric framework that combines structured knowledge graphs with cue-driven, multi-hop retrieval. The system achieves state-of-the-art results on complex question-answering benchmarks while reducing indexing costs.\n\n**Methodology & Approach**\n\nEcphoryRAG's offline phase extracts core entities and metadata from a knowledge graph, storing only essential information to reduce token consumption by up to 94%. During online retrieval, the framework extracts cue entities from queries and performs scalable multi-hop associative search across the knowledge graph. Crucially, EcphoryRAG infers implicit relations between entities dynamically, enabling deep reasoning without exhaustive pre-enumeration of relationships.\n\n**Key Contributions & Findings**\n\nEcphoryRAG sets a new state-of-the-art on challenging QA benchmarks, improving the average Exact Match (EM) score from 0.392 to 0.474 over strong KG-RAG methods like HippoRAG. This result validates the efficacy of the entity-cue-multi-hop retrieval paradigm for complex question answering.\n\n**Technical Details**\n\nThe EcphoryRAG framework combines two phases: Offline Memory System Construction and Online Retrieval as Ecphory for cue-driven recall. The system utilizes a knowledge graph with extracted core entities and metadata, and employs a scalable multi-hop associative search mechanism to infer implicit relations between entities.\n\n**Limitations & Future Work**\n\nWhile EcphoryRAG achieves state-of-the-art results on QA benchmarks, its limitations include the need for further exploration of its scalability in real-world applications. Future research should also investigate more effective ways to integrate explicit relationships into the framework.\n\n**Practical Implications**\n\nEcphoryRAG has significant implications for the development of practical and scalable question-answering systems. Its ability to reduce indexing costs while maintaining superior reasoning accuracy makes it an attractive solution for real-world applications."
    },
    {
        "id": "2510.03663v2",
        "title": "UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG",
        "authors": [
            "Xiangyu Peng",
            "Can Qin",
            "Zeyuan Chen",
            "Ran Xu",
            "Caiming Xiong",
            "Chien-Sheng Wu"
        ],
        "url": "https://arxiv.org/abs/2510.03663v2",
        "Abstract": "Abstract Multimodal retrieval-augmented Generation (MM-RAG) is a key approach for applying large language models (LLMs) and agents to real-world knowledge bases, yet current evaluations are fragmented—focusing on either text or images in isolation, or simplified multimodal setup, failing to capture document-centric multimodal use cases.\nIn this paper, we introduce UniDoc-Bench 1 1 1 The code and data will be available at: https://github.com/SalesforceAIResearch/UniDOC-Bench , the first large-scale, realistic benchmark for MM-RAG built from 70 70 k real-world PDF pages across 8 8 domains.\nOur pipeline extracts and links evidence from text, tables, and\nfigures, then generates 1 , 600 1,600 multimodal QA pairs spanning factual retrieval, comparison, summarization, and logical reasoning queries.\nTo ensure reliability, 20 % 20\\% of QA pairs\nare validated by multiple annotators and expert adjudication. UniDoc-Bench supports apples-to-apples comparison across four paradigms — 1) text-only, 2) image-only, 3) multimodal text–image fusion and 4) multimodal joint retrieval — under a unified protocol with standardized candidate pools, prompts, and evaluation metrics.\nOur experiments show that multimodal text–image fusion RAG systems consistently outperform both unimodal and jointly multimodal embedding–based retrieval, indicating that neither text nor images alone are sufficient and that current multimodal embeddings remain inadequate. Beyond benchmarking, our analysis reveals when and how visual context complements textual evidence, uncovers systematic failure modes, and offers actionable guidance for developing more robust MM-RAG pipelines.",
        "Main": {
            "1 Introduction": "Retrieval-augmented generation (RAG) has become a widely used approach for applying large language models (LLMs) and agents to real-world knowledge bases (Gao et al., 2023 ; Fan et al., 2024 ) . The dominant text-only pipeline applies Optical Character Recognition (OCR) (Li et al., 2022 ; Xue et al., 2024 ; Poznanski et al., 2025 ) to flatten document pages into text, indexes them as chunks, retrieves top-k text passages, and feeds them to a generator. However, many answers depend on information embedded in figures, charts, tables, and complex layouts, where OCR often discards crucial spatial and visual semantics (e.g., map, axes, bar lengths, color encodings) (Ma et al., 2024a ; Faysse et al., 2024a ) . These limitations have driven the rapid development of multimodal RAG (MM-RAG), which embeds documents across modalities (text, tables, and images) and retrieves and reasons over them jointly, emerging as a key paradigm for document intelligence. Current MM-RAG evaluation benchmarks exhibit substantial limitations, as summarized in Table 1 . Many are restricted to a single image or a single document page as reference (Mathew et al., 2021 ; 2022 ; Zhu et al., 2022 ; Li et al., 2024 ; Ma et al., 2024b ) , cover narrow domains Mathew et al. ( 2021 ; 2022 ); Zhu et al. ( 2022 ); Li et al. ( 2024 ) , under-represent modalities (Li et al., 2024 ; Mathew et al., 2022 ) , operate at limited scale (few queries/pages) (Ma et al., 2024b ; Wang et al., 2025b ) or lack a highly relevant database for RAG evaluation (Ma et al., 2024b ) . These gaps hinder fair and comprehensive comparison across methods. Moreover, debatable claims have emerged — such as that “image retrieval is all you need” (Faysse et al., 2024a ; Su et al., 2025 ) or that multimodal retrieval is inherently superior (Zhang et al., 2024b ; Yu et al., 2024b ) — without enough fair and unified evaluation. In response, we introduce UniDoc-Bench , a manually verified benchmark spanning 8 8 domains and covering text, chart, and table content, explicitly designed for cross-modality grounding with examples shown in Figure 1 . Crucially, UniDoc-Bench enables apples-to-apples evaluation of text-retrieval, image-retrieval, multimodal text-image-fusion retrieval, and multimodal joint retrieval pipelines using highly relevant large document database and multi-type, cross-modality-grounding queries under a unified protocol. This setup provides an unbiased view of when multimodal retrieval offers advantages beyond single modalities. In practice, UniDoc-Bench quantifies multimodal gains, guides system design choices, and accelerates the development of effective MM-RAG systems for real-world document intelligence. We curate a high-quality multimodal RAG evaluation benchmark by designing and applying a classification-based filtering scheme to unlabeled, real-world PDF documents (PDFA (Montalvo & Wightman, 2024 ) ), yielding 70 70 k highly relevant pages across eight widely used domains — Finance, Legal, Healthcare, Commerce and Manufacturing, CRM, Energy, Education, and Construction —containing rich cross-modality content, including text, tables, and images. We construct a knowledge graph that links cross-modality contents across documents via overlapping entities, and leverage these connections to synthesize 1,600 QA pairs spanning four question types: factual retrieval , comparison , summarization , and logical reasoning , enabling multi-modality grounding and reflecting realistic retrieval scenarios. To ensure quality, 20 % 20\\% of the QA pairs are evaluated by three independent annotators for faithfulness, completeness, self-containment, human intent, and evidence usability, with disagreements resolved through expert adjudication. Figure 2 illustrates the full pipeline from PDF segmentation to dataset creation and evaluation. In this paper, we compare text-only, image-only, multimodal joint, and text-image-fusion retrieval augmented generation pipelines under a unified setup, using identical candidate pools, fixed top- k k , consistent prompts, and standardized evaluation criteria. We report retrieval metrics ( Recall@10 , Precision@10 ), answer completeness and faithfulness defined at Section 4.2 . We observe consistent gains for text–image-fusion RAG systems ( completeness = 68.4 % 68.4\\% ) over multimodal joint retrieval systems ( 64.1 % 64.1\\% ), text-retrieval systems ( 65.3 % 65.3\\% ), and image-retrieval systems ( 54.5 % 54.5\\% ). This indicates that retrieving text and images separately using dedicated embeddings, then combining them in the final LLM query, outperforms unified embeddings or single-modality retrieval. Moreover, visual evidence improves answer completeness and enhances faithfulness when paired with textual context, though image-only retrieval cannot fully capture the textual information contained in images. Questions requiring images to answer remain challenging for all systems, suggesting that future RAG improvements should prioritize image-dependent queries. In contrast, performance differences across question types, such as comparison or factual retrieval, are minimal. In this paper, we make the following contributions: • We introduce a new multimodal RAG benchmark built from real-world PDF documents, comprising 70k pages across 8 domains, with 1,600 human-verified QA pairs referencing text, figures, and tables, spanning 4 4 question types. • We present an associated data synthesizing pipeline for creating multimodal RAG evaluation datasets, designed to be compatible with any document database. • We propose a fair and reproducible evaluation framework by fixing candidate pools across modalities , and measuring retrieval effectiveness, answer faithfulness, and completeness end-to-end across different RAG systems. Specifically, to ensure fairness when comparing against text-only RAG, we caption images and tables and match them back to the retrieved text chunks before final generation, thereby maintaining a consistent candidate pool. • We conduct a systematic comparison of text-retrieval, image-retrieval, text–image fusion, and multimodal joint retrieval pipelines, analyzing which retrieval strategy performs best under different question types, evidence modalities, and document characteristics, providing practical guidance for choosing MM-RAG systems in real-world data settings.",
            "3 Dataset Curation": "First, a large-scale, high-quality multi-modal database is needed for evaluating RAG systems, where each document contains content-rich figures, tables and corresponding textual information. Documents should be domain-specific and exhibit high inter-document similarity to evaluate effective retrieval. The construction of this database is detailed in Section 3.1 . Then, we require high-quality query–answer pairs to evaluate the RAG system. Each query is designed to reflect realistic human intent and is written as a self-contained question. The corresponding ground-truth answer must be retrievable solely from the curated database and supported by evidence across multiple modalities. In Section 3.2 , we describe our synthetic QA pipeline, and in Section 3.3 , we validate dataset quality through human annotation.",
            "4 Experiments": "To fairly evaluate different RAG systems, we focus on two aspects: retrieval and end-to-end performance. In this section, we first evaluate the retrieval performance of four embedding and retrieval models, including text-only, image-only, and two multimodal approaches (Section 4.1 ). We then assess the end-to-end response performance of six RAG systems that vary in their use of embeddings, retrieval strategies, and LLMs (Section 4.2 ). Together, these experiments highlight the utility of our dataset and provide practical guidance for selecting RAG components.",
            "5 Conclusion": "In this paper, we introduced UniDoc-Bench , a large-scale benchmark for document-centric multimodal RAG, built from 70 70 k real-world PDF pages across 8 8 domains with 1 , 600 1,600 human-verified QA pairs. Our experiments establish a clear performance hierarchy, showing that text-image fusion RAG performs the best , consistently outperforming both joint multimodal (MM) RAG and single-modality RAG systems. This key finding demonstrates that fusing separate, strong retrievers for text and images is currently a more effective strategy than relying on a single joint multimodal embedding or a single modality alone. Our analysis further pinpoints image-dependent queries as the primary challenge for all systems. By providing a standardized platform for fair comparison, UniDoc-Bench serves as a crucial resource to guide the development of more robust and faithful document intelligence systems.",
            "Acknowledgements": "We would like to sincerely thank Nesrine Yakoubi, Caitlyn Cline, Wyatt Miller, Michael Thuo, John Soledad, and Fabriana Louisa Pita for their invaluable efforts in annotating the datasets. Their careful work and dedication were essential to the success of this research."
        },
        "Tuples": [
            [
                "1 Introduction",
                "Retrieval-augmented generation (RAG) has become a widely used approach for applying large language models (LLMs) and agents to real-world knowledge bases (Gao et al., 2023 ; Fan et al., 2024 ) . The dominant text-only pipeline applies Optical Character Recognition (OCR) (Li et al., 2022 ; Xue et al., 2024 ; Poznanski et al., 2025 ) to flatten document pages into text, indexes them as chunks, retrieves top-k text passages, and feeds them to a generator. However, many answers depend on information embedded in figures, charts, tables, and complex layouts, where OCR often discards crucial spatial and visual semantics (e.g., map, axes, bar lengths, color encodings) (Ma et al., 2024a ; Faysse et al., 2024a ) . These limitations have driven the rapid development of multimodal RAG (MM-RAG), which embeds documents across modalities (text, tables, and images) and retrieves and reasons over them jointly, emerging as a key paradigm for document intelligence. Current MM-RAG evaluation benchmarks exhibit substantial limitations, as summarized in Table 1 . Many are restricted to a single image or a single document page as reference (Mathew et al., 2021 ; 2022 ; Zhu et al., 2022 ; Li et al., 2024 ; Ma et al., 2024b ) , cover narrow domains Mathew et al. ( 2021 ; 2022 ); Zhu et al. ( 2022 ); Li et al. ( 2024 ) , under-represent modalities (Li et al., 2024 ; Mathew et al., 2022 ) , operate at limited scale (few queries/pages) (Ma et al., 2024b ; Wang et al., 2025b ) or lack a highly relevant database for RAG evaluation (Ma et al., 2024b ) . These gaps hinder fair and comprehensive comparison across methods. Moreover, debatable claims have emerged — such as that “image retrieval is all you need” (Faysse et al., 2024a ; Su et al., 2025 ) or that multimodal retrieval is inherently superior (Zhang et al., 2024b ; Yu et al., 2024b ) — without enough fair and unified evaluation. In response, we introduce UniDoc-Bench , a manually verified benchmark spanning 8 8 domains and covering text, chart, and table content, explicitly designed for cross-modality grounding with examples shown in Figure 1 . Crucially, UniDoc-Bench enables apples-to-apples evaluation of text-retrieval, image-retrieval, multimodal text-image-fusion retrieval, and multimodal joint retrieval pipelines using highly relevant large document database and multi-type, cross-modality-grounding queries under a unified protocol. This setup provides an unbiased view of when multimodal retrieval offers advantages beyond single modalities. In practice, UniDoc-Bench quantifies multimodal gains, guides system design choices, and accelerates the development of effective MM-RAG systems for real-world document intelligence. We curate a high-quality multimodal RAG evaluation benchmark by designing and applying a classification-based filtering scheme to unlabeled, real-world PDF documents (PDFA (Montalvo & Wightman, 2024 ) ), yielding 70 70 k highly relevant pages across eight widely used domains — Finance, Legal, Healthcare, Commerce and Manufacturing, CRM, Energy, Education, and Construction —containing rich cross-modality content, including text, tables, and images. We construct a knowledge graph that links cross-modality contents across documents via overlapping entities, and leverage these connections to synthesize 1,600 QA pairs spanning four question types: factual retrieval , comparison , summarization , and logical reasoning , enabling multi-modality grounding and reflecting realistic retrieval scenarios. To ensure quality, 20 % 20\\% of the QA pairs are evaluated by three independent annotators for faithfulness, completeness, self-containment, human intent, and evidence usability, with disagreements resolved through expert adjudication. Figure 2 illustrates the full pipeline from PDF segmentation to dataset creation and evaluation. In this paper, we compare text-only, image-only, multimodal joint, and text-image-fusion retrieval augmented generation pipelines under a unified setup, using identical candidate pools, fixed top- k k , consistent prompts, and standardized evaluation criteria. We report retrieval metrics ( Recall@10 , Precision@10 ), answer completeness and faithfulness defined at Section 4.2 . We observe consistent gains for text–image-fusion RAG systems ( completeness = 68.4 % 68.4\\% ) over multimodal joint retrieval systems ( 64.1 % 64.1\\% ), text-retrieval systems ( 65.3 % 65.3\\% ), and image-retrieval systems ( 54.5 % 54.5\\% ). This indicates that retrieving text and images separately using dedicated embeddings, then combining them in the final LLM query, outperforms unified embeddings or single-modality retrieval. Moreover, visual evidence improves answer completeness and enhances faithfulness when paired with textual context, though image-only retrieval cannot fully capture the textual information contained in images. Questions requiring images to answer remain challenging for all systems, suggesting that future RAG improvements should prioritize image-dependent queries. In contrast, performance differences across question types, such as comparison or factual retrieval, are minimal. In this paper, we make the following contributions: • We introduce a new multimodal RAG benchmark built from real-world PDF documents, comprising 70k pages across 8 domains, with 1,600 human-verified QA pairs referencing text, figures, and tables, spanning 4 4 question types. • We present an associated data synthesizing pipeline for creating multimodal RAG evaluation datasets, designed to be compatible with any document database. • We propose a fair and reproducible evaluation framework by fixing candidate pools across modalities , and measuring retrieval effectiveness, answer faithfulness, and completeness end-to-end across different RAG systems. Specifically, to ensure fairness when comparing against text-only RAG, we caption images and tables and match them back to the retrieved text chunks before final generation, thereby maintaining a consistent candidate pool. • We conduct a systematic comparison of text-retrieval, image-retrieval, text–image fusion, and multimodal joint retrieval pipelines, analyzing which retrieval strategy performs best under different question types, evidence modalities, and document characteristics, providing practical guidance for choosing MM-RAG systems in real-world data settings."
            ],
            [
                "3 Dataset Curation",
                "First, a large-scale, high-quality multi-modal database is needed for evaluating RAG systems, where each document contains content-rich figures, tables and corresponding textual information. Documents should be domain-specific and exhibit high inter-document similarity to evaluate effective retrieval. The construction of this database is detailed in Section 3.1 . Then, we require high-quality query–answer pairs to evaluate the RAG system. Each query is designed to reflect realistic human intent and is written as a self-contained question. The corresponding ground-truth answer must be retrievable solely from the curated database and supported by evidence across multiple modalities. In Section 3.2 , we describe our synthetic QA pipeline, and in Section 3.3 , we validate dataset quality through human annotation."
            ],
            [
                "4 Experiments",
                "To fairly evaluate different RAG systems, we focus on two aspects: retrieval and end-to-end performance. In this section, we first evaluate the retrieval performance of four embedding and retrieval models, including text-only, image-only, and two multimodal approaches (Section 4.1 ). We then assess the end-to-end response performance of six RAG systems that vary in their use of embeddings, retrieval strategies, and LLMs (Section 4.2 ). Together, these experiments highlight the utility of our dataset and provide practical guidance for selecting RAG components."
            ],
            [
                "5 Conclusion",
                "In this paper, we introduced UniDoc-Bench , a large-scale benchmark for document-centric multimodal RAG, built from 70 70 k real-world PDF pages across 8 8 domains with 1 , 600 1,600 human-verified QA pairs. Our experiments establish a clear performance hierarchy, showing that text-image fusion RAG performs the best , consistently outperforming both joint multimodal (MM) RAG and single-modality RAG systems. This key finding demonstrates that fusing separate, strong retrievers for text and images is currently a more effective strategy than relying on a single joint multimodal embedding or a single modality alone. Our analysis further pinpoints image-dependent queries as the primary challenge for all systems. By providing a standardized platform for fair comparison, UniDoc-Bench serves as a crucial resource to guide the development of more robust and faithful document intelligence systems."
            ],
            [
                "Acknowledgements",
                "We would like to sincerely thank Nesrine Yakoubi, Caitlyn Cline, Wyatt Miller, Michael Thuo, John Soledad, and Fabriana Louisa Pita for their invaluable efforts in annotating the datasets. Their careful work and dedication were essential to the success of this research."
            ]
        ],
        "section_summaries": [
            [
                "1 Introduction",
                "Retrieval-augmented generation (RAG) has become a widely used approach for applying large language models (LLMs) and agents to real-world knowledge bases (Gao et al., 2023 ; Fan et al., 2024 ) . The dominant text-only pipeline applies Optical Character Recognition (OCR) (Li et al., 2022 ; Xue et al., 2024 ; Poznanski et al., 2025 ) to flatten document pages into text, indexes them as chunks, retrieves top-k text passages, and feeds them to a generator. However, many answers depend on information embedded in figures, charts, tables, and complex layouts, where OCR often discards crucial spatial and visual semantics (e.g., map, axes, bar lengths, color encodings) (Ma et al., 2024a ; Faysse et al., 2024a ) . These limitations have driven the rapid development of multimodal RAG (MM-RAG), which embeds documents across modalities (text, tables, and images) and retrieves and reasons over them jointly, emerging as a key paradigm for document intelligence. Current MM-RAG evaluation benchmarks exhibit substantial limitations, as summarized in Table 1 . Many are restricted to a single image or a single document page as reference (Mathew et al., 2021 ; 2022 ; Zhu et al., 2022 ; Li et al., 2024 ; Ma et al., 2024b ) , cover narrow domains Mathew et al. ( 2021 ; 2022 ); Zhu et al. ( 2022 ); Li et al. ( 2024 ) , under-represent modalities (Li et al., 2024 ; Mathew et al., 2022 ) , operate at limited scale (few queries/pages) (Ma et al., 2024b ; Wang et al., 2025b ) or lack a highly relevant database for RAG evaluation (Ma et al., 2024b ) . These gaps hinder fair and comprehensive comparison across methods. Moreover, debatable claims have emerged — such as that “image retrieval is all you need” (Faysse et al., 2024a ; Su et al., 2025 ) or that multimodal retrieval is inherently superior (Zhang et al., 2024b ; Yu et al., 2024b ) — without enough fair and unified evaluation. In response, we introduce UniDoc-Bench , a manually verified benchmark spanning 8 8 domains and covering text, chart, and table content, explicitly designed for cross-modality grounding with examples shown in Figure 1 . Crucially, UniDoc-Bench enables apples-to-apples evaluation of text-retrieval, image-retrieval, multimodal text-image-fusion retrieval, and multimodal joint retrieval pipelines using highly relevant large document database and multi-type, cross-modality-grounding queries under a unified protocol. This setup provides an unbiased view of when multimodal retrieval offers advantages beyond single modalities. In practice, UniDoc-Bench quantifies multimodal gains, guides system design choices, and accelerates the development of effective MM-RAG systems for real-world document intelligence. We curate a high-quality multimodal RAG evaluation benchmark by designing and applying a classification-based filtering scheme to unlabeled, real-world PDF documents (PDFA (Montalvo & Wightman, 2024 ) ), yielding 70 70 k highly relevant pages across eight widely used domains — Finance, Legal, Healthcare, Commerce and Manufacturing, CRM, Energy, Education, and Construction —containing rich cross-modality content, including text, tables, and images. We construct a knowledge graph that links cross-modality contents across documents via overlapping entities, and leverage these connections to synthesize 1,600 QA pairs spanning four question types: factual retrieval , comparison , summarization , and logical reasoning , enabling multi-modality grounding and reflecting realistic retrieval scenarios. To ensure quality, 20 % 20\\% of the QA pairs are evaluated by three independent annotators for faithfulness, completeness, self-containment, human intent, and evidence usability, with disagreements resolved through expert adjudication. Figure 2 illustrates the full pipeline from PDF segmentation to dataset creation and evaluation. In this paper, we compare text-only, image-only, multimodal joint, and text-image-fusion retrieval augmented generation pipelines under a unified setup, using identical candidate pools, fixed top- k k , consistent prompts, and standardized evaluation criteria. We report retrieval metrics ( Recall@10 , Precision@10 ), answer completeness and faithfulness defined at Section 4.2 . We observe consistent gains for text–image-fusion RAG systems ( completeness = 68.4 % 68.4\\% ) over multimodal joint retrieval systems ( 64.1 % 64.1\\% ), text-retrieval systems ( 65.3 % 65.3\\% ), and image-retrieval systems ( 54.5 % 54.5\\% ). This indicates that retrieving text and images separately using dedicated embeddings, then combining them in the final LLM query, outperforms unified embeddings or single-modality retrieval. Moreover, visual evidence improves answer completeness and enhances faithfulness when paired with textual context, though image-only retrieval cannot fully capture the textual information contained in images. Questions requiring images to answer remain challenging for all systems, suggesting that future RAG improvements should prioritize image-dependent queries. In contrast, performance differences across question types, such as comparison or factual retrieval, are minimal. In this paper, we make the following contributions: • We introduce a new multimodal RAG benchmark built from real-world PDF documents, comprising 70k pages across 8 domains, with 1,600 human-verified QA pairs referencing text, figures, and tables, spanning 4 4 question types. • We present an associated data synthesizing pipeline for creating multimodal RAG evaluation datasets, designed to be compatible with any document database. • We propose a fair and reproducible evaluation framework by fixing candidate pools across modalities , and measuring retrieval effectiveness, answer faithfulness, and completeness end-to-end across different RAG systems. Specifically, to ensure fairness when comparing against text-only RAG, we caption images and tables and match them back to the retrieved text chunks before final generation, thereby maintaining a consistent candidate pool. • We conduct a systematic comparison of text-retrieval, image-retrieval, text–image fusion, and multimodal joint retrieval pipelines, analyzing which retrieval strategy performs best under different question types, evidence modalities, and document characteristics, providing practical guidance for choosing MM-RAG systems in real-world data settings.",
                "Gao et al. (2023), Fan et al. (2024) Li et al. (2022), Xue et al. (2024), Poznanski et al. (2025) Ma et al. (2024a), Faysse et al. (2024a)\nMathew et al. (2021, 2022); Zhu et al. (2022); Li et al. (2024); Ma et al. (2024b) \nFaysse et al. (2024a); Su et al. (2025)\nZhang et al. (2024b), Yu et al. (2024b)\nMontalvo & Wightman (2024)\n70k, 8\n1,600\n20%\n68.4%, 64.1%, 65.3%, 54.5% \nRecall@10, Precision@10"
            ],
            [
                "3 Dataset Curation",
                "First, a large-scale, high-quality multi-modal database is needed for evaluating RAG systems, where each document contains content-rich figures, tables and corresponding textual information. Documents should be domain-specific and exhibit high inter-document similarity to evaluate effective retrieval. The construction of this database is detailed in Section 3.1 . Then, we require high-quality query–answer pairs to evaluate the RAG system. Each query is designed to reflect realistic human intent and is written as a self-contained question. The corresponding ground-truth answer must be retrievable solely from the curated database and supported by evidence across multiple modalities. In Section 3.2 , we describe our synthetic QA pipeline, and in Section 3.3 , we validate dataset quality through human annotation.",
                "A large-scale multi-modal database is needed for RAG system evaluation with domain-specific documents and high inter-document similarity. The database requires high-quality query-answer pairs where answers are retrievable from the curated database, supported by evidence across multiple modalities. \n\nNote: \n- Database construction is detailed in Section 3.1.\n- Synthetic QA pipeline is described in Section 3.2.\n- Dataset quality is validated through human annotation in Section 3.3.\n- RAG stands for Reasoning about Actions and Goals systems."
            ],
            [
                "4 Experiments",
                "To fairly evaluate different RAG systems, we focus on two aspects: retrieval and end-to-end performance. In this section, we first evaluate the retrieval performance of four embedding and retrieval models, including text-only, image-only, and two multimodal approaches (Section 4.1 ). We then assess the end-to-end response performance of six RAG systems that vary in their use of embeddings, retrieval strategies, and LLMs (Section 4.2 ). Together, these experiments highlight the utility of our dataset and provide practical guidance for selecting RAG components.",
                "Four embedding and retrieval models were evaluated. \nSix RAG systems with varying components were assessed in end-to-end performance."
            ],
            [
                "5 Conclusion",
                "In this paper, we introduced UniDoc-Bench , a large-scale benchmark for document-centric multimodal RAG, built from 70 70 k real-world PDF pages across 8 8 domains with 1 , 600 1,600 human-verified QA pairs. Our experiments establish a clear performance hierarchy, showing that text-image fusion RAG performs the best , consistently outperforming both joint multimodal (MM) RAG and single-modality RAG systems. This key finding demonstrates that fusing separate, strong retrievers for text and images is currently a more effective strategy than relying on a single joint multimodal embedding or a single modality alone. Our analysis further pinpoints image-dependent queries as the primary challenge for all systems. By providing a standardized platform for fair comparison, UniDoc-Bench serves as a crucial resource to guide the development of more robust and faithful document intelligence systems.",
                "UniDoc-Bench is a large-scale benchmark with 70k real-world PDF pages across 8 domains and 1,600 human-verified QA pairs. It shows that text-image fusion RAG outperforms joint multimodal (MM) RAG and single-modality RAG systems. The primary challenge for all systems is image-dependent queries."
            ],
            [
                "Acknowledgements",
                "We would like to sincerely thank Nesrine Yakoubi, Caitlyn Cline, Wyatt Miller, Michael Thuo, John Soledad, and Fabriana Louisa Pita for their invaluable efforts in annotating the datasets. Their careful work and dedication were essential to the success of this research.",
                "Nesrine Yakoubi, Caitlyn Cline, Wyatt Miller, Michael Thuo, John Soledad, and Fabriana Louisa Pita contributed to annotating datasets."
            ]
        ],
        "general_summary": "**Research Overview**\n\nThis research addresses the fragmented evaluation of multimodal retrieval-augmented generation (MM-RAG) systems, which are crucial for applying large language models and agents to real-world knowledge bases. The study introduces UniDoc-Bench, a unified benchmark for document-centric MM-RAG, built from 70k real-world PDF pages across 8 domains.\n\n**Methodology & Approach**\n\nUniDoc-Bench extracts and links evidence from text, tables, and figures in PDF documents, generating 1,600 multimodal QA pairs spanning factual retrieval, comparison, summarization, and logical reasoning queries. To ensure reliability, 20% of QA pairs are validated by multiple annotators and expert adjudication.\n\n**Key Contributions & Findings**\n\nUniDoc-Bench supports apples-to-apples comparison across four paradigms: text-only, image-only, multimodal text-image fusion, and joint retrieval. Experiments show that multimodal text-image fusion RAG systems consistently outperform unimodal and jointly multimodal embedding-based retrieval.\n\n**Technical Details**\n\nThe study employs a synthetic QA pipeline to generate 1,600 human-verified QA pairs, with validation through multiple annotators and expert adjudication. Four embedding and retrieval models are evaluated, and six RAG systems with varying components are assessed in end-to-end performance.\n\n**Limitations & Future Work**\n\nThe primary challenge for all systems is image-dependent queries. The study suggests that multimodal embeddings remain inadequate, indicating a need for further research on multimodal text-image fusion and joint retrieval paradigms.\n\n**Practical Implications**\n\nUniDoc-Bench provides actionable guidance for developing more robust MM-RAG pipelines by uncovering systematic failure modes and revealing when and how visual context complements textual evidence. This benchmark can be used to evaluate and improve the performance of RAG systems in real-world applications, leading to more effective knowledge retrieval and application."
    },
    {
        "id": "2510.06719v1",
        "title": "Differentially Private Synthetic Text Generation for Retrieval-Augmented\n  Generation (RAG)",
        "authors": [
            "Junki Mori",
            "Kazuya Kakizaki",
            "Taiki Miyagawa",
            "Jun Sakuma"
        ],
        "url": "https://arxiv.org/abs/2510.06719v1",
        "Abstract": "Abstract Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by grounding them in external knowledge. However, its application in sensitive domains is limited by privacy risks.\nExisting private RAG methods typically rely on query-time differential privacy (DP), which requires repeated noise injection and leads to accumulated privacy loss.\nTo address this issue, we propose DP-SynRAG, a framework that uses LLMs to generate differentially private synthetic RAG databases.\nUnlike prior methods, the synthetic text can be reused once created, thereby avoiding repeated noise injection and additional privacy costs.\nTo preserve essential information for downstream RAG tasks, DP-SynRAG extends private prediction, which instructs LLMs to generate text that mimics subsampled database records in a DP manner.\nExperiments show that DP-SynRAG achieves superior performanec to the state-of-the-art private RAG systems while maintaining a fixed privacy budget, offering a scalable solution for privacy-preserving RAG.",
        "Main": {
            "1 Introduction": "Retrieval-Augmented Generation (RAG) Lewis et al. ( 2020 ) has been widely used to enhance the performance of large language models (LLMs) Gao et al. ( 2024 ) by leveraging external knowledge databases. However, recent studies have identified significant privacy risks in RAG systems when their databases contain sensitive information Zeng et al. ( 2024 ); Qi et al. ( 2025 ); Jiang et al. ( 2025 ); Maio et al. ( 2024 ); Wang et al. ( 2025b ) . For instance, extraction attacks against medical chatbots for patients or recommender systems for customers can expose private data to attackers. Moreover, sensitive information in the retrieved documents may also be inadvertently revealed to benign users during normal interactions through LLM outputs (Figure 1 ). To develop secure RAG systems, differential privacy (DP), a formal framework for protecting individual records, has begun to be used Grislain ( 2025 ); Koga et al. ( 2025 ); Wang et al. ( 2025a ) . Existing approaches ensures DP by injecting noise into the LLM’s responses to user queries, thereby reducing the influence of any single record. However, these methods must add noise to each response; thus, they consume the privacy budget proportionally to the number of queries. Consequently, in typical RAG scenarios involving many queries under a fixed privacy budget, the utility of responses degrades substantially. To avoid repeated noise injection, we propose a text generation method using LLMs, called D ifferentially P rivate Syn thetic text generation for RAG databases (DP-SynRAG). Once synthetic texts with DP guarantees are generated, they can be reused indefinitely as the RAG database without incurring additional privacy budget, regardless of the number of queries. To achieve high-quality private text generation, we adopt private prediction Hong et al. ( 2024 ); Tang et al. ( 2024 ); Amin et al. ( 2024 ); Gao et al. ( 2025 ) , which prompts the LLM with subsampled database records and rephrasing instructions, while perturbing the aggregated output token distributions to limit per-record information leakage. One limitation of these approaches is that they capture only global properties of the entire dataset, discarding not only sensitive details but also important knowledge needed for RAG. DP-SynRAG mitigates this issue by clustering documents in a DP manner based on keywords and document-level embeddings, thereby grouping semantically similar documents and separating distinct topics. Applying private prediction to these clusters in parallel enables large-scale synthetic text generation that preserves cluster-specific knowledge at low privacy cost. Finally, to reduce the effect of low-quality synthetic texts, we apply LLM-based self-filtering to improve downstream RAG performance. We validate our approach on three datasets tailored to our setting. Results show that our method outperforms existing private RAG approaches in most cases while maintaining a fixed privacy budget, demonstrating its effectiveness and scalability for privacy-preserving RAG applications.",
            "4 Proposed Method": "Existing approaches to our problem Grislain ( 2025 ); Koga et al. ( 2025 ) respond to user queries in RAG systems via private prediction mechanisms. However, when directly applied to multiple queries, their total privacy budget grows linearly with the number of queries. To address this, we propose DP-SynRAG, which generates synthetic data resembling the private data in advance through private prediction while ensuring DP. Since using this synthetic data as a knowledge corpus for later RAG inference is post-processing, the privacy budget remains fixed regardless of the number of queries. Current methods for generating synthetic data via private prediction often produce tokens that capture only the average characteristics of randomly subsampled subsets of the original data (see Section 3.3 ). While this supports domain-specific data generation, it often loses the fine-grained factual details useful as RAG context. In contrast, DP-SynRAG first clusters the dataset based on keywords and document-level embeddings under DP, grouping semantically similar documents and separating distinct topics. Applying private prediction to these subsets in parallel can generate large volumes of high-quality synthetic texts covering diverse topics in the original dataset at a low privacy budget.",
            "5 Privacy Analysis": "Our algorithm generates R R synthetic texts with minimal total privacy budget, using the overlapping parallel composition introduced by Smith et al. ( 2022 ) and converted to zCDP (Appendix B ). The formal privacy guarantee is stated below. We sketch the proof here and defer the full version to Appendix B . Theorem 1 . DP-SynRAG (Algorithm 1 in Appendix A ) satisfies ( ε , δ ) (\\varepsilon,\\delta) -DP for any δ > 0 \\delta>0 and ε = ρ + 4 ρ log ⁡ ( 1 / δ ) \\varepsilon=\\rho+\\sqrt{4\\rho\\log(1/\\delta)} , where ρ = K 2 σ h 2 + L ( 1 8 ε θ s 2 + 1 2 σ μ 2 + T 2 ( c τ ) 2 ) . \\rho=\\frac{K}{2\\sigma_{h}^{2}}+L\\left(\\frac{1}{8}\\varepsilon_{\\theta_{s}}^{2}+\\frac{1}{2\\sigma_{\\mu}^{2}}+\\frac{T}{2}\\left(\\frac{c}{\\tau}\\right)^{2}\\right). Proof Overview.. We adopt zero-concentrated differential privacy (zCDP) Bun and Steinke ( 2016 ) , a variant of DP, as it provides tighter composition bounds and more precise privacy accounting. Our algorithm comprises two sequentially composed mechanisms: (a) histogram generation ( M hist M_{\\text{hist}} ) and (b-d) keyword-based clustering followed by parallel operations on each cluster ( M clus M_{\\text{clus}} ). Within M clus M_{\\text{clus}} , each cluster undergoes a sequence of operations: (c) retrieval ( M retr M_{\\text{retr}} ), (d) private prediction ( M pred M_{\\text{pred}} ). We exclude self-filtering from the privacy analysis since it a post-processing. Our proof proceeds in three steps. First, we show that M hist M_{\\text{hist}} , M retr M_{\\text{retr}} , and M pred M_{\\text{pred}} each satisfy ρ hist \\rho_{\\text{hist}} , ρ retr \\rho_{\\text{retr}} , and ρ pred \\rho_{\\text{pred}} -zCDP, respectively, since they rely on Gaussian or exponential mechanisms (or their compositions). Next, each algorithm executed in parallel within a cluster satisfies ( ρ retr + ρ pred ) (\\rho_{\\text{retr}}+\\rho_{\\text{pred}}) -zCDP by sequential composition, and M clus M_{\\text{clus}} satisfies L ( ρ retr + ρ pred ) L(\\rho_{\\text{retr}}+\\rho_{\\text{pred}}) -zCDP due to overlapping parallel composition with L L overlaps. Finally, by sequentially composing M hist M_{\\text{hist}} and M clus M_{\\text{clus}} , the entire algorithm satisfies ρ \\rho -zCDP with ρ = ρ hist + L ( ρ retr + ρ pred ) \\rho=\\rho_{\\text{hist}}+L(\\rho_{\\text{retr}}+\\rho_{\\text{pred}}) . We then convert ρ \\rho -zCDP to ( ε , δ ) (\\varepsilon,\\delta) -DP using the conversion lemma Bun and Steinke ( 2016 ) . ∎",
            "7 Conclusion": "This study introduces DP-SynRAG, a novel framework for generating privacy-preserving synthetic texts for RAG that preserves both data utility and formal DP guarantees. By creating synthetic RAG databases, DP-SynRAG eliminates the need for repeated noise injection and enables unlimited query access within a fixed privacy budget. Experiments on multiple datasets show that DP-SynRAG consistently achieves performance better than existing private RAG methods in most cases, demonstrating its scalability and practical effectiveness.",
            "Limitations": "While DP-SynRAG shows strong performance and scalability for privacy-preserving RAG, several limitations remain. First, the method assumes that the RAG database contains multiple related documents to generate informative synthetic texts. When such redundancy is limited, the generated texts become less informative, reducing downstream performance. This limitation reflects the DP constraint that bounds each record’s influence. Moreover, because clustering relies on keyword overlap, it performs poorly when documents share few common terms. In practice, however, this issue is often mitigated, as documents on similar topics typically exhibit sufficient overlap. Second, like other DP-based text generation methods, DP-SynRAG experiences significant utility loss under an extremely tight privacy budget (e.g., ε total ≈ 1 \\varepsilon_{\\text{total}}\\approx 1 ) due to per-token privacy enforcement. Nonetheless, as shown in the main text, the per-token budget remains sufficiently small enough to keep information leakage negligible, as confirmed empirically. Third, our approach includes several hyperparameters that control clustering and noise calibration. However, as demonstrated in the Appendix C.3 , fixed default values perform consistently well across different models and datasets, minimizing the need for extensive tuning. Finally, when the RAG database is updated, the synthetic database must be regenerated to maintain privacy guarantees. This regeneration, however, does not require additional model retraining, keeping overall maintenance costs modest."
        },
        "Tuples": [
            [
                "1 Introduction",
                "Retrieval-Augmented Generation (RAG) Lewis et al. ( 2020 ) has been widely used to enhance the performance of large language models (LLMs) Gao et al. ( 2024 ) by leveraging external knowledge databases. However, recent studies have identified significant privacy risks in RAG systems when their databases contain sensitive information Zeng et al. ( 2024 ); Qi et al. ( 2025 ); Jiang et al. ( 2025 ); Maio et al. ( 2024 ); Wang et al. ( 2025b ) . For instance, extraction attacks against medical chatbots for patients or recommender systems for customers can expose private data to attackers. Moreover, sensitive information in the retrieved documents may also be inadvertently revealed to benign users during normal interactions through LLM outputs (Figure 1 ). To develop secure RAG systems, differential privacy (DP), a formal framework for protecting individual records, has begun to be used Grislain ( 2025 ); Koga et al. ( 2025 ); Wang et al. ( 2025a ) . Existing approaches ensures DP by injecting noise into the LLM’s responses to user queries, thereby reducing the influence of any single record. However, these methods must add noise to each response; thus, they consume the privacy budget proportionally to the number of queries. Consequently, in typical RAG scenarios involving many queries under a fixed privacy budget, the utility of responses degrades substantially. To avoid repeated noise injection, we propose a text generation method using LLMs, called D ifferentially P rivate Syn thetic text generation for RAG databases (DP-SynRAG). Once synthetic texts with DP guarantees are generated, they can be reused indefinitely as the RAG database without incurring additional privacy budget, regardless of the number of queries. To achieve high-quality private text generation, we adopt private prediction Hong et al. ( 2024 ); Tang et al. ( 2024 ); Amin et al. ( 2024 ); Gao et al. ( 2025 ) , which prompts the LLM with subsampled database records and rephrasing instructions, while perturbing the aggregated output token distributions to limit per-record information leakage. One limitation of these approaches is that they capture only global properties of the entire dataset, discarding not only sensitive details but also important knowledge needed for RAG. DP-SynRAG mitigates this issue by clustering documents in a DP manner based on keywords and document-level embeddings, thereby grouping semantically similar documents and separating distinct topics. Applying private prediction to these clusters in parallel enables large-scale synthetic text generation that preserves cluster-specific knowledge at low privacy cost. Finally, to reduce the effect of low-quality synthetic texts, we apply LLM-based self-filtering to improve downstream RAG performance. We validate our approach on three datasets tailored to our setting. Results show that our method outperforms existing private RAG approaches in most cases while maintaining a fixed privacy budget, demonstrating its effectiveness and scalability for privacy-preserving RAG applications."
            ],
            [
                "4 Proposed Method",
                "Existing approaches to our problem Grislain ( 2025 ); Koga et al. ( 2025 ) respond to user queries in RAG systems via private prediction mechanisms. However, when directly applied to multiple queries, their total privacy budget grows linearly with the number of queries. To address this, we propose DP-SynRAG, which generates synthetic data resembling the private data in advance through private prediction while ensuring DP. Since using this synthetic data as a knowledge corpus for later RAG inference is post-processing, the privacy budget remains fixed regardless of the number of queries. Current methods for generating synthetic data via private prediction often produce tokens that capture only the average characteristics of randomly subsampled subsets of the original data (see Section 3.3 ). While this supports domain-specific data generation, it often loses the fine-grained factual details useful as RAG context. In contrast, DP-SynRAG first clusters the dataset based on keywords and document-level embeddings under DP, grouping semantically similar documents and separating distinct topics. Applying private prediction to these subsets in parallel can generate large volumes of high-quality synthetic texts covering diverse topics in the original dataset at a low privacy budget."
            ],
            [
                "5 Privacy Analysis",
                "Our algorithm generates R R synthetic texts with minimal total privacy budget, using the overlapping parallel composition introduced by Smith et al. ( 2022 ) and converted to zCDP (Appendix B ). The formal privacy guarantee is stated below. We sketch the proof here and defer the full version to Appendix B . Theorem 1 . DP-SynRAG (Algorithm 1 in Appendix A ) satisfies ( ε , δ ) (\\varepsilon,\\delta) -DP for any δ > 0 \\delta>0 and ε = ρ + 4 ρ log ⁡ ( 1 / δ ) \\varepsilon=\\rho+\\sqrt{4\\rho\\log(1/\\delta)} , where ρ = K 2 σ h 2 + L ( 1 8 ε θ s 2 + 1 2 σ μ 2 + T 2 ( c τ ) 2 ) . \\rho=\\frac{K}{2\\sigma_{h}^{2}}+L\\left(\\frac{1}{8}\\varepsilon_{\\theta_{s}}^{2}+\\frac{1}{2\\sigma_{\\mu}^{2}}+\\frac{T}{2}\\left(\\frac{c}{\\tau}\\right)^{2}\\right). Proof Overview.. We adopt zero-concentrated differential privacy (zCDP) Bun and Steinke ( 2016 ) , a variant of DP, as it provides tighter composition bounds and more precise privacy accounting. Our algorithm comprises two sequentially composed mechanisms: (a) histogram generation ( M hist M_{\\text{hist}} ) and (b-d) keyword-based clustering followed by parallel operations on each cluster ( M clus M_{\\text{clus}} ). Within M clus M_{\\text{clus}} , each cluster undergoes a sequence of operations: (c) retrieval ( M retr M_{\\text{retr}} ), (d) private prediction ( M pred M_{\\text{pred}} ). We exclude self-filtering from the privacy analysis since it a post-processing. Our proof proceeds in three steps. First, we show that M hist M_{\\text{hist}} , M retr M_{\\text{retr}} , and M pred M_{\\text{pred}} each satisfy ρ hist \\rho_{\\text{hist}} , ρ retr \\rho_{\\text{retr}} , and ρ pred \\rho_{\\text{pred}} -zCDP, respectively, since they rely on Gaussian or exponential mechanisms (or their compositions). Next, each algorithm executed in parallel within a cluster satisfies ( ρ retr + ρ pred ) (\\rho_{\\text{retr}}+\\rho_{\\text{pred}}) -zCDP by sequential composition, and M clus M_{\\text{clus}} satisfies L ( ρ retr + ρ pred ) L(\\rho_{\\text{retr}}+\\rho_{\\text{pred}}) -zCDP due to overlapping parallel composition with L L overlaps. Finally, by sequentially composing M hist M_{\\text{hist}} and M clus M_{\\text{clus}} , the entire algorithm satisfies ρ \\rho -zCDP with ρ = ρ hist + L ( ρ retr + ρ pred ) \\rho=\\rho_{\\text{hist}}+L(\\rho_{\\text{retr}}+\\rho_{\\text{pred}}) . We then convert ρ \\rho -zCDP to ( ε , δ ) (\\varepsilon,\\delta) -DP using the conversion lemma Bun and Steinke ( 2016 ) . ∎"
            ],
            [
                "7 Conclusion",
                "This study introduces DP-SynRAG, a novel framework for generating privacy-preserving synthetic texts for RAG that preserves both data utility and formal DP guarantees. By creating synthetic RAG databases, DP-SynRAG eliminates the need for repeated noise injection and enables unlimited query access within a fixed privacy budget. Experiments on multiple datasets show that DP-SynRAG consistently achieves performance better than existing private RAG methods in most cases, demonstrating its scalability and practical effectiveness."
            ],
            [
                "Limitations",
                "While DP-SynRAG shows strong performance and scalability for privacy-preserving RAG, several limitations remain. First, the method assumes that the RAG database contains multiple related documents to generate informative synthetic texts. When such redundancy is limited, the generated texts become less informative, reducing downstream performance. This limitation reflects the DP constraint that bounds each record’s influence. Moreover, because clustering relies on keyword overlap, it performs poorly when documents share few common terms. In practice, however, this issue is often mitigated, as documents on similar topics typically exhibit sufficient overlap. Second, like other DP-based text generation methods, DP-SynRAG experiences significant utility loss under an extremely tight privacy budget (e.g., ε total ≈ 1 \\varepsilon_{\\text{total}}\\approx 1 ) due to per-token privacy enforcement. Nonetheless, as shown in the main text, the per-token budget remains sufficiently small enough to keep information leakage negligible, as confirmed empirically. Third, our approach includes several hyperparameters that control clustering and noise calibration. However, as demonstrated in the Appendix C.3 , fixed default values perform consistently well across different models and datasets, minimizing the need for extensive tuning. Finally, when the RAG database is updated, the synthetic database must be regenerated to maintain privacy guarantees. This regeneration, however, does not require additional model retraining, keeping overall maintenance costs modest."
            ]
        ],
        "section_summaries": [
            [
                "1 Introduction",
                "Retrieval-Augmented Generation (RAG) Lewis et al. ( 2020 ) has been widely used to enhance the performance of large language models (LLMs) Gao et al. ( 2024 ) by leveraging external knowledge databases. However, recent studies have identified significant privacy risks in RAG systems when their databases contain sensitive information Zeng et al. ( 2024 ); Qi et al. ( 2025 ); Jiang et al. ( 2025 ); Maio et al. ( 2024 ); Wang et al. ( 2025b ) . For instance, extraction attacks against medical chatbots for patients or recommender systems for customers can expose private data to attackers. Moreover, sensitive information in the retrieved documents may also be inadvertently revealed to benign users during normal interactions through LLM outputs (Figure 1 ). To develop secure RAG systems, differential privacy (DP), a formal framework for protecting individual records, has begun to be used Grislain ( 2025 ); Koga et al. ( 2025 ); Wang et al. ( 2025a ) . Existing approaches ensures DP by injecting noise into the LLM’s responses to user queries, thereby reducing the influence of any single record. However, these methods must add noise to each response; thus, they consume the privacy budget proportionally to the number of queries. Consequently, in typical RAG scenarios involving many queries under a fixed privacy budget, the utility of responses degrades substantially. To avoid repeated noise injection, we propose a text generation method using LLMs, called D ifferentially P rivate Syn thetic text generation for RAG databases (DP-SynRAG). Once synthetic texts with DP guarantees are generated, they can be reused indefinitely as the RAG database without incurring additional privacy budget, regardless of the number of queries. To achieve high-quality private text generation, we adopt private prediction Hong et al. ( 2024 ); Tang et al. ( 2024 ); Amin et al. ( 2024 ); Gao et al. ( 2025 ) , which prompts the LLM with subsampled database records and rephrasing instructions, while perturbing the aggregated output token distributions to limit per-record information leakage. One limitation of these approaches is that they capture only global properties of the entire dataset, discarding not only sensitive details but also important knowledge needed for RAG. DP-SynRAG mitigates this issue by clustering documents in a DP manner based on keywords and document-level embeddings, thereby grouping semantically similar documents and separating distinct topics. Applying private prediction to these clusters in parallel enables large-scale synthetic text generation that preserves cluster-specific knowledge at low privacy cost. Finally, to reduce the effect of low-quality synthetic texts, we apply LLM-based self-filtering to improve downstream RAG performance. We validate our approach on three datasets tailored to our setting. Results show that our method outperforms existing private RAG approaches in most cases while maintaining a fixed privacy budget, demonstrating its effectiveness and scalability for privacy-preserving RAG applications.",
                "Lewis et al. (2020) introduced Retrieval-Augmented Generation (RAG), but recent studies found significant privacy risks when databases contain sensitive information. Differential privacy (DP) was proposed to protect individual records, but existing methods add noise to each response, consuming the privacy budget proportionally. We propose DP-SynRAG, a method that generates synthetic texts with DP guarantees, reusing them indefinitely without incurring additional privacy budget. To achieve high-quality private text generation, we adopt private prediction and apply it to clusters of documents based on keywords and document-level embeddings. Our approach outperforms existing private RAG methods while maintaining a fixed privacy budget, demonstrating its effectiveness and scalability for privacy-preserving RAG applications.\n\nSummary of mentioned numbers:\n- 2020 (Lewis et al.)\n- 2024 (multiple citations)\n- 2025 (multiple citations)\n\nSummary of mentioned names:\n- Lewis et al.\n- Gao et al. (multiple times)\n- Zeng et al.\n- Qi et al.\n- Jiang et al.\n- Maio et al.\n- Wang et al. (multiple times)\n- Grislain\n- Koga et al.\n- Hong et al.\n- Tang et al.\n- Amin et al.\n\nSummary of mentioned abbreviations:\n- LLM (Large Language Models)\n- RAG (Retrieval-Augmented Generation)\n- DP (Differential Privacy)"
            ],
            [
                "4 Proposed Method",
                "Existing approaches to our problem Grislain ( 2025 ); Koga et al. ( 2025 ) respond to user queries in RAG systems via private prediction mechanisms. However, when directly applied to multiple queries, their total privacy budget grows linearly with the number of queries. To address this, we propose DP-SynRAG, which generates synthetic data resembling the private data in advance through private prediction while ensuring DP. Since using this synthetic data as a knowledge corpus for later RAG inference is post-processing, the privacy budget remains fixed regardless of the number of queries. Current methods for generating synthetic data via private prediction often produce tokens that capture only the average characteristics of randomly subsampled subsets of the original data (see Section 3.3 ). While this supports domain-specific data generation, it often loses the fine-grained factual details useful as RAG context. In contrast, DP-SynRAG first clusters the dataset based on keywords and document-level embeddings under DP, grouping semantically similar documents and separating distinct topics. Applying private prediction to these subsets in parallel can generate large volumes of high-quality synthetic texts covering diverse topics in the original dataset at a low privacy budget.",
                "Grislain (2025) and Koga et al. (2025) propose methods for RAG systems with private prediction mechanisms, which have growing total privacy budgets with multiple queries. DP-SynRAG generates synthetic data through private prediction while ensuring differential privacy, using this as a fixed knowledge corpus for later inference, regardless of query number. \n\nKey dates: \n- 2025 (Grislain)\n- 2025 (Koga et al.)"
            ],
            [
                "5 Privacy Analysis",
                "Our algorithm generates R R synthetic texts with minimal total privacy budget, using the overlapping parallel composition introduced by Smith et al. ( 2022 ) and converted to zCDP (Appendix B ). The formal privacy guarantee is stated below. We sketch the proof here and defer the full version to Appendix B . Theorem 1 . DP-SynRAG (Algorithm 1 in Appendix A ) satisfies ( ε , δ ) (\\varepsilon,\\delta) -DP for any δ > 0 \\delta>0 and ε = ρ + 4 ρ log ⁡ ( 1 / δ ) \\varepsilon=\\rho+\\sqrt{4\\rho\\log(1/\\delta)} , where ρ = K 2 σ h 2 + L ( 1 8 ε θ s 2 + 1 2 σ μ 2 + T 2 ( c τ ) 2 ) . \\rho=\\frac{K}{2\\sigma_{h}^{2}}+L\\left(\\frac{1}{8}\\varepsilon_{\\theta_{s}}^{2}+\\frac{1}{2\\sigma_{\\mu}^{2}}+\\frac{T}{2}\\left(\\frac{c}{\\tau}\\right)^{2}\\right). Proof Overview.. We adopt zero-concentrated differential privacy (zCDP) Bun and Steinke ( 2016 ) , a variant of DP, as it provides tighter composition bounds and more precise privacy accounting. Our algorithm comprises two sequentially composed mechanisms: (a) histogram generation ( M hist M_{\\text{hist}} ) and (b-d) keyword-based clustering followed by parallel operations on each cluster ( M clus M_{\\text{clus}} ). Within M clus M_{\\text{clus}} , each cluster undergoes a sequence of operations: (c) retrieval ( M retr M_{\\text{retr}} ), (d) private prediction ( M pred M_{\\text{pred}} ). We exclude self-filtering from the privacy analysis since it a post-processing. Our proof proceeds in three steps. First, we show that M hist M_{\\text{hist}} , M retr M_{\\text{retr}} , and M pred M_{\\text{pred}} each satisfy ρ hist \\rho_{\\text{hist}} , ρ retr \\rho_{\\text{retr}} , and ρ pred \\rho_{\\text{pred}} -zCDP, respectively, since they rely on Gaussian or exponential mechanisms (or their compositions). Next, each algorithm executed in parallel within a cluster satisfies ( ρ retr + ρ pred ) (\\rho_{\\text{retr}}+\\rho_{\\text{pred}}) -zCDP by sequential composition, and M clus M_{\\text{clus}} satisfies L ( ρ retr + ρ pred ) L(\\rho_{\\text{retr}}+\\rho_{\\text{pred}}) -zCDP due to overlapping parallel composition with L L overlaps. Finally, by sequentially composing M hist M_{\\text{hist}} and M clus M_{\\text{clus}} , the entire algorithm satisfies ρ \\rho -zCDP with ρ = ρ hist + L ( ρ retr + ρ pred ) \\rho=\\rho_{\\text{hist}}+L(\\rho_{\\text{retr}}+\\rho_{\\text{pred}}) . We then convert ρ \\rho -zCDP to ( ε , δ ) (\\varepsilon,\\delta) -DP using the conversion lemma Bun and Steinke ( 2016 ) . ∎",
                "Smith et al. (2022) introduced overlapping parallel composition, which is used by our algorithm to generate R synthetic texts with minimal total privacy budget. The formal privacy guarantee states that DP-SynRAG satisfies (ε, δ)-DP for any δ > 0 and ε = ρ + 4ρlog(1/δ), where ρ = K2σh2 + L(1/8εθs2 + 1/2σμ2 + T2(cτ)2). The algorithm comprises two sequentially composed mechanisms: histogram generation (Mhist) and keyword-based clustering followed by parallel operations on each cluster (Mclus)."
            ],
            [
                "7 Conclusion",
                "This study introduces DP-SynRAG, a novel framework for generating privacy-preserving synthetic texts for RAG that preserves both data utility and formal DP guarantees. By creating synthetic RAG databases, DP-SynRAG eliminates the need for repeated noise injection and enables unlimited query access within a fixed privacy budget. Experiments on multiple datasets show that DP-SynRAG consistently achieves performance better than existing private RAG methods in most cases, demonstrating its scalability and practical effectiveness.",
                "DP-SynRAG framework preserves data utility and formal DP guarantees. It eliminates repeated noise injection and enables unlimited query access within a fixed privacy budget. Experiments on multiple datasets show that DP-SynRAG achieves better performance than existing private RAG methods in most cases."
            ],
            [
                "Limitations",
                "While DP-SynRAG shows strong performance and scalability for privacy-preserving RAG, several limitations remain. First, the method assumes that the RAG database contains multiple related documents to generate informative synthetic texts. When such redundancy is limited, the generated texts become less informative, reducing downstream performance. This limitation reflects the DP constraint that bounds each record’s influence. Moreover, because clustering relies on keyword overlap, it performs poorly when documents share few common terms. In practice, however, this issue is often mitigated, as documents on similar topics typically exhibit sufficient overlap. Second, like other DP-based text generation methods, DP-SynRAG experiences significant utility loss under an extremely tight privacy budget (e.g., ε total ≈ 1 \\varepsilon_{\\text{total}}\\approx 1 ) due to per-token privacy enforcement. Nonetheless, as shown in the main text, the per-token budget remains sufficiently small enough to keep information leakage negligible, as confirmed empirically. Third, our approach includes several hyperparameters that control clustering and noise calibration. However, as demonstrated in the Appendix C.3 , fixed default values perform consistently well across different models and datasets, minimizing the need for extensive tuning. Finally, when the RAG database is updated, the synthetic database must be regenerated to maintain privacy guarantees. This regeneration, however, does not require additional model retraining, keeping overall maintenance costs modest.",
                "DP-SynRAG has several limitations: \n- It requires multiple related documents in the RAG database for informative synthetic texts, degrading performance otherwise.\n- Clustering performs poorly with few common terms between documents.\n- Tight privacy budget (ε total ≈ 1) leads to significant utility loss.\n- Default hyperparameter values perform well across models and datasets.\n- Regenerating the synthetic database after RAG updates is necessary for maintaining privacy guarantees."
            ]
        ],
        "general_summary": "**Research Overview**\nThis research addresses the problem of privacy risks in Retrieval-Augmented Generation (RAG) systems, particularly when dealing with sensitive information. The proposed solution, DP-SynRAG, uses large language models to generate differentially private synthetic RAG databases, eliminating repeated noise injection and enabling unlimited query access within a fixed privacy budget.\n\n**Methodology & Approach**\nDP-SynRAG generates synthetic data through private prediction while ensuring differential privacy, using this as a fixed knowledge corpus for later inference. The approach consists of two sequentially composed mechanisms: histogram generation (Mhist) and keyword-based clustering followed by parallel operations on each cluster (Mclus). Private prediction is applied to clusters of documents based on keywords and document-level embeddings.\n\n**Key Contributions & Findings**\nDP-SynRAG achieves superior performance to state-of-the-art private RAG systems while maintaining a fixed privacy budget, demonstrating its effectiveness and scalability for privacy-preserving RAG applications. Experiments show that DP-SynRAG outperforms existing private RAG methods in most cases.\n\n**Technical Details**\nThe algorithm comprises two sequentially composed mechanisms: histogram generation (Mhist) and keyword-based clustering followed by parallel operations on each cluster (Mclus). Private prediction is applied to clusters of documents based on keywords and document-level embeddings. The formal privacy guarantee states that DP-SynRAG satisfies (ε, δ)-DP for any δ > 0 and ε = ρ + 4ρlog(1/δ), where ρ = K2σh2 + L(1/8εθs2 + 1/2σμ2 + T2(cτ)2).\n\n**Limitations & Future Work**\nDP-SynRAG has several limitations: (1) it requires multiple related documents in the RAG database for informative synthetic texts, degrading performance otherwise; (2) clustering performs poorly with few common terms between documents. Future work should focus on addressing these limitations and exploring alternative approaches.\n\n**Practical Implications**\nThe proposed solution, DP-SynRAG, has significant practical implications for privacy-preserving RAG applications, enabling unlimited query access within a fixed privacy budget while maintaining data utility and formal DP guarantees."
    },
    {
        "id": "2502.11113v2",
        "title": "Valuable Hallucinations: Realizable Non-realistic Propositions",
        "authors": [
            "Qiucheng Chen",
            "Bo Wang"
        ],
        "url": "https://arxiv.org/abs/2502.11113v2",
        "Abstract": "Abstract This paper introduces the first formal definition of valuable hallucinations in large language models (LLMs), addressing a gap in the existing literature. We provide a systematic definition and analysis of hallucination value, proposing methods for enhancing the value of hallucinations. In contrast to previous works, which often treat hallucinations as a broad flaw, we focus on the potential value that certain types of hallucinations can offer in specific contexts. Hallucinations in LLMs generally refer to the generation of unfaithful, fabricated, inconsistent, or nonsensical content. Rather than viewing all hallucinations negatively, this paper gives formal representations and manual judgments of \"valuable hallucinations\" and explores how realizable non-realistic propositions—ideas that are not currently true but could be achievable under certain conditions—can have constructive value. We present experiments using the Qwen2.5 model and HalluQA dataset, employing ReAct prompting (which involves reasoning, confidence assessment, and answer verification) to control and optimize hallucinations. Our findings show that ReAct prompting results in a 5.12% reduction in overall hallucinations and an increase in the proportion of valuable hallucinations from 6.45% to 7.92%. These results demonstrate that systematically controlling hallucinations can improve their usefulness without compromising factual reliability. 1 1 1 The paper uses an AI assistant to refine the expression of certain sections, but the research and coding parts of the paper were entirely conducted without the use of AI.",
        "Main": {
            "3 Methodology": "In this section, we outline the methodology used to explore and control hallucinations in large language models (LLMs), with a focus on increasing the proportion of valuable hallucinations. Our approach combines prompt engineering, reflection techniques, and other advanced methods such as retrieval-augmented generation (RAG) and meta-learning. The goal is not to eliminate hallucinations entirely but to control them in a way that maximizes their potential value. That is to say, we need to increase the proportion of \"valuable hallucinations\" in the hallucinations, not to increase the proportion of hallucinations in the LLM-generated content.",
            "4 Conclusion": "In this paper, we have explored the concept of valuable hallucinations in large language models (LLMs) and demonstrated that not all hallucinations are detrimental. By redefining hallucinations as realizable but non-realistic propositions , we have shown that certain types of hallucinations can provide innovative and inspiring ideas, offering new perspectives or solutions to real-world problems. Our work challenges the prevailing view that hallucinations are entirely harmful and provides a framework for identifying and utilizing their potential value.",
            "5 Limitations": "While this paper provides a foundation for understanding and utilizing valuable hallucinations in large language models (LLMs), there are several limitations that need to be acknowledged. These limitations highlight areas for future research and improvement."
        },
        "Tuples": [
            [
                "3 Methodology",
                "In this section, we outline the methodology used to explore and control hallucinations in large language models (LLMs), with a focus on increasing the proportion of valuable hallucinations. Our approach combines prompt engineering, reflection techniques, and other advanced methods such as retrieval-augmented generation (RAG) and meta-learning. The goal is not to eliminate hallucinations entirely but to control them in a way that maximizes their potential value. That is to say, we need to increase the proportion of \"valuable hallucinations\" in the hallucinations, not to increase the proportion of hallucinations in the LLM-generated content."
            ],
            [
                "4 Conclusion",
                "In this paper, we have explored the concept of valuable hallucinations in large language models (LLMs) and demonstrated that not all hallucinations are detrimental. By redefining hallucinations as realizable but non-realistic propositions , we have shown that certain types of hallucinations can provide innovative and inspiring ideas, offering new perspectives or solutions to real-world problems. Our work challenges the prevailing view that hallucinations are entirely harmful and provides a framework for identifying and utilizing their potential value."
            ],
            [
                "5 Limitations",
                "While this paper provides a foundation for understanding and utilizing valuable hallucinations in large language models (LLMs), there are several limitations that need to be acknowledged. These limitations highlight areas for future research and improvement."
            ]
        ],
        "section_summaries": [
            [
                "3 Methodology",
                "In this section, we outline the methodology used to explore and control hallucinations in large language models (LLMs), with a focus on increasing the proportion of valuable hallucinations. Our approach combines prompt engineering, reflection techniques, and other advanced methods such as retrieval-augmented generation (RAG) and meta-learning. The goal is not to eliminate hallucinations entirely but to control them in a way that maximizes their potential value. That is to say, we need to increase the proportion of \"valuable hallucinations\" in the hallucinations, not to increase the proportion of hallucinations in the LLM-generated content.",
                "Methodology combines prompt engineering, reflection techniques, retrieval-augmented generation (RAG), and meta-learning to control \"valuable hallucinations\" in large language models."
            ],
            [
                "4 Conclusion",
                "In this paper, we have explored the concept of valuable hallucinations in large language models (LLMs) and demonstrated that not all hallucinations are detrimental. By redefining hallucinations as realizable but non-realistic propositions , we have shown that certain types of hallucinations can provide innovative and inspiring ideas, offering new perspectives or solutions to real-world problems. Our work challenges the prevailing view that hallucinations are entirely harmful and provides a framework for identifying and utilizing their potential value.",
                "Valuable hallucinations in LLMs are not always detrimental. Hallucinations can be realizable but non-realistic propositions, providing innovative ideas and new perspectives on problems."
            ],
            [
                "5 Limitations",
                "While this paper provides a foundation for understanding and utilizing valuable hallucinations in large language models (LLMs), there are several limitations that need to be acknowledged. These limitations highlight areas for future research and improvement.",
                "This paper has 5 limitations that require future research."
            ]
        ],
        "general_summary": "**Research Overview**\n\nThe research addresses the gap in existing literature by introducing a formal definition of \"valuable hallucinations\" in large language models (LLMs). Valuable hallucinations refer to realizable non-realistic propositions that can provide innovative ideas and new perspectives on problems.\n\n**Methodology & Approach**\n\nThe proposed solution combines prompt engineering, reflection techniques, retrieval-augmented generation (RAG), and meta-learning to control \"valuable hallucinations\" in LLMs. The Qwen2.5 model and HalluQA dataset are employed for experiments, using ReAct prompting to optimize hallucinations.\n\n**Key Contributions & Findings**\n\nThe research presents a 5.12% reduction in overall hallucinations and an increase in the proportion of valuable hallucinations from 6.45% to 7.92%. These results demonstrate that systematically controlling hallucinations can improve their usefulness without compromising factual reliability.\n\n**Technical Details**\n\nReAct prompting involves reasoning, confidence assessment, and answer verification techniques. The paper employs a meta-learning approach to control and optimize hallucinations, utilizing RAG and reflection techniques for retrieval-augmented generation.\n\n**Limitations & Future Work**\n\nThe research has 5 limitations that require future investigation, including the need for more extensive experimentation and the development of more sophisticated methods for controlling valuable hallucinations.\n\n**Practical Implications**\n\nThis research can inform the development of LLMs that balance innovation and factual reliability. By systematically controlling hallucinations, it is possible to harness their potential value while minimizing their negative impact on factual accuracy."
    },
    {
        "id": "2503.07833v1",
        "title": "HalluVerse25: Fine-grained Multilingual Benchmark Dataset for LLM\n  Hallucinations",
        "authors": [
            "Samir Abdaljalil",
            "Hasan Kurban",
            "Erchin Serpedin"
        ],
        "url": "https://arxiv.org/abs/2503.07833v1",
        "Abstract": "",
        "Main": {
            "1 Introduction": "Large Language Models (LLMs) have demonstrated promising results in various natural language processing (NLP) tasks, including summarization Liu et al. ( 2024 ); Ramprasad et al. ( 2024 ) , machine translation Zhu et al. ( 2024 ) , question answering Kamalloo et al. ( 2023 ) , and many more. Despite this, the phenomenon of producing fabricated and nonfactual content, known as \"hallucinations”, has also been on the rise Banerjee et al. ( 2024 ) . Hallucinations can lead to the spread of misinformation, as they can sometimes appear to be plausible, particularly to individuals who are unfamiliar with the subject matter being generated Berberette et al. ( 2024 ) . As a result, analyzing and detecting hallucinations has become a major focus in the literature. Several datasets have been developed to understand and analyze hallucinations generated by LLMs Mubarak et al. ( 2024 ); Li et al. ( 2023 ); Manakul et al. ( 2023 ); Mishra et al. ( 2024 ) . Existing datasets, however, mostly consist of monolingual data, primarily in English, and often present hallucination in a binary classification setting. Such datasets lack the fine-grained annotation necessary for a deeper understanding of hallucination types. To fill this gap, we introduce HalluVerse25 , a fine-grained multilingual LLM hallucination dataset. Our main contributions are as follows: • We introduce the first multilingual fine-grained LLM hallucination dataset. The dataset spans across three languages, namely English, Arabic, and Turkish, with 1310, 828, and 978 data samples, respectively. This dataset is made publicly available to advance research in the field 1 1 1 Available Upon Request . • We detail a comprehensive and reproducible dataset creation and annotation protocol. • We perform a detailed analysis of the performance of proprietary LLMs on detecting fine-grained hallucinations in multiple settings. The remainder of the paper is organized as follows: Section 2 discusses relevant literature on hallucination datasets and benchmarks. Section 3 details the dataset construction methodology. Section 4 presents a comprehensive analysis of the dataset. Section 5 discusses experimental results of several LLMs on the dataset. Finally, Section 6 concludes the paper and outlines future directions.",
            "3 Constructing HalluVerse25": "The main objective behind the construction of this dataset is to understand what different types of hallucinations look like in multiple languages. The dataset includes the original factual sentences and their hallucinated versions, as well as the hallucination type associated with each edited sentence. The construction pipeline is illustrated in Fig. 1 . Furthermore, samples from the final dataset are shown in Table 1 .",
            "6 Conclusion": "In this paper, we present HalluVerse25 , a fine-grained multilingual dataset for LLM-generated hallucinations, constructed by injecting hallucinated content using LLMs. To create this dataset, we collect biographical data from well-known figures from Wikipedia and extract factual sentences that are then used as input for an LLM to generate hallucinated text. HalluVerse25 is the first self-contained multilingual dataset that categorizes fine-grained hallucination types across multiple languages. To ensure high data quality, we implement a rigorous human annotation protocol for verification. Additionally, we conduct a detailed evaluation of LLM performance on the dataset, providing insights into how well these models identify different types of hallucinations in multilingual settings. We hope that HalluVerse25 will serve as a valuable resource for advancing research on LLM hallucinations in diverse linguistic contexts.",
            "7 Limitations": "The data set is collected on the basis of the availability of biographical information hosted on Wikidata and Wikipedia. Although we show in Section 4 that there is a decent amount of representation in terms of professions and countries, we must acknowledge the fact that it is unlikely that such platforms represent an accurate distribution of individuals in the world. For future work, our aim is to fill this gap by controlling the distribution in the data extraction process to clearly ensure a fair and distributed representation in the output. In addition, this dataset focuses on biographical data. Although this is a promising start in terms of creating multilingual benchmarks for fine-grained hallucination in LLMs, it is important to improve the versatility of the publicly available datasets by including other tasks as well, such as summarization, question answering, and more. Future work will build on this by exploring several multilingual tasks to add to this benchmark."
        },
        "Tuples": [
            [
                "1 Introduction",
                "Large Language Models (LLMs) have demonstrated promising results in various natural language processing (NLP) tasks, including summarization Liu et al. ( 2024 ); Ramprasad et al. ( 2024 ) , machine translation Zhu et al. ( 2024 ) , question answering Kamalloo et al. ( 2023 ) , and many more. Despite this, the phenomenon of producing fabricated and nonfactual content, known as \"hallucinations”, has also been on the rise Banerjee et al. ( 2024 ) . Hallucinations can lead to the spread of misinformation, as they can sometimes appear to be plausible, particularly to individuals who are unfamiliar with the subject matter being generated Berberette et al. ( 2024 ) . As a result, analyzing and detecting hallucinations has become a major focus in the literature. Several datasets have been developed to understand and analyze hallucinations generated by LLMs Mubarak et al. ( 2024 ); Li et al. ( 2023 ); Manakul et al. ( 2023 ); Mishra et al. ( 2024 ) . Existing datasets, however, mostly consist of monolingual data, primarily in English, and often present hallucination in a binary classification setting. Such datasets lack the fine-grained annotation necessary for a deeper understanding of hallucination types. To fill this gap, we introduce HalluVerse25 , a fine-grained multilingual LLM hallucination dataset. Our main contributions are as follows: • We introduce the first multilingual fine-grained LLM hallucination dataset. The dataset spans across three languages, namely English, Arabic, and Turkish, with 1310, 828, and 978 data samples, respectively. This dataset is made publicly available to advance research in the field 1 1 1 Available Upon Request . • We detail a comprehensive and reproducible dataset creation and annotation protocol. • We perform a detailed analysis of the performance of proprietary LLMs on detecting fine-grained hallucinations in multiple settings. The remainder of the paper is organized as follows: Section 2 discusses relevant literature on hallucination datasets and benchmarks. Section 3 details the dataset construction methodology. Section 4 presents a comprehensive analysis of the dataset. Section 5 discusses experimental results of several LLMs on the dataset. Finally, Section 6 concludes the paper and outlines future directions."
            ],
            [
                "3 Constructing HalluVerse25",
                "The main objective behind the construction of this dataset is to understand what different types of hallucinations look like in multiple languages. The dataset includes the original factual sentences and their hallucinated versions, as well as the hallucination type associated with each edited sentence. The construction pipeline is illustrated in Fig. 1 . Furthermore, samples from the final dataset are shown in Table 1 ."
            ],
            [
                "6 Conclusion",
                "In this paper, we present HalluVerse25 , a fine-grained multilingual dataset for LLM-generated hallucinations, constructed by injecting hallucinated content using LLMs. To create this dataset, we collect biographical data from well-known figures from Wikipedia and extract factual sentences that are then used as input for an LLM to generate hallucinated text. HalluVerse25 is the first self-contained multilingual dataset that categorizes fine-grained hallucination types across multiple languages. To ensure high data quality, we implement a rigorous human annotation protocol for verification. Additionally, we conduct a detailed evaluation of LLM performance on the dataset, providing insights into how well these models identify different types of hallucinations in multilingual settings. We hope that HalluVerse25 will serve as a valuable resource for advancing research on LLM hallucinations in diverse linguistic contexts."
            ],
            [
                "7 Limitations",
                "The data set is collected on the basis of the availability of biographical information hosted on Wikidata and Wikipedia. Although we show in Section 4 that there is a decent amount of representation in terms of professions and countries, we must acknowledge the fact that it is unlikely that such platforms represent an accurate distribution of individuals in the world. For future work, our aim is to fill this gap by controlling the distribution in the data extraction process to clearly ensure a fair and distributed representation in the output. In addition, this dataset focuses on biographical data. Although this is a promising start in terms of creating multilingual benchmarks for fine-grained hallucination in LLMs, it is important to improve the versatility of the publicly available datasets by including other tasks as well, such as summarization, question answering, and more. Future work will build on this by exploring several multilingual tasks to add to this benchmark."
            ]
        ],
        "section_summaries": [
            [
                "1 Introduction",
                "Large Language Models (LLMs) have demonstrated promising results in various natural language processing (NLP) tasks, including summarization Liu et al. ( 2024 ); Ramprasad et al. ( 2024 ) , machine translation Zhu et al. ( 2024 ) , question answering Kamalloo et al. ( 2023 ) , and many more. Despite this, the phenomenon of producing fabricated and nonfactual content, known as \"hallucinations”, has also been on the rise Banerjee et al. ( 2024 ) . Hallucinations can lead to the spread of misinformation, as they can sometimes appear to be plausible, particularly to individuals who are unfamiliar with the subject matter being generated Berberette et al. ( 2024 ) . As a result, analyzing and detecting hallucinations has become a major focus in the literature. Several datasets have been developed to understand and analyze hallucinations generated by LLMs Mubarak et al. ( 2024 ); Li et al. ( 2023 ); Manakul et al. ( 2023 ); Mishra et al. ( 2024 ) . Existing datasets, however, mostly consist of monolingual data, primarily in English, and often present hallucination in a binary classification setting. Such datasets lack the fine-grained annotation necessary for a deeper understanding of hallucination types. To fill this gap, we introduce HalluVerse25 , a fine-grained multilingual LLM hallucination dataset. Our main contributions are as follows: • We introduce the first multilingual fine-grained LLM hallucination dataset. The dataset spans across three languages, namely English, Arabic, and Turkish, with 1310, 828, and 978 data samples, respectively. This dataset is made publicly available to advance research in the field 1 1 1 Available Upon Request . • We detail a comprehensive and reproducible dataset creation and annotation protocol. • We perform a detailed analysis of the performance of proprietary LLMs on detecting fine-grained hallucinations in multiple settings. The remainder of the paper is organized as follows: Section 2 discusses relevant literature on hallucination datasets and benchmarks. Section 3 details the dataset construction methodology. Section 4 presents a comprehensive analysis of the dataset. Section 5 discusses experimental results of several LLMs on the dataset. Finally, Section 6 concludes the paper and outlines future directions.",
                "Liu et al., Ramprasad et al., Zhu et al., Kamalloo et al., Banerjee et al., Berberette et al., Mubarak et al., Li et al., Manakul et al., Mishra et al. (2023-2024).\nHalluVerse25, a multilingual LLM hallucination dataset spanning English, Arabic, and Turkish with 1310, 828, and 978 data samples respectively.\nProprietary LLMs performance on detecting fine-grained hallucinations in multiple settings analyzed."
            ],
            [
                "3 Constructing HalluVerse25",
                "The main objective behind the construction of this dataset is to understand what different types of hallucinations look like in multiple languages. The dataset includes the original factual sentences and their hallucinated versions, as well as the hallucination type associated with each edited sentence. The construction pipeline is illustrated in Fig. 1 . Furthermore, samples from the final dataset are shown in Table 1 .",
                "The main objective is to understand hallucinations in multiple languages using original factual sentences and their hallucinated versions, with associated types of hallucination.\n\nNote: HalluVerse25 construction pipeline is illustrated in Fig. 1, with samples shown in Table 1."
            ],
            [
                "6 Conclusion",
                "In this paper, we present HalluVerse25 , a fine-grained multilingual dataset for LLM-generated hallucinations, constructed by injecting hallucinated content using LLMs. To create this dataset, we collect biographical data from well-known figures from Wikipedia and extract factual sentences that are then used as input for an LLM to generate hallucinated text. HalluVerse25 is the first self-contained multilingual dataset that categorizes fine-grained hallucination types across multiple languages. To ensure high data quality, we implement a rigorous human annotation protocol for verification. Additionally, we conduct a detailed evaluation of LLM performance on the dataset, providing insights into how well these models identify different types of hallucinations in multilingual settings. We hope that HalluVerse25 will serve as a valuable resource for advancing research on LLM hallucinations in diverse linguistic contexts.",
                "HalluVerse25 is a fine-grained multilingual dataset with 3 key components: \n- Collected biographical data from Wikipedia, \n- Injected hallucinated content using LLMs, \n- Categorized by type across multiple languages."
            ],
            [
                "7 Limitations",
                "The data set is collected on the basis of the availability of biographical information hosted on Wikidata and Wikipedia. Although we show in Section 4 that there is a decent amount of representation in terms of professions and countries, we must acknowledge the fact that it is unlikely that such platforms represent an accurate distribution of individuals in the world. For future work, our aim is to fill this gap by controlling the distribution in the data extraction process to clearly ensure a fair and distributed representation in the output. In addition, this dataset focuses on biographical data. Although this is a promising start in terms of creating multilingual benchmarks for fine-grained hallucination in LLMs, it is important to improve the versatility of the publicly available datasets by including other tasks as well, such as summarization, question answering, and more. Future work will build on this by exploring several multilingual tasks to add to this benchmark.",
                "The dataset is collected from Wikidata and Wikipedia, but its distribution may not accurately represent the world's population. The focus is on biographical data, which can be improved upon by adding other tasks like summarization and question answering. Future work aims to address these limitations."
            ]
        ],
        "general_summary": "**Research Overview**\nHalluVerse25 is a fine-grained multilingual benchmark dataset designed to assess the performance of Large Language Models (LLMs) in detecting hallucinations across multiple languages, including English, Arabic, and Turkish.\n\n**Methodology & Approach**\nThe HalluVerse25 dataset was constructed using a pipeline that includes collecting original factual sentences from Wikipedia and injecting hallucinated content using LLMs. The data is categorized by type of hallucination across multiple languages. Specifically, 1310, 828, and 978 data samples are available for English, Arabic, and Turkish respectively.\n\n**Key Contributions & Findings**\nHalluVerse25 provides a fine-grained multilingual dataset with three key components: collected biographical data from Wikipedia, injected hallucinated content using LLMs, and categorized by type across multiple languages. This dataset is specifically designed to assess the performance of proprietary LLMs in detecting fine-grained hallucinations.\n\n**Technical Details**\nThe HalluVerse25 construction pipeline involves collecting original factual sentences from Wikipedia (using Wikidata) and injecting hallucinated content using LLMs. The data is categorized by type of hallucination, providing a nuanced understanding of multilingual language model performance.\n\n**Limitations & Future Work**\nCurrent limitations include the potential bias in dataset distribution, which may not accurately represent the world's population. Future work aims to address these limitations by expanding the dataset to include other tasks such as summarization and question answering.\n\n**Practical Implications**\nThe HalluVerse25 dataset has significant practical implications for improving LLM performance in detecting hallucinations across multiple languages. Its fine-grained categorization of hallucination types can inform the development of more accurate and robust language models."
    },
    {
        "id": "2501.15046v1",
        "title": "Evaluating Hallucination in Large Vision-Language Models based on\n  Context-Aware Object Similarities",
        "authors": [
            "Shounak Datta",
            "Dhanasekar Sundararaman"
        ],
        "url": "https://arxiv.org/abs/2501.15046v1",
        "Abstract": "Abstract Despite their impressive performance on multi-modal tasks, large vision-language models (LVLMs) tend to suffer from hallucinations. An important type is object hallucination, where LVLMs generate objects that are inconsistent with the images shown to the model. Existing works typically attempt to quantify object hallucinations by detecting and measuring the fraction of hallucinated objects in generated captions. Additionally, more recent work also measures object hallucinations by directly querying the LVLM with binary questions about the presence of likely hallucinated objects based on object statistics like top- k 𝑘 k italic_k frequent objects and top- k 𝑘 k italic_k co-occurring objects. In this paper, we present Context-Aware Object Similarities (CAOS), a novel approach for evaluating object hallucination in LVLMs using object statistics as well as the generated captions. CAOS uniquely integrates object statistics with semantic relationships between objects in captions and ground-truth data. Moreover, existing approaches usually only detect and measure hallucinations belonging to a predetermined set of in-domain objects (typically the set of all ground-truth objects for the training dataset) and ignore generated objects that are not part of this set, leading to under-evaluation. To address this, we further employ language model–based object recognition to detect potentially out-of-domain hallucinated objects and use an ensemble of LVLMs for verifying the presence of such objects in the query image. CAOS also examines the sequential dynamics of object generation, shedding light on how the order of object appearance influences hallucinations, and employs word embedding models to analyze the semantic reasons behind hallucinations. By providing a systematic framework to identify and interpret object hallucinations, CAOS aims to offer a nuanced understanding of both the hallucination tendencies of LVLMs and the factors contributing to object hallucinations.",
        "Main": {
            "Introduction": "Large language models (LLMs) like LLaMA, Gemini, and Mixtral (Touvron et al. 2023a , b ; Team et al. 2023 ; Jiang et al. 2024 ) have demonstrated remarkable capabilities in natural language processing tasks. Building upon this success, researchers have focused on integrating powerful LLMs with visual encoders to create large vision-language models (LVLMs) (Liu et al. 2024a ; Gong et al. 2023 ; Zhu et al. 2023 ; Dai et al. 2024 ; Ye et al. 2023 ) . These LVLMs leverage the language understanding abilities of LLMs while enhancing their capabilities to process and interpret visual information seamlessly. LVLMs typically utilize the visual encoders to analyze image data, while replacing the original language encoders with state-of-the-art LLMs. Through a combination of vision-language pretraining and fine-tuning on visual instructions (Wang et al. 2021 ) , LVLMs have demonstrated impressive performance on complex tasks that require integrating visual and linguistic information. LVLMs exhibit robust proficiency across various vision-language tasks, showcasing their versatility and adaptability. For instance, they excel in tasks such as image captioning (Herdade et al. 2019 ) , generating descriptive textual representations of visual content to bridge the semantic gap between images and language. Additionally, LVLMs demonstrate proficiency in visual question answering (Antol et al. 2015 ; Wu et al. 2017 ) , comprehending and responding to queries posed in natural language based on visual input. The development of LVLMs marks a significant milestone in the convergence of vision and language modalities, opening up new avenues for research and application in multi-modal models. However, LLMs, and similarly LVLMs, often suffer from the issue of hallucination (Rawte, Sheth, and Das 2023 ; Liu et al. 2024b ; Xu, Jain, and Kankanhalli 2024 ) , where the generated content contains information that is inconsistent or unfaithful to the provided input data. Hallucination refers to the phenomenon where the generated content contains nonsensical, contradictory or factually incorrect information that violates the input instructions or prompts. A common form of hallucination in LVLMs is object hallucination (Rohrbach et al. 2018; Li et al. 2023; Zhou et al. 2023), where the model describes objects or entities that are not present in the visual input. Recent studies have shown that LVLMs suffer from severe object hallucination issues (Li et al. 2023; Zhou et al. 2023), often generating descriptions that include objects inconsistent with the inputs. Hallucinations can be influenced by the visual instructions or prompts, as objects frequently occurring in the instructions or co-occurring with image objects are more prone to being hallucinated. Hallucination in both LLMs and LVLMs can be problematic, as it can lead to the generation of unreliable or misleading information. This issue undermines the reliability of LVLM outputs despite their semantic coherence. Moreover, it also poses potential risks in real-world applications where accurate interpretation of visual content is of paramount importance. Therefore, it has prompted researchers to develop evaluation benchmarks and mitigation techniques to detect and mitigate hallucination in these models. Existing works like (Li et al. 2023 ; Pham and Schott 2024 ) use object statistics such as top- k 𝑘 k italic_k objects and frequently co-occurring objects to determine the causes of hallucination and belong to a family of methods which ensure LVLMs respond to a set of predefined questions to evaluate hallucinations (Lovenia et al. 2023 ) . Others works may focus on a particular aspects of object hallucinations, like numbers (Zhang, Zhang, and Wan 2024 ) . Moreover, the object statistics mentioned above can also then be used to mitigate hallucinations by post-hoc rectification (Zhou et al. 2023 ) . In addition to the object statistics, the generated captions also contain additional information about an LVLM’s tendency to hallucinate, in the form of semantic relationships between the objects in the generated text. However, existing methods do not attempt to investigate the effect that the interplay between object statistics and the semantics of the objects present either in the query image or the already generated context can have on object hallucinations during the remainder of the generation process. Consequently, in this paper, we present a novel approach named context-aware object similarities (CAOS) for systematically evaluating object hallucination in LVLMs using the generated captions. By focusing on the interplay between generated objects (hallucinated or otherwise), their position in the generated captions, ground-truth objects as well as the object statistics of the training data, CAOS attempts to offers a more nuanced understanding of object hallucination dynamics and encompasses the the following key improvements over existing works: • Detect hallucinations of out-of-domain objects: Unlike existing methods which rely either on rule-based parsing of known in-domain objects (Rohrbach et al. 2018 ) or on known occurrence statistics of these objects from the training data (Li et al. 2023 ) , we propose a novel way to augment object detection from generated captions using LLMs and to verify the existence of candidate out-of-domain objects in the images using an oracle consisting of an ensemble of LVLMs. • Sequential generation dynamics: Recognizing the sequential nature of caption generation, CAOS investigates how the order of object appearance influences hallucination. By delving into this, our approach sheds light on how the dynamics of object generation impacts object hallucinations in addition to other critical factors, namely ground-truth objects and frequently occurring objects in the training dataset. • Semantic reasons behind hallucinations: Unlike traditional evaluation metrics, CAOS employs word embedding models to scrutinize the semantic relation between hallucinated objects and ground-truth objects, other objects in the generated captions and frequent objects from the training dataset. With an aim to foster the development of more robust and reliable LVLMs for diverse real-world applications, CAOS intend to enrich the existing literature on evaluating object hallucinations by offering a framework for better identifying object hallucinations in LVLMs, quantifying the semantic reasons behind such hallucinations, and for interpreting these results (potentially in tandem with existing metrics such as CHAIR (Rohrbach et al. 2018 ) and POPE (Li et al. 2023 ) ) to obtain a meaningful ranking not just based on an LVLM’s affinity to hallucinate but also the factors influencing such hallucinations.",
            "Related Works": "There are a number of related works on evaluating object hallucinations in LVLMs. Rohrbach et al. ( 2018 ) proposed the CHAIR family of metrics which uses rule-based parsing to identify in-domain objects in an LVLM-generated annotation and then calculates the fraction of hallucinated objects per caption and the fraction of annotations with hallucinated objects in a set of generated annotations. Another relevant precursor to our work is POPE (Li et al. 2023 ) . POPE measures the tendency of an LVLM to hallucinate objects by asking yes/no questions to the model about whether certain objects exist in the image, instead of directly evaluating the generated annotations. The choice of objects to be used for such queries can be random, or based on object statistics of the pre-training datasets, such as objects known to co-occur with ground-truth objects, or most frequent objects in the dataset. H-POPE (Pham and Schott 2024 ) is an extension of POPE that assesses hallucinations in object existence as well as attributes by hierarchically refining the yes/no questions from coarse-to-fine-grained, progressively probing about the attributes of the objects in the image. Lovenia et al. ( 2023 ) proposed NOPE, that uses a static set of negative pronouns to determine if a model hallucinates. There are a couple of works that detect object hallucination and perform post-hoc tuning to generate captions that do not contain hallucinations (Zhou et al. 2023 ; Dai et al. 2022 ) .",
            "CAOS: Context-Aware Object Similarities": "To account for all the different factors that can influence object hallucinations, we propose a set of evaluation metrics based on context-aware object similarities, called CAOS, to holistically measure how the contents of the image, the sequential ordering of generated objects, as well as the dominant contents of the training dataset influences hallucination. The proposed group of measures consists of CAOS T , CAOS X , CAOS K , CAOS T/X , CAOS X/K , and CAOS avg . For a given word embedding model E 𝐸 E italic_E , the CAOS T , CAOS X and CAOS K scores measure the maximum cosine similarity between the embeddings of a hallucinated object and a set of other objects. In particular, CAOS T measures the maximum cosine similarity for a given hallucinated object with all the ground-truth objects present in the image. CAOS X , on the other hand, measures the maximum cosine similarity between a hallucinate object and all past objects appearing before itself in the generated caption. In practice, we also consider all ground-truth objects, irrespective of their position in the generated caption, to be valid past objects for all hallucinated objects, since the ground-truth objects may always influence generation due to their presence in the image. Finally, since it is well-known that the most frequent objects present in the training datasets can also influence hallucinations (Li et al. 2023 ) , CAOS K measures the maximum cosine similarity between the embeddings of the hallucinated object and the top- k 𝑘 k italic_k frequent objects in the training dataset (MSCOCO in our experiments). Furthermore, since it is often relatively more tolerable for hallucinations to be semantically related to the ground-truth objects known to be present in the image than to other objects in the generated caption, we calculate CAOS T/X to be the ratio between CAOS T and CAOS X . A high CAOS T/X signifies that the hallucinations are mostly influenced by ground-truth objects. Similarly, it may be relatively more tolerable for the hallucinations to be related to past objects in the generated caption (including all ground-truth objects) than to frequent objects in the training dataset which may not have any relation to the contents of the image being described. Therefore, a high value of CAOS T/K , which is the ratio between CAOS X and CAOS K , may be desirable. Lastly, we also calculate CAOS avg which is the mean of CAOS T , CAOS X , and CAOS K . We argue that a high value of CAOS avg is also desirable in most cases. This is because high CAOS avg values denote that the hallucinations can be accounted for by the mentioned factors and is less likely to have been caused by unknown eccentricities of the LVLM. Given an image, the associated ground-truth object labels, and a caption generated by the LVLM for the image, we begin by identifying all objects in the captions. We then identify which of the objects are hallucinated based on the ground-truth object labels (or using an oracle for out-of-domain objects). In order of their appearance in the generated caption, we calculate the CAOS scores for all the hallucinated objects. Consequently, for a given caption containing multiple hallucinated objects, we report average values for all the CAOS scores over all hallucinated objects in the caption.",
            "Results": "We conduct all our experiments on the subset of 2000 images from the MSCOCO (Lin et al. 2014 ) validation set identical to that used by POPE (Li et al. 2023 ) . For each of these 2000 images, we use the instructions “Provide a brief description of the given image.” and “Question: Generate a short caption of the image. Answer: ” to probe 5 LVLMs, namely InstructBLIP (Dai et al. 2024 ) , LLaVA (Liu et al. 2024a ) , mPLUG-Owl (Ye et al. 2023 ) , MiniGPT-4 (Zhu et al. 2023 ) , and MultimodalGPT (Gong et al. 2023 ) to generate captions for the image. A detailed comparison of the LVLMs, in terms of model sizes and training recipes is shown in the Appendix. In Table 1 , we report all 6 CAOS scores, namely CAOS T , CAOS X , CAOS K , CAOS T/X , CAOS X/K , and CAOS avg , using two different word embedding models, GloVe (Pennington, Socher, and Manning 2014 ) and MiniLM-L6 (Wang et al. 2020 ; all-MiniLM-L6-v2 ) . We choose these embedding models as they form embeddings based on slightly different objectives. GloVe aims to assign similar embeddings to objects which co-occur is a corpus of documents, while MiniLM-L6 assigns semantically meaningful embeddings based on pretraining on a large number of language tasks such as question answering, natural language inference, etc. For the CAOS K scores, we choose k = 3 𝑘 3 k=3 italic_k = 3 based on the trends of CAOS K scores across k 𝑘 k italic_k values (see a full discussion in the subsequent section). Additionally, we also report precision (i.e., the fraction of detected objects which are not hallucinations) and recall (which is the fraction of actual ground-truth objects from the query image that are mentioned in the generated caption). Due to the relative instability of the precision measure (related to CHAIR I (Rohrbach et al. 2018 ) ), we also report POPE-F1 scores for all the models (Li et al. 2023 ) . We also report the fraction of captions having hallucinated objects, which is equivalent to the CHAIR S metric proposed by Rohrbach et al. ( 2018 ). LVLMs which are prone to generating shorter captions with fewer objects consequently have less scope for hallucination. However, such models also have limited ability to generate faithful descriptive captions for the images. Ideally, a model should have the ability to generate as many objects as needed to generate a particular image while minimizing hallucinations. Therefore, we have also reported the average number of objects generated per caption, as a loose proxy for the capacity of an LVLM to generate articulate captions. Overall, the CAOS scores trend similarly across the LVLMs for GloVe and MiniLM-L6. All of the LVLMs exhibit higher CAOS K scores relative to the CAOS T and CAOS X scores, implying that all of these models have a high tendency to hallucinate verbatim the frequent objects from the training dataset (i.e. MSCOCO). We also observe a trade-off between the CAOS T scores (or consequently the related CAOS X scores) and the CAOS K scores. In other words, LVLMs like InstructBLIP, LLaVA and mPLUG-Owl which have higher CAOS K scores tend to have lower CAOS T (and CAOS X ) scores. This hints at the fact that these models have a relatively higher tendency to hallucinate common objects from the training dataset while the remainder of the models, viz. MiniGPT-4 and MultimodalGPT, have a higher tendency for hallucinations related to the ground-truth objects and the preceding objects in the generated response. In order to better compare the performance of the different LVLMs, we also create a radar plot in Figure 3 , excluding the CAOS T , CAOS X , and CAOS K scores (since those scores do not offer a straightforward way to compare the models, which can be done instead with the other CAOS scores). InstructBLIP exhibits the highest 1-CHAIR S (or equivalently the lowest CHAIR S ) value as well as the highest precision and POPE-F1 scores, implying that it hallucinates less than the other models. However, this is in part due to its tendency to generate a lower number of objects per caption which also results in low recall. Moreover, InstructBLIP also has low CAOS X/K scores. On the other hand, MiniGPT-4 appears to overall have competitive performance across most of the axes, suggesting that it hallucinates less compared to most of the other contenders and that a relatively greater fraction of it’s hallucinations are related to the ground-truth objects actually present in the image (a somewhat tolerable property for hallucinations that do happen).",
            "Conclusion and Limitations": "Existing methods do not investigate the interplay between object statistics and the semantics of objects in query images or generated context, leaving a gap in understanding object hallucinations during the generation process. To address this, we propose the novel CAOS framework for systematically evaluating object hallucination in LVLMs using generated captions. CAOS focuses on the interaction between generated objects (hallucinated or otherwise), their positional dynamics, ground-truth objects, and object statistics from training data to offer a deeper understanding of hallucination dynamics. Our key contributions include detecting out-of-domain hallucinated objects using LLMs and an oracle based on an ensemble of LVLMs, analyzing sequential generation dynamics, and employing word embedding models to explore the semantic relationships behind hallucinations. We conduct experiments with several diverse LVLMs and find that CAOS effectively identifies hallucinations and provides insights into trade-offs, such as the tendency of certain models to hallucinate frequent objects from training datasets versus ground-truth-related objects. Notably, MiniGPT-4 demonstrates competitive performance across metrics, suggesting that it tends to hallucinate fewer and more contextually relevant objects. In summary, CAOS provides a systematic and nuanced framework for understanding hallucination dynamics, supporting the development of more reliable and robust LVLMs. It is important to note the limitations of our study despite the extensive exploration undertaken. We focus only on object hallucination in LVLMs, leaving out other performance aspects such as the ability to generate more articulate or contextually coherent responses. Moreover, we use a partial validation set of 2000 MSCOCO images due to computational constraints, which could potentially skew our results. However, for consistency with existing works, we retained the same subset of images employed by Li et al. ( 2023 ). Additionally, our reliance on rule-based object detection, augmented by an LLM and an oracle for verification, may occasionally lead to inaccuracies due to errors in any of these components, though such cases are likely rare. Finally, our analysis considers only a small subset of state-of-the-art LVLMs, excluding some newer or closed-source models. Nevertheless, we view these findings as a step forward in developing more reliable and human-aligned LVLMs. Future work could extend the CAOS framework to encompass other types of hallucinations, such as spatial, relational, or numerical inconsistencies, offering a holistic evaluation of an LVLM’s multimodal understanding."
        },
        "Tuples": [
            [
                "Introduction",
                "Large language models (LLMs) like LLaMA, Gemini, and Mixtral (Touvron et al. 2023a , b ; Team et al. 2023 ; Jiang et al. 2024 ) have demonstrated remarkable capabilities in natural language processing tasks. Building upon this success, researchers have focused on integrating powerful LLMs with visual encoders to create large vision-language models (LVLMs) (Liu et al. 2024a ; Gong et al. 2023 ; Zhu et al. 2023 ; Dai et al. 2024 ; Ye et al. 2023 ) . These LVLMs leverage the language understanding abilities of LLMs while enhancing their capabilities to process and interpret visual information seamlessly. LVLMs typically utilize the visual encoders to analyze image data, while replacing the original language encoders with state-of-the-art LLMs. Through a combination of vision-language pretraining and fine-tuning on visual instructions (Wang et al. 2021 ) , LVLMs have demonstrated impressive performance on complex tasks that require integrating visual and linguistic information. LVLMs exhibit robust proficiency across various vision-language tasks, showcasing their versatility and adaptability. For instance, they excel in tasks such as image captioning (Herdade et al. 2019 ) , generating descriptive textual representations of visual content to bridge the semantic gap between images and language. Additionally, LVLMs demonstrate proficiency in visual question answering (Antol et al. 2015 ; Wu et al. 2017 ) , comprehending and responding to queries posed in natural language based on visual input. The development of LVLMs marks a significant milestone in the convergence of vision and language modalities, opening up new avenues for research and application in multi-modal models. However, LLMs, and similarly LVLMs, often suffer from the issue of hallucination (Rawte, Sheth, and Das 2023 ; Liu et al. 2024b ; Xu, Jain, and Kankanhalli 2024 ) , where the generated content contains information that is inconsistent or unfaithful to the provided input data. Hallucination refers to the phenomenon where the generated content contains nonsensical, contradictory or factually incorrect information that violates the input instructions or prompts. A common form of hallucination in LVLMs is object hallucination (Rohrbach et al. 2018; Li et al. 2023; Zhou et al. 2023), where the model describes objects or entities that are not present in the visual input. Recent studies have shown that LVLMs suffer from severe object hallucination issues (Li et al. 2023; Zhou et al. 2023), often generating descriptions that include objects inconsistent with the inputs. Hallucinations can be influenced by the visual instructions or prompts, as objects frequently occurring in the instructions or co-occurring with image objects are more prone to being hallucinated. Hallucination in both LLMs and LVLMs can be problematic, as it can lead to the generation of unreliable or misleading information. This issue undermines the reliability of LVLM outputs despite their semantic coherence. Moreover, it also poses potential risks in real-world applications where accurate interpretation of visual content is of paramount importance. Therefore, it has prompted researchers to develop evaluation benchmarks and mitigation techniques to detect and mitigate hallucination in these models. Existing works like (Li et al. 2023 ; Pham and Schott 2024 ) use object statistics such as top- k 𝑘 k italic_k objects and frequently co-occurring objects to determine the causes of hallucination and belong to a family of methods which ensure LVLMs respond to a set of predefined questions to evaluate hallucinations (Lovenia et al. 2023 ) . Others works may focus on a particular aspects of object hallucinations, like numbers (Zhang, Zhang, and Wan 2024 ) . Moreover, the object statistics mentioned above can also then be used to mitigate hallucinations by post-hoc rectification (Zhou et al. 2023 ) . In addition to the object statistics, the generated captions also contain additional information about an LVLM’s tendency to hallucinate, in the form of semantic relationships between the objects in the generated text. However, existing methods do not attempt to investigate the effect that the interplay between object statistics and the semantics of the objects present either in the query image or the already generated context can have on object hallucinations during the remainder of the generation process. Consequently, in this paper, we present a novel approach named context-aware object similarities (CAOS) for systematically evaluating object hallucination in LVLMs using the generated captions. By focusing on the interplay between generated objects (hallucinated or otherwise), their position in the generated captions, ground-truth objects as well as the object statistics of the training data, CAOS attempts to offers a more nuanced understanding of object hallucination dynamics and encompasses the the following key improvements over existing works: • Detect hallucinations of out-of-domain objects: Unlike existing methods which rely either on rule-based parsing of known in-domain objects (Rohrbach et al. 2018 ) or on known occurrence statistics of these objects from the training data (Li et al. 2023 ) , we propose a novel way to augment object detection from generated captions using LLMs and to verify the existence of candidate out-of-domain objects in the images using an oracle consisting of an ensemble of LVLMs. • Sequential generation dynamics: Recognizing the sequential nature of caption generation, CAOS investigates how the order of object appearance influences hallucination. By delving into this, our approach sheds light on how the dynamics of object generation impacts object hallucinations in addition to other critical factors, namely ground-truth objects and frequently occurring objects in the training dataset. • Semantic reasons behind hallucinations: Unlike traditional evaluation metrics, CAOS employs word embedding models to scrutinize the semantic relation between hallucinated objects and ground-truth objects, other objects in the generated captions and frequent objects from the training dataset. With an aim to foster the development of more robust and reliable LVLMs for diverse real-world applications, CAOS intend to enrich the existing literature on evaluating object hallucinations by offering a framework for better identifying object hallucinations in LVLMs, quantifying the semantic reasons behind such hallucinations, and for interpreting these results (potentially in tandem with existing metrics such as CHAIR (Rohrbach et al. 2018 ) and POPE (Li et al. 2023 ) ) to obtain a meaningful ranking not just based on an LVLM’s affinity to hallucinate but also the factors influencing such hallucinations."
            ],
            [
                "Related Works",
                "There are a number of related works on evaluating object hallucinations in LVLMs. Rohrbach et al. ( 2018 ) proposed the CHAIR family of metrics which uses rule-based parsing to identify in-domain objects in an LVLM-generated annotation and then calculates the fraction of hallucinated objects per caption and the fraction of annotations with hallucinated objects in a set of generated annotations. Another relevant precursor to our work is POPE (Li et al. 2023 ) . POPE measures the tendency of an LVLM to hallucinate objects by asking yes/no questions to the model about whether certain objects exist in the image, instead of directly evaluating the generated annotations. The choice of objects to be used for such queries can be random, or based on object statistics of the pre-training datasets, such as objects known to co-occur with ground-truth objects, or most frequent objects in the dataset. H-POPE (Pham and Schott 2024 ) is an extension of POPE that assesses hallucinations in object existence as well as attributes by hierarchically refining the yes/no questions from coarse-to-fine-grained, progressively probing about the attributes of the objects in the image. Lovenia et al. ( 2023 ) proposed NOPE, that uses a static set of negative pronouns to determine if a model hallucinates. There are a couple of works that detect object hallucination and perform post-hoc tuning to generate captions that do not contain hallucinations (Zhou et al. 2023 ; Dai et al. 2022 ) ."
            ],
            [
                "CAOS: Context-Aware Object Similarities",
                "To account for all the different factors that can influence object hallucinations, we propose a set of evaluation metrics based on context-aware object similarities, called CAOS, to holistically measure how the contents of the image, the sequential ordering of generated objects, as well as the dominant contents of the training dataset influences hallucination. The proposed group of measures consists of CAOS T , CAOS X , CAOS K , CAOS T/X , CAOS X/K , and CAOS avg . For a given word embedding model E 𝐸 E italic_E , the CAOS T , CAOS X and CAOS K scores measure the maximum cosine similarity between the embeddings of a hallucinated object and a set of other objects. In particular, CAOS T measures the maximum cosine similarity for a given hallucinated object with all the ground-truth objects present in the image. CAOS X , on the other hand, measures the maximum cosine similarity between a hallucinate object and all past objects appearing before itself in the generated caption. In practice, we also consider all ground-truth objects, irrespective of their position in the generated caption, to be valid past objects for all hallucinated objects, since the ground-truth objects may always influence generation due to their presence in the image. Finally, since it is well-known that the most frequent objects present in the training datasets can also influence hallucinations (Li et al. 2023 ) , CAOS K measures the maximum cosine similarity between the embeddings of the hallucinated object and the top- k 𝑘 k italic_k frequent objects in the training dataset (MSCOCO in our experiments). Furthermore, since it is often relatively more tolerable for hallucinations to be semantically related to the ground-truth objects known to be present in the image than to other objects in the generated caption, we calculate CAOS T/X to be the ratio between CAOS T and CAOS X . A high CAOS T/X signifies that the hallucinations are mostly influenced by ground-truth objects. Similarly, it may be relatively more tolerable for the hallucinations to be related to past objects in the generated caption (including all ground-truth objects) than to frequent objects in the training dataset which may not have any relation to the contents of the image being described. Therefore, a high value of CAOS T/K , which is the ratio between CAOS X and CAOS K , may be desirable. Lastly, we also calculate CAOS avg which is the mean of CAOS T , CAOS X , and CAOS K . We argue that a high value of CAOS avg is also desirable in most cases. This is because high CAOS avg values denote that the hallucinations can be accounted for by the mentioned factors and is less likely to have been caused by unknown eccentricities of the LVLM. Given an image, the associated ground-truth object labels, and a caption generated by the LVLM for the image, we begin by identifying all objects in the captions. We then identify which of the objects are hallucinated based on the ground-truth object labels (or using an oracle for out-of-domain objects). In order of their appearance in the generated caption, we calculate the CAOS scores for all the hallucinated objects. Consequently, for a given caption containing multiple hallucinated objects, we report average values for all the CAOS scores over all hallucinated objects in the caption."
            ],
            [
                "Results",
                "We conduct all our experiments on the subset of 2000 images from the MSCOCO (Lin et al. 2014 ) validation set identical to that used by POPE (Li et al. 2023 ) . For each of these 2000 images, we use the instructions “Provide a brief description of the given image.” and “Question: Generate a short caption of the image. Answer: ” to probe 5 LVLMs, namely InstructBLIP (Dai et al. 2024 ) , LLaVA (Liu et al. 2024a ) , mPLUG-Owl (Ye et al. 2023 ) , MiniGPT-4 (Zhu et al. 2023 ) , and MultimodalGPT (Gong et al. 2023 ) to generate captions for the image. A detailed comparison of the LVLMs, in terms of model sizes and training recipes is shown in the Appendix. In Table 1 , we report all 6 CAOS scores, namely CAOS T , CAOS X , CAOS K , CAOS T/X , CAOS X/K , and CAOS avg , using two different word embedding models, GloVe (Pennington, Socher, and Manning 2014 ) and MiniLM-L6 (Wang et al. 2020 ; all-MiniLM-L6-v2 ) . We choose these embedding models as they form embeddings based on slightly different objectives. GloVe aims to assign similar embeddings to objects which co-occur is a corpus of documents, while MiniLM-L6 assigns semantically meaningful embeddings based on pretraining on a large number of language tasks such as question answering, natural language inference, etc. For the CAOS K scores, we choose k = 3 𝑘 3 k=3 italic_k = 3 based on the trends of CAOS K scores across k 𝑘 k italic_k values (see a full discussion in the subsequent section). Additionally, we also report precision (i.e., the fraction of detected objects which are not hallucinations) and recall (which is the fraction of actual ground-truth objects from the query image that are mentioned in the generated caption). Due to the relative instability of the precision measure (related to CHAIR I (Rohrbach et al. 2018 ) ), we also report POPE-F1 scores for all the models (Li et al. 2023 ) . We also report the fraction of captions having hallucinated objects, which is equivalent to the CHAIR S metric proposed by Rohrbach et al. ( 2018 ). LVLMs which are prone to generating shorter captions with fewer objects consequently have less scope for hallucination. However, such models also have limited ability to generate faithful descriptive captions for the images. Ideally, a model should have the ability to generate as many objects as needed to generate a particular image while minimizing hallucinations. Therefore, we have also reported the average number of objects generated per caption, as a loose proxy for the capacity of an LVLM to generate articulate captions. Overall, the CAOS scores trend similarly across the LVLMs for GloVe and MiniLM-L6. All of the LVLMs exhibit higher CAOS K scores relative to the CAOS T and CAOS X scores, implying that all of these models have a high tendency to hallucinate verbatim the frequent objects from the training dataset (i.e. MSCOCO). We also observe a trade-off between the CAOS T scores (or consequently the related CAOS X scores) and the CAOS K scores. In other words, LVLMs like InstructBLIP, LLaVA and mPLUG-Owl which have higher CAOS K scores tend to have lower CAOS T (and CAOS X ) scores. This hints at the fact that these models have a relatively higher tendency to hallucinate common objects from the training dataset while the remainder of the models, viz. MiniGPT-4 and MultimodalGPT, have a higher tendency for hallucinations related to the ground-truth objects and the preceding objects in the generated response. In order to better compare the performance of the different LVLMs, we also create a radar plot in Figure 3 , excluding the CAOS T , CAOS X , and CAOS K scores (since those scores do not offer a straightforward way to compare the models, which can be done instead with the other CAOS scores). InstructBLIP exhibits the highest 1-CHAIR S (or equivalently the lowest CHAIR S ) value as well as the highest precision and POPE-F1 scores, implying that it hallucinates less than the other models. However, this is in part due to its tendency to generate a lower number of objects per caption which also results in low recall. Moreover, InstructBLIP also has low CAOS X/K scores. On the other hand, MiniGPT-4 appears to overall have competitive performance across most of the axes, suggesting that it hallucinates less compared to most of the other contenders and that a relatively greater fraction of it’s hallucinations are related to the ground-truth objects actually present in the image (a somewhat tolerable property for hallucinations that do happen)."
            ],
            [
                "Conclusion and Limitations",
                "Existing methods do not investigate the interplay between object statistics and the semantics of objects in query images or generated context, leaving a gap in understanding object hallucinations during the generation process. To address this, we propose the novel CAOS framework for systematically evaluating object hallucination in LVLMs using generated captions. CAOS focuses on the interaction between generated objects (hallucinated or otherwise), their positional dynamics, ground-truth objects, and object statistics from training data to offer a deeper understanding of hallucination dynamics. Our key contributions include detecting out-of-domain hallucinated objects using LLMs and an oracle based on an ensemble of LVLMs, analyzing sequential generation dynamics, and employing word embedding models to explore the semantic relationships behind hallucinations. We conduct experiments with several diverse LVLMs and find that CAOS effectively identifies hallucinations and provides insights into trade-offs, such as the tendency of certain models to hallucinate frequent objects from training datasets versus ground-truth-related objects. Notably, MiniGPT-4 demonstrates competitive performance across metrics, suggesting that it tends to hallucinate fewer and more contextually relevant objects. In summary, CAOS provides a systematic and nuanced framework for understanding hallucination dynamics, supporting the development of more reliable and robust LVLMs. It is important to note the limitations of our study despite the extensive exploration undertaken. We focus only on object hallucination in LVLMs, leaving out other performance aspects such as the ability to generate more articulate or contextually coherent responses. Moreover, we use a partial validation set of 2000 MSCOCO images due to computational constraints, which could potentially skew our results. However, for consistency with existing works, we retained the same subset of images employed by Li et al. ( 2023 ). Additionally, our reliance on rule-based object detection, augmented by an LLM and an oracle for verification, may occasionally lead to inaccuracies due to errors in any of these components, though such cases are likely rare. Finally, our analysis considers only a small subset of state-of-the-art LVLMs, excluding some newer or closed-source models. Nevertheless, we view these findings as a step forward in developing more reliable and human-aligned LVLMs. Future work could extend the CAOS framework to encompass other types of hallucinations, such as spatial, relational, or numerical inconsistencies, offering a holistic evaluation of an LVLM’s multimodal understanding."
            ]
        ],
        "section_summaries": [
            [
                "Introduction",
                "Large language models (LLMs) like LLaMA, Gemini, and Mixtral (Touvron et al. 2023a , b ; Team et al. 2023 ; Jiang et al. 2024 ) have demonstrated remarkable capabilities in natural language processing tasks. Building upon this success, researchers have focused on integrating powerful LLMs with visual encoders to create large vision-language models (LVLMs) (Liu et al. 2024a ; Gong et al. 2023 ; Zhu et al. 2023 ; Dai et al. 2024 ; Ye et al. 2023 ) . These LVLMs leverage the language understanding abilities of LLMs while enhancing their capabilities to process and interpret visual information seamlessly. LVLMs typically utilize the visual encoders to analyze image data, while replacing the original language encoders with state-of-the-art LLMs. Through a combination of vision-language pretraining and fine-tuning on visual instructions (Wang et al. 2021 ) , LVLMs have demonstrated impressive performance on complex tasks that require integrating visual and linguistic information. LVLMs exhibit robust proficiency across various vision-language tasks, showcasing their versatility and adaptability. For instance, they excel in tasks such as image captioning (Herdade et al. 2019 ) , generating descriptive textual representations of visual content to bridge the semantic gap between images and language. Additionally, LVLMs demonstrate proficiency in visual question answering (Antol et al. 2015 ; Wu et al. 2017 ) , comprehending and responding to queries posed in natural language based on visual input. The development of LVLMs marks a significant milestone in the convergence of vision and language modalities, opening up new avenues for research and application in multi-modal models. However, LLMs, and similarly LVLMs, often suffer from the issue of hallucination (Rawte, Sheth, and Das 2023 ; Liu et al. 2024b ; Xu, Jain, and Kankanhalli 2024 ) , where the generated content contains information that is inconsistent or unfaithful to the provided input data. Hallucination refers to the phenomenon where the generated content contains nonsensical, contradictory or factually incorrect information that violates the input instructions or prompts. A common form of hallucination in LVLMs is object hallucination (Rohrbach et al. 2018; Li et al. 2023; Zhou et al. 2023), where the model describes objects or entities that are not present in the visual input. Recent studies have shown that LVLMs suffer from severe object hallucination issues (Li et al. 2023; Zhou et al. 2023), often generating descriptions that include objects inconsistent with the inputs. Hallucinations can be influenced by the visual instructions or prompts, as objects frequently occurring in the instructions or co-occurring with image objects are more prone to being hallucinated. Hallucination in both LLMs and LVLMs can be problematic, as it can lead to the generation of unreliable or misleading information. This issue undermines the reliability of LVLM outputs despite their semantic coherence. Moreover, it also poses potential risks in real-world applications where accurate interpretation of visual content is of paramount importance. Therefore, it has prompted researchers to develop evaluation benchmarks and mitigation techniques to detect and mitigate hallucination in these models. Existing works like (Li et al. 2023 ; Pham and Schott 2024 ) use object statistics such as top- k 𝑘 k italic_k objects and frequently co-occurring objects to determine the causes of hallucination and belong to a family of methods which ensure LVLMs respond to a set of predefined questions to evaluate hallucinations (Lovenia et al. 2023 ) . Others works may focus on a particular aspects of object hallucinations, like numbers (Zhang, Zhang, and Wan 2024 ) . Moreover, the object statistics mentioned above can also then be used to mitigate hallucinations by post-hoc rectification (Zhou et al. 2023 ) . In addition to the object statistics, the generated captions also contain additional information about an LVLM’s tendency to hallucinate, in the form of semantic relationships between the objects in the generated text. However, existing methods do not attempt to investigate the effect that the interplay between object statistics and the semantics of the objects present either in the query image or the already generated context can have on object hallucinations during the remainder of the generation process. Consequently, in this paper, we present a novel approach named context-aware object similarities (CAOS) for systematically evaluating object hallucination in LVLMs using the generated captions. By focusing on the interplay between generated objects (hallucinated or otherwise), their position in the generated captions, ground-truth objects as well as the object statistics of the training data, CAOS attempts to offers a more nuanced understanding of object hallucination dynamics and encompasses the the following key improvements over existing works: • Detect hallucinations of out-of-domain objects: Unlike existing methods which rely either on rule-based parsing of known in-domain objects (Rohrbach et al. 2018 ) or on known occurrence statistics of these objects from the training data (Li et al. 2023 ) , we propose a novel way to augment object detection from generated captions using LLMs and to verify the existence of candidate out-of-domain objects in the images using an oracle consisting of an ensemble of LVLMs. • Sequential generation dynamics: Recognizing the sequential nature of caption generation, CAOS investigates how the order of object appearance influences hallucination. By delving into this, our approach sheds light on how the dynamics of object generation impacts object hallucinations in addition to other critical factors, namely ground-truth objects and frequently occurring objects in the training dataset. • Semantic reasons behind hallucinations: Unlike traditional evaluation metrics, CAOS employs word embedding models to scrutinize the semantic relation between hallucinated objects and ground-truth objects, other objects in the generated captions and frequent objects from the training dataset. With an aim to foster the development of more robust and reliable LVLMs for diverse real-world applications, CAOS intend to enrich the existing literature on evaluating object hallucinations by offering a framework for better identifying object hallucinations in LVLMs, quantifying the semantic reasons behind such hallucinations, and for interpreting these results (potentially in tandem with existing metrics such as CHAIR (Rohrbach et al. 2018 ) and POPE (Li et al. 2023 ) ) to obtain a meaningful ranking not just based on an LVLM’s affinity to hallucinate but also the factors influencing such hallucinations.",
                "LLMs like LLaMA, Gemini, and Mixtral have demonstrated remarkable capabilities in natural language processing tasks. Researchers integrated powerful LLMs with visual encoders to create LVLMs, which leverage language understanding abilities while enhancing their capabilities to process and interpret visual information. Through a combination of vision-language pretraining and fine-tuning on visual instructions, LVLMs have demonstrated impressive performance on complex tasks that require integrating visual and linguistic information.\n\nLLMs often suffer from the issue of hallucination, where generated content contains nonsensical, contradictory or factually incorrect information. Hallucinations can be influenced by visual instructions or prompts, as objects frequently occurring in them are more prone to being hallucinated. Researchers have developed evaluation benchmarks and mitigation techniques to detect and mitigate hallucination in these models.\n\nA novel approach named context-aware object similarities (CAOS) is presented for systematically evaluating object hallucination in LVLMs using generated captions. CAOS attempts to offer a more nuanced understanding of object hallucination dynamics, focusing on the interplay between generated objects, their position in the generated captions, ground-truth objects and the object statistics of the training data.\n\nExisting methods rely either on rule-based parsing or known occurrence statistics, while CAOS proposes a novel way to augment object detection from generated captions using LLMs. CAOS also investigates how the order of object appearance influences hallucination and employs word embedding models to scrutinize the semantic relation between hallucinated objects and ground-truth objects.\n\nExisting works like CHAIR (Rohrbach et al. 2018 ) and POPE (Li et al. 2023) have evaluated LVLMs, but CAOS offers a framework for better identifying object hallucinations in LVLMs, quantifying the semantic reasons behind such hallucinations, and interpreting these results to obtain a meaningful ranking based on an LVLM’s affinity to hallucinate and factors influencing such hallucinations.\n\nLVLMs exhibit robust proficiency across various vision-language tasks, showcasing their versatility and adaptability. They excel in tasks such as image captioning, generating descriptive textual representations of visual content to bridge the semantic gap between images and language.\n\nObject statistics can be used to mitigate hallucinations by post-hoc rectification. However, existing methods do not investigate the effect that the interplay between object statistics and the semantics of objects present either in the query image or the already generated context can have on object hallucinations during the remainder of the generation process.\n\nCAOS aims to enrich the existing literature on evaluating object hallucinations by offering a framework for better identifying object hallucinations in LVLMs, quantifying the semantic reasons behind such hallucinations, and interpreting these results to obtain a meaningful ranking not just based on an LVLM’s affinity to hallucinate but also the factors influencing such hallucinations."
            ],
            [
                "Related Works",
                "There are a number of related works on evaluating object hallucinations in LVLMs. Rohrbach et al. ( 2018 ) proposed the CHAIR family of metrics which uses rule-based parsing to identify in-domain objects in an LVLM-generated annotation and then calculates the fraction of hallucinated objects per caption and the fraction of annotations with hallucinated objects in a set of generated annotations. Another relevant precursor to our work is POPE (Li et al. 2023 ) . POPE measures the tendency of an LVLM to hallucinate objects by asking yes/no questions to the model about whether certain objects exist in the image, instead of directly evaluating the generated annotations. The choice of objects to be used for such queries can be random, or based on object statistics of the pre-training datasets, such as objects known to co-occur with ground-truth objects, or most frequent objects in the dataset. H-POPE (Pham and Schott 2024 ) is an extension of POPE that assesses hallucinations in object existence as well as attributes by hierarchically refining the yes/no questions from coarse-to-fine-grained, progressively probing about the attributes of the objects in the image. Lovenia et al. ( 2023 ) proposed NOPE, that uses a static set of negative pronouns to determine if a model hallucinates. There are a couple of works that detect object hallucination and perform post-hoc tuning to generate captions that do not contain hallucinations (Zhou et al. 2023 ; Dai et al. 2022 ) .",
                "Rohrbach et al. (2018) proposed CHAIR, Li et al. (2023) proposed POPE, Pham and Schott (2024) proposed H-POPE as extensions to POPE, Lovenia et al. (2023) proposed NOPE, and Zhou et al. (2023), Dai et al. (2022) addressed object hallucination detection and post-hoc tuning."
            ],
            [
                "CAOS: Context-Aware Object Similarities",
                "To account for all the different factors that can influence object hallucinations, we propose a set of evaluation metrics based on context-aware object similarities, called CAOS, to holistically measure how the contents of the image, the sequential ordering of generated objects, as well as the dominant contents of the training dataset influences hallucination. The proposed group of measures consists of CAOS T , CAOS X , CAOS K , CAOS T/X , CAOS X/K , and CAOS avg . For a given word embedding model E 𝐸 E italic_E , the CAOS T , CAOS X and CAOS K scores measure the maximum cosine similarity between the embeddings of a hallucinated object and a set of other objects. In particular, CAOS T measures the maximum cosine similarity for a given hallucinated object with all the ground-truth objects present in the image. CAOS X , on the other hand, measures the maximum cosine similarity between a hallucinate object and all past objects appearing before itself in the generated caption. In practice, we also consider all ground-truth objects, irrespective of their position in the generated caption, to be valid past objects for all hallucinated objects, since the ground-truth objects may always influence generation due to their presence in the image. Finally, since it is well-known that the most frequent objects present in the training datasets can also influence hallucinations (Li et al. 2023 ) , CAOS K measures the maximum cosine similarity between the embeddings of the hallucinated object and the top- k 𝑘 k italic_k frequent objects in the training dataset (MSCOCO in our experiments). Furthermore, since it is often relatively more tolerable for hallucinations to be semantically related to the ground-truth objects known to be present in the image than to other objects in the generated caption, we calculate CAOS T/X to be the ratio between CAOS T and CAOS X . A high CAOS T/X signifies that the hallucinations are mostly influenced by ground-truth objects. Similarly, it may be relatively more tolerable for the hallucinations to be related to past objects in the generated caption (including all ground-truth objects) than to frequent objects in the training dataset which may not have any relation to the contents of the image being described. Therefore, a high value of CAOS T/K , which is the ratio between CAOS X and CAOS K , may be desirable. Lastly, we also calculate CAOS avg which is the mean of CAOS T , CAOS X , and CAOS K . We argue that a high value of CAOS avg is also desirable in most cases. This is because high CAOS avg values denote that the hallucinations can be accounted for by the mentioned factors and is less likely to have been caused by unknown eccentricities of the LVLM. Given an image, the associated ground-truth object labels, and a caption generated by the LVLM for the image, we begin by identifying all objects in the captions. We then identify which of the objects are hallucinated based on the ground-truth object labels (or using an oracle for out-of-domain objects). In order of their appearance in the generated caption, we calculate the CAOS scores for all the hallucinated objects. Consequently, for a given caption containing multiple hallucinated objects, we report average values for all the CAOS scores over all hallucinated objects in the caption.",
                "We propose CAOS, a set of evaluation metrics to measure how image contents, sequential ordering, and training dataset influence object hallucinations. The metrics include CAOS T, CAOS X, CAOS K, CAOS T/X, CAOS X/K, and CAOS avg. For each word embedding model E, CAOS scores measure cosine similarities between hallucinated objects and other objects or frequent training objects.\n\nMetrics:\n- CAOS T: max cosine similarity with ground-truth objects in image\n- CAOS X: max cosine similarity with past objects in generated caption\n- CAOS K: max cosine similarity with top-k frequent objects in training dataset\n- CAOS T/X: ratio of CAOS T to CAOS X\n- CAOS X/K: ratio of CAOS X to CAOS K\n- CAOS avg: mean of CAOS T, CAOS X, and CAOS K"
            ],
            [
                "Results",
                "We conduct all our experiments on the subset of 2000 images from the MSCOCO (Lin et al. 2014 ) validation set identical to that used by POPE (Li et al. 2023 ) . For each of these 2000 images, we use the instructions “Provide a brief description of the given image.” and “Question: Generate a short caption of the image. Answer: ” to probe 5 LVLMs, namely InstructBLIP (Dai et al. 2024 ) , LLaVA (Liu et al. 2024a ) , mPLUG-Owl (Ye et al. 2023 ) , MiniGPT-4 (Zhu et al. 2023 ) , and MultimodalGPT (Gong et al. 2023 ) to generate captions for the image. A detailed comparison of the LVLMs, in terms of model sizes and training recipes is shown in the Appendix. In Table 1 , we report all 6 CAOS scores, namely CAOS T , CAOS X , CAOS K , CAOS T/X , CAOS X/K , and CAOS avg , using two different word embedding models, GloVe (Pennington, Socher, and Manning 2014 ) and MiniLM-L6 (Wang et al. 2020 ; all-MiniLM-L6-v2 ) . We choose these embedding models as they form embeddings based on slightly different objectives. GloVe aims to assign similar embeddings to objects which co-occur is a corpus of documents, while MiniLM-L6 assigns semantically meaningful embeddings based on pretraining on a large number of language tasks such as question answering, natural language inference, etc. For the CAOS K scores, we choose k = 3 𝑘 3 k=3 italic_k = 3 based on the trends of CAOS K scores across k 𝑘 k italic_k values (see a full discussion in the subsequent section). Additionally, we also report precision (i.e., the fraction of detected objects which are not hallucinations) and recall (which is the fraction of actual ground-truth objects from the query image that are mentioned in the generated caption). Due to the relative instability of the precision measure (related to CHAIR I (Rohrbach et al. 2018 ) ), we also report POPE-F1 scores for all the models (Li et al. 2023 ) . We also report the fraction of captions having hallucinated objects, which is equivalent to the CHAIR S metric proposed by Rohrbach et al. ( 2018 ). LVLMs which are prone to generating shorter captions with fewer objects consequently have less scope for hallucination. However, such models also have limited ability to generate faithful descriptive captions for the images. Ideally, a model should have the ability to generate as many objects as needed to generate a particular image while minimizing hallucinations. Therefore, we have also reported the average number of objects generated per caption, as a loose proxy for the capacity of an LVLM to generate articulate captions. Overall, the CAOS scores trend similarly across the LVLMs for GloVe and MiniLM-L6. All of the LVLMs exhibit higher CAOS K scores relative to the CAOS T and CAOS X scores, implying that all of these models have a high tendency to hallucinate verbatim the frequent objects from the training dataset (i.e. MSCOCO). We also observe a trade-off between the CAOS T scores (or consequently the related CAOS X scores) and the CAOS K scores. In other words, LVLMs like InstructBLIP, LLaVA and mPLUG-Owl which have higher CAOS K scores tend to have lower CAOS T (and CAOS X ) scores. This hints at the fact that these models have a relatively higher tendency to hallucinate common objects from the training dataset while the remainder of the models, viz. MiniGPT-4 and MultimodalGPT, have a higher tendency for hallucinations related to the ground-truth objects and the preceding objects in the generated response. In order to better compare the performance of the different LVLMs, we also create a radar plot in Figure 3 , excluding the CAOS T , CAOS X , and CAOS K scores (since those scores do not offer a straightforward way to compare the models, which can be done instead with the other CAOS scores). InstructBLIP exhibits the highest 1-CHAIR S (or equivalently the lowest CHAIR S ) value as well as the highest precision and POPE-F1 scores, implying that it hallucinates less than the other models. However, this is in part due to its tendency to generate a lower number of objects per caption which also results in low recall. Moreover, InstructBLIP also has low CAOS X/K scores. On the other hand, MiniGPT-4 appears to overall have competitive performance across most of the axes, suggesting that it hallucinates less compared to most of the other contenders and that a relatively greater fraction of it’s hallucinations are related to the ground-truth objects actually present in the image (a somewhat tolerable property for hallucinations that do happen).",
                "We conduct experiments on 2000 MSCOCO images using 5 LVLMs: InstructBLIP, LLaVA, mPLUG-Owl, MiniGPT-4, and MultimodalGPT. We report CAOS scores for 6 metrics using GloVe and MiniLM-L6 word embeddings. The models exhibit higher CAOS K scores than CAOS T and CAOS X scores, indicating a tendency to hallucinate frequent objects from the training dataset. We also observe a trade-off between CAOS T scores and CAOS K scores across the LVLMs. InstructBLIP has high 1-CHAIR S value and POPE-F1 scores but low recall and CAOS X/K scores due to generating fewer objects per caption. MiniGPT-4 shows competitive performance with lower hallucination rates related to ground-truth objects."
            ],
            [
                "Conclusion and Limitations",
                "Existing methods do not investigate the interplay between object statistics and the semantics of objects in query images or generated context, leaving a gap in understanding object hallucinations during the generation process. To address this, we propose the novel CAOS framework for systematically evaluating object hallucination in LVLMs using generated captions. CAOS focuses on the interaction between generated objects (hallucinated or otherwise), their positional dynamics, ground-truth objects, and object statistics from training data to offer a deeper understanding of hallucination dynamics. Our key contributions include detecting out-of-domain hallucinated objects using LLMs and an oracle based on an ensemble of LVLMs, analyzing sequential generation dynamics, and employing word embedding models to explore the semantic relationships behind hallucinations. We conduct experiments with several diverse LVLMs and find that CAOS effectively identifies hallucinations and provides insights into trade-offs, such as the tendency of certain models to hallucinate frequent objects from training datasets versus ground-truth-related objects. Notably, MiniGPT-4 demonstrates competitive performance across metrics, suggesting that it tends to hallucinate fewer and more contextually relevant objects. In summary, CAOS provides a systematic and nuanced framework for understanding hallucination dynamics, supporting the development of more reliable and robust LVLMs. It is important to note the limitations of our study despite the extensive exploration undertaken. We focus only on object hallucination in LVLMs, leaving out other performance aspects such as the ability to generate more articulate or contextually coherent responses. Moreover, we use a partial validation set of 2000 MSCOCO images due to computational constraints, which could potentially skew our results. However, for consistency with existing works, we retained the same subset of images employed by Li et al. ( 2023 ). Additionally, our reliance on rule-based object detection, augmented by an LLM and an oracle for verification, may occasionally lead to inaccuracies due to errors in any of these components, though such cases are likely rare. Finally, our analysis considers only a small subset of state-of-the-art LVLMs, excluding some newer or closed-source models. Nevertheless, we view these findings as a step forward in developing more reliable and human-aligned LVLMs. Future work could extend the CAOS framework to encompass other types of hallucinations, such as spatial, relational, or numerical inconsistencies, offering a holistic evaluation of an LVLM’s multimodal understanding.",
                "CAOS (Captive Attention-based Object Statistics) framework:\n- Investigates interplay between object statistics and object semantics in query images and generated context\n- Detects out-of-domain hallucinated objects using LLMs and an oracle based on ensemble of LVLMs\n- Analyzes sequential generation dynamics and employs word embedding models to explore semantic relationships behind hallucinations\n\nExperiments with diverse LVLMs:\n- CAOS effectively identifies hallucinations and provides insights into trade-offs between frequent object hallucination and ground-truth-related objects\n- MiniGPT-4 demonstrates competitive performance across metrics, suggesting fewer and more contextually relevant object hallucinations\n\nLimitations:\n- Focused on object hallucination in LVLMs, excluding other performance aspects like articulate or coherent response generation\n- Used partial validation set of 2000 MSCOCO images due to computational constraints\n- Relies on rule-based object detection with potential inaccuracies\n- Analyzes only a small subset of state-of-the-art LVLMs, excluding newer or closed-source models"
            ]
        ],
        "general_summary": "**Research Overview**\n\nThis research addresses the issue of hallucination in large vision-language models (LVLMs), particularly object hallucination where generated objects are inconsistent with the images shown to the model. The proposed method, Context-Aware Object Similarities (CAOS), integrates object statistics with semantic relationships between objects in captions and ground-truth data to evaluate object hallucinations.\n\n**Methodology & Approach**\n\nThe CAOS framework evaluates object hallucinations by combining object statistics with semantic relationships between objects in generated captions and ground-truth data. It uses language model-based object recognition to detect potentially out-of-domain hallucinated objects and employs an ensemble of LVLMs for verification. Additionally, CAOS examines the sequential dynamics of object generation and analyzes the semantic reasons behind hallucinations using word embedding models.\n\n**Key Contributions & Findings**\n\nThe research contributes a novel approach (CAOS) for evaluating object hallucination in LVLMs, which provides a nuanced understanding of both the hallucination tendencies of LVLMs and the factors contributing to object hallucinations. The experiments conducted on 2000 MSCOCO images using 5 LVLMs show that CAOS effectively identifies hallucinations and provides insights into trade-offs between frequent object hallucination and ground-truth-related objects.\n\n**Technical Details**\n\nThe CAOS framework employs six metrics: CAOS T, CAOS X, CAOS K, CAOS T/X, CAOS X/K, and CAOS avg. These metrics measure cosine similarities between hallucinated objects and other objects or frequent training objects using word embedding models (GloVe and MiniLM-L6). The language model-based object recognition uses an ensemble of LVLMs to verify the presence of out-of-domain hallucinated objects.\n\n**Limitations & Future Work**\n\nThe research has several limitations, including its focus on object hallucination in LVLMs, excluding other performance aspects like articulate or coherent response generation. Additionally, the experiments were conducted on a partial validation set of 2000 MSCOCO images due to computational constraints, and the analysis only considered a small subset of state-of-the-art LVLMs.\n\n**Practical Implications**\n\nThe CAOS framework has practical implications for evaluating and improving the performance of LVLMs in various vision-language tasks. By providing a nuanced understanding of object hallucinations, CAOS can help developers identify areas for improvement and optimize their models to reduce hallucination rates."
    },
    {
        "id": "2505.04847v1",
        "title": "Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards",
        "authors": [
            "Manveer Singh Tamber",
            "Forrest Sheng Bao",
            "Chenyu Xu",
            "Ge Luo",
            "Suleman Kazi",
            "Minseok Bae",
            "Miaoran Li",
            "Ofer Mendelevitch",
            "Renyi Qu",
            "Jimmy Lin"
        ],
        "url": "https://arxiv.org/abs/2505.04847v1",
        "Abstract": "Abstract Hallucinations remain a persistent challenge for LLMs.\nRAG aims to reduce hallucinations by grounding responses in contexts.\nHowever, even when provided context, LLMs still frequently introduce unsupported information or contradictions.\nThis paper presents our efforts to measure LLM hallucinations with a focus on summarization tasks, assessing how often various LLMs introduce hallucinations when summarizing documents.\nWe discuss Vectara’s existing LLM hallucination leaderboard, based on the Hughes Hallucination Evaluation Model (HHEM).\nWhile HHEM and Vectara’s Hallucination Leaderboard have garnered great research interest, we examine challenges faced by HHEM and current hallucination detection methods by analyzing the effectiveness of these methods on existing hallucination datasets.\nTo address these limitations, we propose FaithJudge, an LLM-as-a-judge approach guided by few-shot human hallucination annotations, which substantially improves automated LLM hallucination evaluation over current methods.\nWe introduce an enhanced hallucination leaderboard centered on FaithJudge 1 1 1 https://github.com/vectara/FaithJudge , alongside our current hallucination leaderboard 2 2 2 https://github.com/vectara/hallucination-leaderboard , enabling more reliable benchmarking of LLMs for hallucinations in RAG.",
        "Main": {
            "1 Introduction": "LLMs excel in various tasks, but frequently produce hallucinations, generating false or misleading information unsupported by provided contexts or world knowledge Ji et al. ( 2023 ); Huang et al. ( 2025 ); Lin et al. ( 2022 ); Tang et al. ( 2023 ) . While Retrieval-Augmented Generation (RAG) approaches Guu et al. ( 2020 ); Lewis et al. ( 2020b ); Shuster et al. ( 2021 ) attempt to mitigate hallucinations by grounding responses in external trusted contexts, they do not fully eliminate hallucinations, as LLMs often introduce details unsupported by retrieved contexts, misrepresent information, or generate outright contradictions Niu et al. ( 2024 ) . An ongoing challenge within RAG is ensuring context-faithfulness Niu et al. ( 2024 ); Jia et al. ( 2023 ); Ming et al. ( 2024 ) . Detecting when LLMs deviate from the information in the provided context remains a difficult problem. Although there has been progress, hallucination detection methods, including fine-tuned detectors largely for evaluating summaries Zhou et al. ( 2021 ); Gekhman et al. ( 2023 ); Honovich et al. ( 2022 ); Zha et al. ( 2023 ); Tang et al. ( 2024a ) and LLM-as-a-judge techniques Zheng et al. ( 2023 ); Luo et al. ( 2023 ); Jacovi et al. ( 2025 ) , continue to struggle with the accurate identification of LLM-generated hallucinations. In this paper, we study and aim to improve hallucination evaluation in RAG by building on prior work in summary consistency evaluation. We analyze the capabilities and limitations of current hallucination detection methods, including fine-tuned models such as Vectara’s Hughes Hallucination Evaluation Model (HHEM) Bao et al. ( 2024 ) and zero-shot methods using LLM judges. To overcome the challenges of LLM-as-a-judge techniques for zero-shot hallucination detection and fine-tuned hallucination detection models, we introduce FaithJudge, an LLM-as-a-judge approach guided by few-shot human annotations of hallucinations. FaithJudge leverages labelled hallucinations from diverse LLM generations to automate the evaluation of LLMs on their propensity to hallucinate when summarizing the same articles or using the same articles to respond to queries. This approach results in notably higher agreement with human judgments compared with existing automated methods. Additionally, we introduce an enhanced hallucination leaderboard based on FaithJudge, enabling more reliable benchmarking of hallucinations in LLM-generated summaries and responses. We discuss both Vectara’s existing hallucination leaderboard Hughes and Bae ( 2023 ) based on HHEM and our new leaderboard based on FaithJudge. Hallucinations within RAG remain frequent and problematic, even in leading LLMs. Our approach contributes toward more accurate hallucination evaluation, aiding the development of more trustworthy generative AI systems. Vectara serves customers across diverse industries. For many of these customers, addressing hallucinations in LLM outputs is a critical priority. Driven by this customer-centric need, we developed our hallucination leaderboard and benchmarking methods, which are presented in this paper.",
            "2 Background": "Accurate hallucination detection is essential for reliably quantifying hallucination rates in LLMs. Numerous datasets have been developed for evaluating hallucinations in summarization tasks. Earlier datasets, such as SummaC Laban et al. ( 2022 ) and AggreFact Tang et al. ( 2023 ) , aggregated multiple resources, standardized labels, and classification taxonomies. However, these primarily focused on summaries from pre-ChatGPT models like fine-tuned T5 Raffel et al. ( 2020 ) , BART Lewis et al. ( 2020a ) , and PEGASUS Zhang et al. ( 2020 ) models, potentially limiting their relevance to contemporary LLMs that may produce more nuanced and difficult to identify hallucinations. Recent benchmarks address this limitation by incorporating summaries generated by modern LLMs. TofuEval Tang et al. ( 2024b ) provided hallucination labels on topic-focused dialogue summarization tasks with LLMs including GPT-3.5-Turbo, Vicuna Chiang et al. ( 2023 ) and WizardLM Xu et al. ( 2023 ) . Similarly, HaluEval Li et al. ( 2023 ) included ChatGPT-generated hallucinations across summarization, question answering (QA), and dialogue tasks, while RAGTruth Niu et al. ( 2024 ) also annotated responses from models including GPT-3.5, GPT-4 OpenAI ( 2023 ) , Llama-2 Touvron et al. ( 2023 ) , and Mistral Jiang et al. ( 2023 ) . FaithBench Bao et al. ( 2025 ) presents human annotations of challenging hallucinations in summaries from 10 modern LLMs from 8 different model families (detailed further in Section 4 ). Due to limited large-scale, human-annotated data for training hallucination detectors, early detection methods relied heavily on natural language inference (NLI) or question-answering (QA) systems Fabbri et al. ( 2022 ) . For instance, SummaC aggregated sentence-level NLI entailment scores between document-summary sentence pairs. AlignScore Zha et al. ( 2023 ) extended this by training detection models on multiple semantic alignment tasks evaluated at the chunk level. MiniCheck Tang et al. ( 2024a ) addressed data scarcity by synthesizing hallucinated examples using GPT-4 for model training. Modern LLMs’ strong zero-shot instruction-following capabilities have also enabled LLM-as-a-judge methods Zheng et al. ( 2023 ); Luo et al. ( 2023 ); Jacovi et al. ( 2025 ); Gao et al. ( 2023 ) . Instead of evaluating entire generated summaries, approaches like FACTSCORE Min et al. ( 2023 ) and RAGAS Es et al. ( 2024 ) decompose summaries into claims for granular hallucination detection. Like Vectara’s Hallucination Leaderboard Hughes and Bae ( 2023 ) , other efforts like FACTS Grounding Jacovi et al. ( 2025 ) and Galileo’s Hallucination Index Galileo ( 2023 ) also provide leaderboards to benchmark hallucinations in LLMs. Galileo’s Hallucination Index employs GPT-4o as a single LLM judge, whereas FACTS Grounding ensembles evaluations from three different LLM judges: GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-Pro. Nonetheless, hallucination detection remains challenging, with modest effectiveness observed across current methods. Benchmarks such as AggreFact, RAGTruth, TofuEval, and FaithBench consistently show limitations in existing hallucination detectors, including LLM-based methods. Notably, FaithBench highlighted that even the best current models achieve near 50% accuracy. Both RAGTruth and TofuEval further suggest that smaller, fine-tuned detection models can perform competitively with or even outperform LLM-based evaluation approaches.",
            "3 Vectara’s Hallucination Leaderboard": "In 2023, Vectara’s Hallucination Leaderboard Hughes and Bae ( 2023 ) was released using Vectara’s hallucination-detection model, HHEM-1.0-open. This model was later updated to HHEM-2.0 with stronger effectiveness, the ability to handle longer contexts, and multilingual capabilities. The current leaderboard relies on the open version, HHEM-2.1-open, publicly released on HuggingFace 3 3 3 https://huggingface.co/vectara/hallucination_evaluation_model . To date, HHEM has been downloaded over 3.5 million times, reflecting strong community interest and adoption. While specific training details remain confidential, we note that HHEM-2.1-open was trained using the RAGTruth training set among other datasets. To build Vectara’s Hallucination Leaderboard, articles were selected from diverse sources such as BBC News, CNN, Wikipedia, and the Daily Mail, following prior work on summarization evaluation and factuality verification Narayan et al. ( 2018 ); Maynez et al. ( 2020 ); Schuster et al. ( 2021 ); Thorne et al. ( 2018 ); Fabbri et al. ( 2021 ); Huang et al. ( 2020 ); Pagnoni et al. ( 2021 ); Hermann et al. ( 2015 ) . Articles containing objectionable or explicit content, which LLMs may refuse to summarize, were specifically excluded. The resulting dataset comprised articles with a median length of approximately 217 words (25th percentile: 42 words; 75th percentile: 424 words). LLMs are evaluated by prompting them to generate concise summaries strictly grounded in the provided passages. HHEM then assesses the proportion of summaries generated by the LLM containing hallucinations. Refusals are tracked by measuring the proportion of short responses (5 words or fewer). Users are also invited to submit specific models for evaluation. Continuously updated, the leaderboard now benchmarks hallucination rates of over 130 different LLMs, typically evaluating new models as soon as they become publicly available to track ongoing advances in the field.",
            "4 FaithBench": "FaithBench Bao et al. ( 2025 ) examined hallucinations in LLM-generated summaries and assessed the effectiveness of hallucination detection methods through human annotations. It includes summaries from ten state-of-the-art LLMs, including GPT-4o, GPT-3.5, Claude-3.5-Sonnet, Gemini-1.5-Flash Gemini Team ( 2024 ) , and open-source models like Llama-3.1 Grattafiori et al. ( 2024 ) , revealing that hallucinations remain frequent and detection methods often fail to identify them accurately. Human annotators labelled hallucinations as Unwanted when the summary contained contradictory or unsupported information, Benign when the information was supported by world knowledge, but absent from the article, or Questionable when the classification was unclear. Articles in FaithBench were selected from Vectara’s Hallucination Leaderboard based on frequent disagreements on summaries among hallucination detection models. True-NLI, TrueTeacher, HHEM-2.1-open, and GPT-4o/GPT-3.5 judges using the CoT prompt from Luo et al. ( 2023 ) were used to identify articles where summary hallucination classifications were most disagreed upon. The dataset includes 75 articles, each with ten annotated summaries from different LLMs.",
            "5 FaithJudge": "Human annotation is the gold standard for hallucination detection, but it is time-consuming and expensive. FaithJudge offers a scalable alternative by leveraging hallucination annotations to guide an LLM judge in evaluating new summaries. We also expand FaithJudge to other RAG tasks, including question-answering (QA) and writing overviews from structured data in the JSON format using the RAGTruth dataset Niu et al. ( 2024 ) . This is detailed further in the Appendix A . To assess a summary, FaithJudge involves prompting an LLM judge with other summaries of the same article, along with their corresponding hallucination annotations. These annotations include hallucination spans, source references, and labels of either Benign, Unwanted, or Questionable, identified by human annotators. To evaluate the effectiveness of FaithJudge, we use the fact that each FaithBench article has summaries from ten different LLMs. The judge is given the other nine annotated summaries as context, and its assessments on each summary from FaithBench are compared to human annotations. As shown in Section 6 , FaithJudge substantially improves automated hallucination evaluation, outperforming existing detection methods by leveraging human-labelled examples. This allows for more accurate automated hallucination evaluation, where existing hallucination detection methods continue to lag.",
            "7 Leaderboard Rankings": "Figure 3 compares the ranking of the 10 LLMs studied in FaithBench based on human-annotated hallucinations with rankings from FaithJudge and Vectara’s existing hallucination leaderboard. The left-most plot shows that rankings vary depending on the type of hallucination considered: Unwanted, Benign, or Questionable, even when assessed by human annotation. The other plots show that when considering all types of hallucination annotations, rankings in FaithBench align more closely with FaithJudge than with the existing leaderboard. FaithJudge rankings show six inversions compared to rankings from FaithBench considering Unwanted, Benign, and Questionable hallucinations, while the existing leaderboard rankings using HHEM shows 16 inversions.",
            "8 Conclusion": "In this paper, we presented our efforts at Vectara in evaluating and benchmarking hallucinations in RAG, discussing and building on our established hallucination leaderboard, and proposing FaithJudge. We identified effectiveness limitations in existing hallucination detection methods, including our own HHEM model. To address these challenges, we proposed FaithJudge, an approach that leverages human hallucination annotations to enhance automated hallucination detection, achieving greater effectiveness, but requiring annotations from summaries of the same articles. Beyond FaithBench, we extend FaithJudge to additional RAG tasks, including question answering and data-to-text generation, using annotated examples of hallucinations from the RAGTruth dataset. We discuss this further in Appendix A . We also apply FaithJudge to a broader set of LLMs, producing leaderboard-style rankings that currently include 30 models. We share some of these results in Appendix C , providing a framework for more accurate faithfulness evaluation across diverse models and RAG tasks. We hope to continue to update our leaderboard to evaluate new models and to use improved LLM judges.",
            "Acknowledgements": "We respectfully acknowledge the late Simon Mark Hughes, who led the development of the original HHEM model and Vectara’s Hallucination Leaderboard. His contributions laid important groundwork for Vectara’s ongoing research and continue to leave a lasting influence on our work.",
            "Limitations": "There are some limitations with our evaluation methodology. First, our evaluation focuses exclusively on faithfulness and does not address the overall quality or usefulness of summaries and answers. Though summary and answer quality are important in RAG applications, we consider this evaluation somewhat orthogonal to faithfulness. One issue to consider is that an extractive summarizer or an LLM that simply copies parts of or the entire article in its response would technically avoid hallucinations. Nonetheless, we maintain that evaluating LLMs through hallucinations in generated summaries is promising because these hallucinations remain persistent. Finally, while the o3-mini-high judge demonstrates strong effectiveness, there remains room for enhancing accuracy and agreement with human annotators. We hope that as LLMs continue to improve, replacing o3-mini-high in FaithJudge would allow for more accurate and reliable evaluation."
        },
        "Tuples": [
            [
                "1 Introduction",
                "LLMs excel in various tasks, but frequently produce hallucinations, generating false or misleading information unsupported by provided contexts or world knowledge Ji et al. ( 2023 ); Huang et al. ( 2025 ); Lin et al. ( 2022 ); Tang et al. ( 2023 ) . While Retrieval-Augmented Generation (RAG) approaches Guu et al. ( 2020 ); Lewis et al. ( 2020b ); Shuster et al. ( 2021 ) attempt to mitigate hallucinations by grounding responses in external trusted contexts, they do not fully eliminate hallucinations, as LLMs often introduce details unsupported by retrieved contexts, misrepresent information, or generate outright contradictions Niu et al. ( 2024 ) . An ongoing challenge within RAG is ensuring context-faithfulness Niu et al. ( 2024 ); Jia et al. ( 2023 ); Ming et al. ( 2024 ) . Detecting when LLMs deviate from the information in the provided context remains a difficult problem. Although there has been progress, hallucination detection methods, including fine-tuned detectors largely for evaluating summaries Zhou et al. ( 2021 ); Gekhman et al. ( 2023 ); Honovich et al. ( 2022 ); Zha et al. ( 2023 ); Tang et al. ( 2024a ) and LLM-as-a-judge techniques Zheng et al. ( 2023 ); Luo et al. ( 2023 ); Jacovi et al. ( 2025 ) , continue to struggle with the accurate identification of LLM-generated hallucinations. In this paper, we study and aim to improve hallucination evaluation in RAG by building on prior work in summary consistency evaluation. We analyze the capabilities and limitations of current hallucination detection methods, including fine-tuned models such as Vectara’s Hughes Hallucination Evaluation Model (HHEM) Bao et al. ( 2024 ) and zero-shot methods using LLM judges. To overcome the challenges of LLM-as-a-judge techniques for zero-shot hallucination detection and fine-tuned hallucination detection models, we introduce FaithJudge, an LLM-as-a-judge approach guided by few-shot human annotations of hallucinations. FaithJudge leverages labelled hallucinations from diverse LLM generations to automate the evaluation of LLMs on their propensity to hallucinate when summarizing the same articles or using the same articles to respond to queries. This approach results in notably higher agreement with human judgments compared with existing automated methods. Additionally, we introduce an enhanced hallucination leaderboard based on FaithJudge, enabling more reliable benchmarking of hallucinations in LLM-generated summaries and responses. We discuss both Vectara’s existing hallucination leaderboard Hughes and Bae ( 2023 ) based on HHEM and our new leaderboard based on FaithJudge. Hallucinations within RAG remain frequent and problematic, even in leading LLMs. Our approach contributes toward more accurate hallucination evaluation, aiding the development of more trustworthy generative AI systems. Vectara serves customers across diverse industries. For many of these customers, addressing hallucinations in LLM outputs is a critical priority. Driven by this customer-centric need, we developed our hallucination leaderboard and benchmarking methods, which are presented in this paper."
            ],
            [
                "2 Background",
                "Accurate hallucination detection is essential for reliably quantifying hallucination rates in LLMs. Numerous datasets have been developed for evaluating hallucinations in summarization tasks. Earlier datasets, such as SummaC Laban et al. ( 2022 ) and AggreFact Tang et al. ( 2023 ) , aggregated multiple resources, standardized labels, and classification taxonomies. However, these primarily focused on summaries from pre-ChatGPT models like fine-tuned T5 Raffel et al. ( 2020 ) , BART Lewis et al. ( 2020a ) , and PEGASUS Zhang et al. ( 2020 ) models, potentially limiting their relevance to contemporary LLMs that may produce more nuanced and difficult to identify hallucinations. Recent benchmarks address this limitation by incorporating summaries generated by modern LLMs. TofuEval Tang et al. ( 2024b ) provided hallucination labels on topic-focused dialogue summarization tasks with LLMs including GPT-3.5-Turbo, Vicuna Chiang et al. ( 2023 ) and WizardLM Xu et al. ( 2023 ) . Similarly, HaluEval Li et al. ( 2023 ) included ChatGPT-generated hallucinations across summarization, question answering (QA), and dialogue tasks, while RAGTruth Niu et al. ( 2024 ) also annotated responses from models including GPT-3.5, GPT-4 OpenAI ( 2023 ) , Llama-2 Touvron et al. ( 2023 ) , and Mistral Jiang et al. ( 2023 ) . FaithBench Bao et al. ( 2025 ) presents human annotations of challenging hallucinations in summaries from 10 modern LLMs from 8 different model families (detailed further in Section 4 ). Due to limited large-scale, human-annotated data for training hallucination detectors, early detection methods relied heavily on natural language inference (NLI) or question-answering (QA) systems Fabbri et al. ( 2022 ) . For instance, SummaC aggregated sentence-level NLI entailment scores between document-summary sentence pairs. AlignScore Zha et al. ( 2023 ) extended this by training detection models on multiple semantic alignment tasks evaluated at the chunk level. MiniCheck Tang et al. ( 2024a ) addressed data scarcity by synthesizing hallucinated examples using GPT-4 for model training. Modern LLMs’ strong zero-shot instruction-following capabilities have also enabled LLM-as-a-judge methods Zheng et al. ( 2023 ); Luo et al. ( 2023 ); Jacovi et al. ( 2025 ); Gao et al. ( 2023 ) . Instead of evaluating entire generated summaries, approaches like FACTSCORE Min et al. ( 2023 ) and RAGAS Es et al. ( 2024 ) decompose summaries into claims for granular hallucination detection. Like Vectara’s Hallucination Leaderboard Hughes and Bae ( 2023 ) , other efforts like FACTS Grounding Jacovi et al. ( 2025 ) and Galileo’s Hallucination Index Galileo ( 2023 ) also provide leaderboards to benchmark hallucinations in LLMs. Galileo’s Hallucination Index employs GPT-4o as a single LLM judge, whereas FACTS Grounding ensembles evaluations from three different LLM judges: GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-Pro. Nonetheless, hallucination detection remains challenging, with modest effectiveness observed across current methods. Benchmarks such as AggreFact, RAGTruth, TofuEval, and FaithBench consistently show limitations in existing hallucination detectors, including LLM-based methods. Notably, FaithBench highlighted that even the best current models achieve near 50% accuracy. Both RAGTruth and TofuEval further suggest that smaller, fine-tuned detection models can perform competitively with or even outperform LLM-based evaluation approaches."
            ],
            [
                "3 Vectara’s Hallucination Leaderboard",
                "In 2023, Vectara’s Hallucination Leaderboard Hughes and Bae ( 2023 ) was released using Vectara’s hallucination-detection model, HHEM-1.0-open. This model was later updated to HHEM-2.0 with stronger effectiveness, the ability to handle longer contexts, and multilingual capabilities. The current leaderboard relies on the open version, HHEM-2.1-open, publicly released on HuggingFace 3 3 3 https://huggingface.co/vectara/hallucination_evaluation_model . To date, HHEM has been downloaded over 3.5 million times, reflecting strong community interest and adoption. While specific training details remain confidential, we note that HHEM-2.1-open was trained using the RAGTruth training set among other datasets. To build Vectara’s Hallucination Leaderboard, articles were selected from diverse sources such as BBC News, CNN, Wikipedia, and the Daily Mail, following prior work on summarization evaluation and factuality verification Narayan et al. ( 2018 ); Maynez et al. ( 2020 ); Schuster et al. ( 2021 ); Thorne et al. ( 2018 ); Fabbri et al. ( 2021 ); Huang et al. ( 2020 ); Pagnoni et al. ( 2021 ); Hermann et al. ( 2015 ) . Articles containing objectionable or explicit content, which LLMs may refuse to summarize, were specifically excluded. The resulting dataset comprised articles with a median length of approximately 217 words (25th percentile: 42 words; 75th percentile: 424 words). LLMs are evaluated by prompting them to generate concise summaries strictly grounded in the provided passages. HHEM then assesses the proportion of summaries generated by the LLM containing hallucinations. Refusals are tracked by measuring the proportion of short responses (5 words or fewer). Users are also invited to submit specific models for evaluation. Continuously updated, the leaderboard now benchmarks hallucination rates of over 130 different LLMs, typically evaluating new models as soon as they become publicly available to track ongoing advances in the field."
            ],
            [
                "4 FaithBench",
                "FaithBench Bao et al. ( 2025 ) examined hallucinations in LLM-generated summaries and assessed the effectiveness of hallucination detection methods through human annotations. It includes summaries from ten state-of-the-art LLMs, including GPT-4o, GPT-3.5, Claude-3.5-Sonnet, Gemini-1.5-Flash Gemini Team ( 2024 ) , and open-source models like Llama-3.1 Grattafiori et al. ( 2024 ) , revealing that hallucinations remain frequent and detection methods often fail to identify them accurately. Human annotators labelled hallucinations as Unwanted when the summary contained contradictory or unsupported information, Benign when the information was supported by world knowledge, but absent from the article, or Questionable when the classification was unclear. Articles in FaithBench were selected from Vectara’s Hallucination Leaderboard based on frequent disagreements on summaries among hallucination detection models. True-NLI, TrueTeacher, HHEM-2.1-open, and GPT-4o/GPT-3.5 judges using the CoT prompt from Luo et al. ( 2023 ) were used to identify articles where summary hallucination classifications were most disagreed upon. The dataset includes 75 articles, each with ten annotated summaries from different LLMs."
            ],
            [
                "5 FaithJudge",
                "Human annotation is the gold standard for hallucination detection, but it is time-consuming and expensive. FaithJudge offers a scalable alternative by leveraging hallucination annotations to guide an LLM judge in evaluating new summaries. We also expand FaithJudge to other RAG tasks, including question-answering (QA) and writing overviews from structured data in the JSON format using the RAGTruth dataset Niu et al. ( 2024 ) . This is detailed further in the Appendix A . To assess a summary, FaithJudge involves prompting an LLM judge with other summaries of the same article, along with their corresponding hallucination annotations. These annotations include hallucination spans, source references, and labels of either Benign, Unwanted, or Questionable, identified by human annotators. To evaluate the effectiveness of FaithJudge, we use the fact that each FaithBench article has summaries from ten different LLMs. The judge is given the other nine annotated summaries as context, and its assessments on each summary from FaithBench are compared to human annotations. As shown in Section 6 , FaithJudge substantially improves automated hallucination evaluation, outperforming existing detection methods by leveraging human-labelled examples. This allows for more accurate automated hallucination evaluation, where existing hallucination detection methods continue to lag."
            ],
            [
                "7 Leaderboard Rankings",
                "Figure 3 compares the ranking of the 10 LLMs studied in FaithBench based on human-annotated hallucinations with rankings from FaithJudge and Vectara’s existing hallucination leaderboard. The left-most plot shows that rankings vary depending on the type of hallucination considered: Unwanted, Benign, or Questionable, even when assessed by human annotation. The other plots show that when considering all types of hallucination annotations, rankings in FaithBench align more closely with FaithJudge than with the existing leaderboard. FaithJudge rankings show six inversions compared to rankings from FaithBench considering Unwanted, Benign, and Questionable hallucinations, while the existing leaderboard rankings using HHEM shows 16 inversions."
            ],
            [
                "8 Conclusion",
                "In this paper, we presented our efforts at Vectara in evaluating and benchmarking hallucinations in RAG, discussing and building on our established hallucination leaderboard, and proposing FaithJudge. We identified effectiveness limitations in existing hallucination detection methods, including our own HHEM model. To address these challenges, we proposed FaithJudge, an approach that leverages human hallucination annotations to enhance automated hallucination detection, achieving greater effectiveness, but requiring annotations from summaries of the same articles. Beyond FaithBench, we extend FaithJudge to additional RAG tasks, including question answering and data-to-text generation, using annotated examples of hallucinations from the RAGTruth dataset. We discuss this further in Appendix A . We also apply FaithJudge to a broader set of LLMs, producing leaderboard-style rankings that currently include 30 models. We share some of these results in Appendix C , providing a framework for more accurate faithfulness evaluation across diverse models and RAG tasks. We hope to continue to update our leaderboard to evaluate new models and to use improved LLM judges."
            ],
            [
                "Acknowledgements",
                "We respectfully acknowledge the late Simon Mark Hughes, who led the development of the original HHEM model and Vectara’s Hallucination Leaderboard. His contributions laid important groundwork for Vectara’s ongoing research and continue to leave a lasting influence on our work."
            ],
            [
                "Limitations",
                "There are some limitations with our evaluation methodology. First, our evaluation focuses exclusively on faithfulness and does not address the overall quality or usefulness of summaries and answers. Though summary and answer quality are important in RAG applications, we consider this evaluation somewhat orthogonal to faithfulness. One issue to consider is that an extractive summarizer or an LLM that simply copies parts of or the entire article in its response would technically avoid hallucinations. Nonetheless, we maintain that evaluating LLMs through hallucinations in generated summaries is promising because these hallucinations remain persistent. Finally, while the o3-mini-high judge demonstrates strong effectiveness, there remains room for enhancing accuracy and agreement with human annotators. We hope that as LLMs continue to improve, replacing o3-mini-high in FaithJudge would allow for more accurate and reliable evaluation."
            ]
        ],
        "section_summaries": [
            [
                "1 Introduction",
                "LLMs excel in various tasks, but frequently produce hallucinations, generating false or misleading information unsupported by provided contexts or world knowledge Ji et al. ( 2023 ); Huang et al. ( 2025 ); Lin et al. ( 2022 ); Tang et al. ( 2023 ) . While Retrieval-Augmented Generation (RAG) approaches Guu et al. ( 2020 ); Lewis et al. ( 2020b ); Shuster et al. ( 2021 ) attempt to mitigate hallucinations by grounding responses in external trusted contexts, they do not fully eliminate hallucinations, as LLMs often introduce details unsupported by retrieved contexts, misrepresent information, or generate outright contradictions Niu et al. ( 2024 ) . An ongoing challenge within RAG is ensuring context-faithfulness Niu et al. ( 2024 ); Jia et al. ( 2023 ); Ming et al. ( 2024 ) . Detecting when LLMs deviate from the information in the provided context remains a difficult problem. Although there has been progress, hallucination detection methods, including fine-tuned detectors largely for evaluating summaries Zhou et al. ( 2021 ); Gekhman et al. ( 2023 ); Honovich et al. ( 2022 ); Zha et al. ( 2023 ); Tang et al. ( 2024a ) and LLM-as-a-judge techniques Zheng et al. ( 2023 ); Luo et al. ( 2023 ); Jacovi et al. ( 2025 ) , continue to struggle with the accurate identification of LLM-generated hallucinations. In this paper, we study and aim to improve hallucination evaluation in RAG by building on prior work in summary consistency evaluation. We analyze the capabilities and limitations of current hallucination detection methods, including fine-tuned models such as Vectara’s Hughes Hallucination Evaluation Model (HHEM) Bao et al. ( 2024 ) and zero-shot methods using LLM judges. To overcome the challenges of LLM-as-a-judge techniques for zero-shot hallucination detection and fine-tuned hallucination detection models, we introduce FaithJudge, an LLM-as-a-judge approach guided by few-shot human annotations of hallucinations. FaithJudge leverages labelled hallucinations from diverse LLM generations to automate the evaluation of LLMs on their propensity to hallucinate when summarizing the same articles or using the same articles to respond to queries. This approach results in notably higher agreement with human judgments compared with existing automated methods. Additionally, we introduce an enhanced hallucination leaderboard based on FaithJudge, enabling more reliable benchmarking of hallucinations in LLM-generated summaries and responses. We discuss both Vectara’s existing hallucination leaderboard Hughes and Bae ( 2023 ) based on HHEM and our new leaderboard based on FaithJudge. Hallucinations within RAG remain frequent and problematic, even in leading LLMs. Our approach contributes toward more accurate hallucination evaluation, aiding the development of more trustworthy generative AI systems. Vectara serves customers across diverse industries. For many of these customers, addressing hallucinations in LLM outputs is a critical priority. Driven by this customer-centric need, we developed our hallucination leaderboard and benchmarking methods, which are presented in this paper.",
                "LLMs often produce hallucinations, even with RAG approaches that attempt to mitigate them. Current hallucination detection methods struggle to accurately identify LLM-generated hallucinations. We introduce FaithJudge, an LLM-as-a-judge approach guided by few-shot human annotations of hallucinations, which results in higher agreement with human judgments compared to existing automated methods. We also present an enhanced hallucination leaderboard based on FaithJudge for more reliable benchmarking of hallucinations in LLM-generated summaries and responses."
            ],
            [
                "2 Background",
                "Accurate hallucination detection is essential for reliably quantifying hallucination rates in LLMs. Numerous datasets have been developed for evaluating hallucinations in summarization tasks. Earlier datasets, such as SummaC Laban et al. ( 2022 ) and AggreFact Tang et al. ( 2023 ) , aggregated multiple resources, standardized labels, and classification taxonomies. However, these primarily focused on summaries from pre-ChatGPT models like fine-tuned T5 Raffel et al. ( 2020 ) , BART Lewis et al. ( 2020a ) , and PEGASUS Zhang et al. ( 2020 ) models, potentially limiting their relevance to contemporary LLMs that may produce more nuanced and difficult to identify hallucinations. Recent benchmarks address this limitation by incorporating summaries generated by modern LLMs. TofuEval Tang et al. ( 2024b ) provided hallucination labels on topic-focused dialogue summarization tasks with LLMs including GPT-3.5-Turbo, Vicuna Chiang et al. ( 2023 ) and WizardLM Xu et al. ( 2023 ) . Similarly, HaluEval Li et al. ( 2023 ) included ChatGPT-generated hallucinations across summarization, question answering (QA), and dialogue tasks, while RAGTruth Niu et al. ( 2024 ) also annotated responses from models including GPT-3.5, GPT-4 OpenAI ( 2023 ) , Llama-2 Touvron et al. ( 2023 ) , and Mistral Jiang et al. ( 2023 ) . FaithBench Bao et al. ( 2025 ) presents human annotations of challenging hallucinations in summaries from 10 modern LLMs from 8 different model families (detailed further in Section 4 ). Due to limited large-scale, human-annotated data for training hallucination detectors, early detection methods relied heavily on natural language inference (NLI) or question-answering (QA) systems Fabbri et al. ( 2022 ) . For instance, SummaC aggregated sentence-level NLI entailment scores between document-summary sentence pairs. AlignScore Zha et al. ( 2023 ) extended this by training detection models on multiple semantic alignment tasks evaluated at the chunk level. MiniCheck Tang et al. ( 2024a ) addressed data scarcity by synthesizing hallucinated examples using GPT-4 for model training. Modern LLMs’ strong zero-shot instruction-following capabilities have also enabled LLM-as-a-judge methods Zheng et al. ( 2023 ); Luo et al. ( 2023 ); Jacovi et al. ( 2025 ); Gao et al. ( 2023 ) . Instead of evaluating entire generated summaries, approaches like FACTSCORE Min et al. ( 2023 ) and RAGAS Es et al. ( 2024 ) decompose summaries into claims for granular hallucination detection. Like Vectara’s Hallucination Leaderboard Hughes and Bae ( 2023 ) , other efforts like FACTS Grounding Jacovi et al. ( 2025 ) and Galileo’s Hallucination Index Galileo ( 2023 ) also provide leaderboards to benchmark hallucinations in LLMs. Galileo’s Hallucination Index employs GPT-4o as a single LLM judge, whereas FACTS Grounding ensembles evaluations from three different LLM judges: GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-Pro. Nonetheless, hallucination detection remains challenging, with modest effectiveness observed across current methods. Benchmarks such as AggreFact, RAGTruth, TofuEval, and FaithBench consistently show limitations in existing hallucination detectors, including LLM-based methods. Notably, FaithBench highlighted that even the best current models achieve near 50% accuracy. Both RAGTruth and TofuEval further suggest that smaller, fine-tuned detection models can perform competitively with or even outperform LLM-based evaluation approaches.",
                "SummaC Laban et al., AggreFact Tang et al. primarily focused on pre-ChatGPT models like T5 Raffel et al., BART Lewis et al., and PEGASUS Zhang et al. However, recent benchmarks address this limitation by incorporating summaries generated by modern LLMs including GPT-3.5-Turbo, Vicuna Chiang et al., WizardLM Xu et al., HaluEval Li et al., RAGTruth Niu et al., FaithBench Bao et al.\n\nRAGTruth and TofuEval suggest smaller, fine-tuned detection models can perform competitively with or even outperform LLM-based evaluation approaches."
            ],
            [
                "3 Vectara’s Hallucination Leaderboard",
                "In 2023, Vectara’s Hallucination Leaderboard Hughes and Bae ( 2023 ) was released using Vectara’s hallucination-detection model, HHEM-1.0-open. This model was later updated to HHEM-2.0 with stronger effectiveness, the ability to handle longer contexts, and multilingual capabilities. The current leaderboard relies on the open version, HHEM-2.1-open, publicly released on HuggingFace 3 3 3 https://huggingface.co/vectara/hallucination_evaluation_model . To date, HHEM has been downloaded over 3.5 million times, reflecting strong community interest and adoption. While specific training details remain confidential, we note that HHEM-2.1-open was trained using the RAGTruth training set among other datasets. To build Vectara’s Hallucination Leaderboard, articles were selected from diverse sources such as BBC News, CNN, Wikipedia, and the Daily Mail, following prior work on summarization evaluation and factuality verification Narayan et al. ( 2018 ); Maynez et al. ( 2020 ); Schuster et al. ( 2021 ); Thorne et al. ( 2018 ); Fabbri et al. ( 2021 ); Huang et al. ( 2020 ); Pagnoni et al. ( 2021 ); Hermann et al. ( 2015 ) . Articles containing objectionable or explicit content, which LLMs may refuse to summarize, were specifically excluded. The resulting dataset comprised articles with a median length of approximately 217 words (25th percentile: 42 words; 75th percentile: 424 words). LLMs are evaluated by prompting them to generate concise summaries strictly grounded in the provided passages. HHEM then assesses the proportion of summaries generated by the LLM containing hallucinations. Refusals are tracked by measuring the proportion of short responses (5 words or fewer). Users are also invited to submit specific models for evaluation. Continuously updated, the leaderboard now benchmarks hallucination rates of over 130 different LLMs, typically evaluating new models as soon as they become publicly available to track ongoing advances in the field.",
                "Vectara's Hallucination Leaderboard was released in 2023 using HHEM-1.0-open and later updated to HHEM-2.1-open on HuggingFace, with over 3.5 million downloads. The leaderboard tracks hallucinations in LLMs, excluding objectionable content. It evaluates models by generating concise summaries grounded in passages, assessing hallucination rates. Over 130 LLMs are benchmarked, with new models evaluated upon release."
            ],
            [
                "4 FaithBench",
                "FaithBench Bao et al. ( 2025 ) examined hallucinations in LLM-generated summaries and assessed the effectiveness of hallucination detection methods through human annotations. It includes summaries from ten state-of-the-art LLMs, including GPT-4o, GPT-3.5, Claude-3.5-Sonnet, Gemini-1.5-Flash Gemini Team ( 2024 ) , and open-source models like Llama-3.1 Grattafiori et al. ( 2024 ) , revealing that hallucinations remain frequent and detection methods often fail to identify them accurately. Human annotators labelled hallucinations as Unwanted when the summary contained contradictory or unsupported information, Benign when the information was supported by world knowledge, but absent from the article, or Questionable when the classification was unclear. Articles in FaithBench were selected from Vectara’s Hallucination Leaderboard based on frequent disagreements on summaries among hallucination detection models. True-NLI, TrueTeacher, HHEM-2.1-open, and GPT-4o/GPT-3.5 judges using the CoT prompt from Luo et al. ( 2023 ) were used to identify articles where summary hallucination classifications were most disagreed upon. The dataset includes 75 articles, each with ten annotated summaries from different LLMs.",
                "Bao et al. (2025) examined hallucinations in LLM-generated summaries using FaithBench, a dataset of 75 articles with annotations from 10 state-of-the-art LLMs. Hallucinations were detected as Unwanted, Benign, or Questionable by human annotators based on CoT prompts from Luo et al. (2023). Disagreements among models led to article selection for FaithBench, revealing frequent hallucinations and detection failures in GPT-4o, GPT-3.5, Gemini-1.5, Llama-3.1, True-NLI, TrueTeacher, HHEM-2.1-open, and others."
            ],
            [
                "5 FaithJudge",
                "Human annotation is the gold standard for hallucination detection, but it is time-consuming and expensive. FaithJudge offers a scalable alternative by leveraging hallucination annotations to guide an LLM judge in evaluating new summaries. We also expand FaithJudge to other RAG tasks, including question-answering (QA) and writing overviews from structured data in the JSON format using the RAGTruth dataset Niu et al. ( 2024 ) . This is detailed further in the Appendix A . To assess a summary, FaithJudge involves prompting an LLM judge with other summaries of the same article, along with their corresponding hallucination annotations. These annotations include hallucination spans, source references, and labels of either Benign, Unwanted, or Questionable, identified by human annotators. To evaluate the effectiveness of FaithJudge, we use the fact that each FaithBench article has summaries from ten different LLMs. The judge is given the other nine annotated summaries as context, and its assessments on each summary from FaithBench are compared to human annotations. As shown in Section 6 , FaithJudge substantially improves automated hallucination evaluation, outperforming existing detection methods by leveraging human-labelled examples. This allows for more accurate automated hallucination evaluation, where existing hallucination detection methods continue to lag.",
                "FaithJudge offers a scalable alternative to human annotation for hallucination detection using LLM judges guided by hallucination annotations. It improves automated hallucination evaluation by leveraging human-labelled examples, outperforming existing detection methods. \n\nNiu et al., 2024 \nRAGTruth dataset\nLLM (Large Language Model)\nFaithBench\nJSON"
            ],
            [
                "7 Leaderboard Rankings",
                "Figure 3 compares the ranking of the 10 LLMs studied in FaithBench based on human-annotated hallucinations with rankings from FaithJudge and Vectara’s existing hallucination leaderboard. The left-most plot shows that rankings vary depending on the type of hallucination considered: Unwanted, Benign, or Questionable, even when assessed by human annotation. The other plots show that when considering all types of hallucination annotations, rankings in FaithBench align more closely with FaithJudge than with the existing leaderboard. FaithJudge rankings show six inversions compared to rankings from FaithBench considering Unwanted, Benign, and Questionable hallucinations, while the existing leaderboard rankings using HHEM shows 16 inversions.",
                "10 LLMs were studied in FaithBench. Rankings varied by type of hallucination: Unwanted, Benign, or Questionable. FaithJudge had 6 inversions with FaithBench's combined rankings and existing leaderboard had 16 inversions with HHEM."
            ],
            [
                "8 Conclusion",
                "In this paper, we presented our efforts at Vectara in evaluating and benchmarking hallucinations in RAG, discussing and building on our established hallucination leaderboard, and proposing FaithJudge. We identified effectiveness limitations in existing hallucination detection methods, including our own HHEM model. To address these challenges, we proposed FaithJudge, an approach that leverages human hallucination annotations to enhance automated hallucination detection, achieving greater effectiveness, but requiring annotations from summaries of the same articles. Beyond FaithBench, we extend FaithJudge to additional RAG tasks, including question answering and data-to-text generation, using annotated examples of hallucinations from the RAGTruth dataset. We discuss this further in Appendix A . We also apply FaithJudge to a broader set of LLMs, producing leaderboard-style rankings that currently include 30 models. We share some of these results in Appendix C , providing a framework for more accurate faithfulness evaluation across diverse models and RAG tasks. We hope to continue to update our leaderboard to evaluate new models and to use improved LLM judges.",
                "We proposed FaithJudge, an approach that leverages human annotations to enhance automated hallucination detection, achieving greater effectiveness in 30 models on RAG tasks, including question answering and data-to-text generation. \n\nHHEM model was identified with limitations, and a leaderboard-style ranking is provided for diverse models and tasks."
            ],
            [
                "Acknowledgements",
                "We respectfully acknowledge the late Simon Mark Hughes, who led the development of the original HHEM model and Vectara’s Hallucination Leaderboard. His contributions laid important groundwork for Vectara’s ongoing research and continue to leave a lasting influence on our work.",
                "Simon Mark Hughes, HHEM model, Vectara's Hallucination Leaderboard, Vectara."
            ],
            [
                "Limitations",
                "There are some limitations with our evaluation methodology. First, our evaluation focuses exclusively on faithfulness and does not address the overall quality or usefulness of summaries and answers. Though summary and answer quality are important in RAG applications, we consider this evaluation somewhat orthogonal to faithfulness. One issue to consider is that an extractive summarizer or an LLM that simply copies parts of or the entire article in its response would technically avoid hallucinations. Nonetheless, we maintain that evaluating LLMs through hallucinations in generated summaries is promising because these hallucinations remain persistent. Finally, while the o3-mini-high judge demonstrates strong effectiveness, there remains room for enhancing accuracy and agreement with human annotators. We hope that as LLMs continue to improve, replacing o3-mini-high in FaithJudge would allow for more accurate and reliable evaluation.",
                "Our methodology has limitations, specifically focusing on faithfulness without assessing overall quality or usefulness. Evaluating hallucinations in generated summaries is still promising despite potential issues with extractive summarizers or LLMs that copy content. The o3-mini-high judge's effectiveness can be improved with enhanced accuracy and agreement with human annotators."
            ]
        ],
        "general_summary": "**Research Overview**\nThis paper addresses the problem of LLM (Large Language Model) hallucinations in RAG (Reinforced Adaptive Generator), a method aimed at mitigating these hallucinations by grounding responses in contexts. However, even when provided context, LLMs often introduce unsupported information or contradictions.\n\n**Methodology & Approach**\nThe proposed solution is FaithJudge, an LLM-as-a-judge approach guided by few-shot human hallucination annotations. This approach leverages human-labelled examples to improve automated hallucination evaluation and outperforms existing detection methods. The researchers also present an enhanced hallucination leaderboard centered on FaithJudge for more reliable benchmarking of LLMs for hallucinations in RAG.\n\n**Key Contributions & Findings**\nThe main contributions of this work are the development of FaithJudge, a scalable approach to automated hallucination evaluation, and an enhanced hallucination leaderboard. The researchers demonstrate that FaithJudge achieves higher agreement with human judgments compared to existing automated methods, resulting in improved hallucination detection. They also report that FaithJudge outperforms existing detection methods on 30 models across various RAG tasks.\n\n**Technical Details**\nFaithJudge uses LLM judges guided by hallucination annotations to detect and rank hallucinations in generated summaries. The approach leverages human-labelled examples from the RAGTruth dataset and the FaithBench dataset, which contains annotated articles with summaries generated by state-of-the-art LLMs. The enhanced hallucination leaderboard is centered on FaithJudge and tracks hallucinations in over 130 LLMs.\n\n**Limitations & Future Work**\nThe researchers acknowledge that their methodology has limitations, primarily focusing on faithfulness without assessing overall quality or usefulness. They also note potential issues with extractive summarizers or LLMs that copy content. The effectiveness of the o3-mini-high judge can be improved with enhanced accuracy and agreement with human annotators.\n\n**Practical Implications**\nThis research provides a more reliable benchmarking system for evaluating hallucinations in LLM-generated summaries, enabling developers to improve their models' faithfulness and overall quality. The proposed approach has practical implications for improving the reliability of LLMs in various applications, including question answering and data-to-text generation."
    },
    {
        "id": "2506.00448v1",
        "title": "Fact-Controlled Diagnosis of Hallucinations in Medical Text\n  Summarization",
        "authors": [
            "Suhas BN",
            "Han-Chin Shing",
            "Lei Xu",
            "Mitch Strong",
            "Jon Burnsky",
            "Jessica Ofor",
            "Jordan R. Mason",
            "Susan Chen",
            "Sundararajan Srinivasan",
            "Chaitanya Shivade",
            "Jack Moriarty",
            "Joseph Paul Cohen"
        ],
        "url": "https://arxiv.org/abs/2506.00448v1",
        "Abstract": "Abstract Hallucinations in large language models (LLMs) during summarization of patient-clinician dialogues pose significant risks to patient care and clinical decision-making.\nHowever, the phenomenon remains understudied in the clinical domain, with uncertainty surrounding the applicability of general-domain hallucination detectors.\nThe rarity and randomness of hallucinations further complicate their investigation.\nIn this paper, we conduct an evaluation of hallucination detection methods in the medical domain, and construct two datasets for the purpose: A fact-controlled Leave-N-out dataset – generated by systematically removing facts from source dialogues to induce hallucinated content in summaries; and a natural hallucination dataset – arising organically during LLM-based medical summarization.\nWe show that general-domain detectors struggle to detect clinical hallucinations, and that performance on fact-controlled hallucinations does not reliably predict effectiveness on natural hallucinations.\nWe then develop fact-based approaches that count hallucinations, offering explainability not available with existing methods.\nNotably, our LLM-based detectors, which we developed using fact-controlled hallucinations, generalize well to detecting real-world clinical hallucinations.\nThis research contributes a suite of specialized metrics supported by expert-annotated datasets to advance faithful clinical summarization systems.",
        "Main": {
            "1 Introduction": "The ability to generate precise summaries of patient-clinician dialogues and medical documentation is fundamental to healthcare delivery, enabling rapid comprehension of patient histories and informed clinical decision-making while reducing clinician’s burnout [ 1 , 2 , 3 ] . Language models have enabled the development of automated clinical text summarization systems. However, a major challenge with these systems is the problem of “hallucination” - when the summary includes information that is not present in or misinterpreted from the original text. Hallucinations in clinical summaries pose significant risks to patient care, potentially leading to misdiagnosis, inappropriate treatment decisions, erosion of trust between patients and providers, as well as between providers and their software vendors. As healthcare increasingly adopts automated summarization tools, the ability to detect and prevent such hallucinations becomes crucial for ensuring patient safety and maintaining clinical accuracy. The development of effective hallucination detection methods in clinical summarization is hindered by the lack of suitable datasets. Existing datasets often lack the specificity required for clinical contexts, do not provide controllable hallucinations, and may not capture the granular errors that can occur in medical summaries. To address these limitations, we propose the creation of a fact-controlled synthetic dataset that allows for precise control over the type and extent of hallucinations. We complement this synthetic dataset with another dataset consisting of natural hallucinations to ensure that we can measure performance in both a closed controllable setting and in real-world scenarios. In this work, we systematically explore how existing methods detect hallucinations in clinical text summarization. We utilize two novel datasets created with clinical experts: Leave-N-Out (LNO) and Natural Hallucination (NH), which have omitted facts in different ways to exercise the capabilities of hallucination detection methods. This paper aims to address the following research questions: RQ1: How can we evaluate hallucination detection for medical tasks? RQ2: Does the performance of hallucination detection metrics on synthetic data transfer to real-world clinical summarization data? RQ3: How best can fact based hallucination detection be implemented? To investigate these questions, we evaluate many baseline metrics, including classical, entailment-based, and LLM-based approaches on datasets described in § 3 . Additionally, we develop many fact based methods for clinical summarization in § 4 . Our findings demonstrate that LLM-based methods, particularly those using fact alignment, significantly outperform traditional metrics in hallucination detection. These methods show strong generalization, maintaining high performance across both synthetic and real-world datasets. Notably, our proposed methods also excel in detecting high-severity hallucinations, crucial for ensuring patient safety in clinical applications.",
            "2 Related Work": "Traditional lexical and semantic overlap metrics such as ROUGE [ 4 ] , BLEU [ 5 ] , and BERT-Score [ 6 ] offer straightforward implementation but struggle with semantic understanding. More sophisticated approaches include entailment-based metrics (e.g., FactCC [ 7 ] , SummaC [ 8 ] , AlignScore [ 9 ] ), question-answering based metrics (e.g., FEQA [ 10 ] , QuestEval [ 11 ] , Q 2 [ 12 ] ), and information theory-based metrics (e.g., InfoLM [ 13 ] ). Recent developments include ensemble approaches like FENICE [ 14 ] and LLM-based metrics such as DocLens [ 15 ] . While DocLens represents an innovative approach to medical text evaluation, its methodology, which relies on claim generation and citation matching, doesn’t directly address the need for hallucination detection in clinical summaries. Furthermore, its partial dependence on reference summaries conflicts with our goal of developing reference-free evaluation methods.",
            "3 Hallucination Datasets": "We focus our research on the ACI-Bench dataset [ 16 ] , which comprises clinician-patient conversation transcripts and their corresponding clinical summaries (SOAP Notes [ 17 ] ). A SOAP note is a structured way for healthcare providers to document patient information, including Subjective (patient reports), Objective (observable data), Assessment (provider’s evaluation), and Plan (treatment strategy). The transcripts are generally longer and use layman’s terms, while the summaries are written using medical terminology.",
            "5 Conclusion": "We presented a novel fact-controlled approach to generate domain specific evaluation benchmarks using fact removal to rewrite the input in the LNO dataset. This approach makes it tractable to generate domain specific evaluation data when natural hallucinations are difficult to find in large numbers. While this dataset was helpful in developing methods, high performance on this task didn’t always translate into high performance on the real-world natural hallucination dataset. Existing baseline methods show limited effectiveness on detecting clinical hallucinations, highlighting the need for specialized approaches. We also found that existing methods didn’t generate an interpretable score as much as a count of hallucinations could and incorporated that into our approaches. We demonstrate fact based approaches offer clear explainability into what the hallucinations are as shown in Figure 2 . Our proposed LLM-based evaluation methods, particularly fact alignment, generalize well across datasets and outperform existing metrics, achieving correlations up to 0.43 on synthetic data and 0.37 on real-world data. This suggests that decomposing evaluation into atomic fact verification provides a more robust framework for hallucination detection.",
            "6 Limitations": "The study primarily uses the ACI-Bench dataset, which may not represent the full complexity of clinical language. Additionally, our research focuses solely on SOAP notes, excluding other clinical documents like discharge summaries or progress reports. Fact based analysis is limited by the definition of what is a fact. While we tried to prompt models to extract atomic facts this often didn’t work leading to undercounting of hallucinations. This study also utilizes LLM-based evaluation methods which are computationally expensive, potentially limiting their practical application which may limit the utility."
        },
        "Tuples": [
            [
                "1 Introduction",
                "The ability to generate precise summaries of patient-clinician dialogues and medical documentation is fundamental to healthcare delivery, enabling rapid comprehension of patient histories and informed clinical decision-making while reducing clinician’s burnout [ 1 , 2 , 3 ] . Language models have enabled the development of automated clinical text summarization systems. However, a major challenge with these systems is the problem of “hallucination” - when the summary includes information that is not present in or misinterpreted from the original text. Hallucinations in clinical summaries pose significant risks to patient care, potentially leading to misdiagnosis, inappropriate treatment decisions, erosion of trust between patients and providers, as well as between providers and their software vendors. As healthcare increasingly adopts automated summarization tools, the ability to detect and prevent such hallucinations becomes crucial for ensuring patient safety and maintaining clinical accuracy. The development of effective hallucination detection methods in clinical summarization is hindered by the lack of suitable datasets. Existing datasets often lack the specificity required for clinical contexts, do not provide controllable hallucinations, and may not capture the granular errors that can occur in medical summaries. To address these limitations, we propose the creation of a fact-controlled synthetic dataset that allows for precise control over the type and extent of hallucinations. We complement this synthetic dataset with another dataset consisting of natural hallucinations to ensure that we can measure performance in both a closed controllable setting and in real-world scenarios. In this work, we systematically explore how existing methods detect hallucinations in clinical text summarization. We utilize two novel datasets created with clinical experts: Leave-N-Out (LNO) and Natural Hallucination (NH), which have omitted facts in different ways to exercise the capabilities of hallucination detection methods. This paper aims to address the following research questions: RQ1: How can we evaluate hallucination detection for medical tasks? RQ2: Does the performance of hallucination detection metrics on synthetic data transfer to real-world clinical summarization data? RQ3: How best can fact based hallucination detection be implemented? To investigate these questions, we evaluate many baseline metrics, including classical, entailment-based, and LLM-based approaches on datasets described in § 3 . Additionally, we develop many fact based methods for clinical summarization in § 4 . Our findings demonstrate that LLM-based methods, particularly those using fact alignment, significantly outperform traditional metrics in hallucination detection. These methods show strong generalization, maintaining high performance across both synthetic and real-world datasets. Notably, our proposed methods also excel in detecting high-severity hallucinations, crucial for ensuring patient safety in clinical applications."
            ],
            [
                "2 Related Work",
                "Traditional lexical and semantic overlap metrics such as ROUGE [ 4 ] , BLEU [ 5 ] , and BERT-Score [ 6 ] offer straightforward implementation but struggle with semantic understanding. More sophisticated approaches include entailment-based metrics (e.g., FactCC [ 7 ] , SummaC [ 8 ] , AlignScore [ 9 ] ), question-answering based metrics (e.g., FEQA [ 10 ] , QuestEval [ 11 ] , Q 2 [ 12 ] ), and information theory-based metrics (e.g., InfoLM [ 13 ] ). Recent developments include ensemble approaches like FENICE [ 14 ] and LLM-based metrics such as DocLens [ 15 ] . While DocLens represents an innovative approach to medical text evaluation, its methodology, which relies on claim generation and citation matching, doesn’t directly address the need for hallucination detection in clinical summaries. Furthermore, its partial dependence on reference summaries conflicts with our goal of developing reference-free evaluation methods."
            ],
            [
                "3 Hallucination Datasets",
                "We focus our research on the ACI-Bench dataset [ 16 ] , which comprises clinician-patient conversation transcripts and their corresponding clinical summaries (SOAP Notes [ 17 ] ). A SOAP note is a structured way for healthcare providers to document patient information, including Subjective (patient reports), Objective (observable data), Assessment (provider’s evaluation), and Plan (treatment strategy). The transcripts are generally longer and use layman’s terms, while the summaries are written using medical terminology."
            ],
            [
                "5 Conclusion",
                "We presented a novel fact-controlled approach to generate domain specific evaluation benchmarks using fact removal to rewrite the input in the LNO dataset. This approach makes it tractable to generate domain specific evaluation data when natural hallucinations are difficult to find in large numbers. While this dataset was helpful in developing methods, high performance on this task didn’t always translate into high performance on the real-world natural hallucination dataset. Existing baseline methods show limited effectiveness on detecting clinical hallucinations, highlighting the need for specialized approaches. We also found that existing methods didn’t generate an interpretable score as much as a count of hallucinations could and incorporated that into our approaches. We demonstrate fact based approaches offer clear explainability into what the hallucinations are as shown in Figure 2 . Our proposed LLM-based evaluation methods, particularly fact alignment, generalize well across datasets and outperform existing metrics, achieving correlations up to 0.43 on synthetic data and 0.37 on real-world data. This suggests that decomposing evaluation into atomic fact verification provides a more robust framework for hallucination detection."
            ],
            [
                "6 Limitations",
                "The study primarily uses the ACI-Bench dataset, which may not represent the full complexity of clinical language. Additionally, our research focuses solely on SOAP notes, excluding other clinical documents like discharge summaries or progress reports. Fact based analysis is limited by the definition of what is a fact. While we tried to prompt models to extract atomic facts this often didn’t work leading to undercounting of hallucinations. This study also utilizes LLM-based evaluation methods which are computationally expensive, potentially limiting their practical application which may limit the utility."
            ]
        ],
        "section_summaries": [
            [
                "1 Introduction",
                "The ability to generate precise summaries of patient-clinician dialogues and medical documentation is fundamental to healthcare delivery, enabling rapid comprehension of patient histories and informed clinical decision-making while reducing clinician’s burnout [ 1 , 2 , 3 ] . Language models have enabled the development of automated clinical text summarization systems. However, a major challenge with these systems is the problem of “hallucination” - when the summary includes information that is not present in or misinterpreted from the original text. Hallucinations in clinical summaries pose significant risks to patient care, potentially leading to misdiagnosis, inappropriate treatment decisions, erosion of trust between patients and providers, as well as between providers and their software vendors. As healthcare increasingly adopts automated summarization tools, the ability to detect and prevent such hallucinations becomes crucial for ensuring patient safety and maintaining clinical accuracy. The development of effective hallucination detection methods in clinical summarization is hindered by the lack of suitable datasets. Existing datasets often lack the specificity required for clinical contexts, do not provide controllable hallucinations, and may not capture the granular errors that can occur in medical summaries. To address these limitations, we propose the creation of a fact-controlled synthetic dataset that allows for precise control over the type and extent of hallucinations. We complement this synthetic dataset with another dataset consisting of natural hallucinations to ensure that we can measure performance in both a closed controllable setting and in real-world scenarios. In this work, we systematically explore how existing methods detect hallucinations in clinical text summarization. We utilize two novel datasets created with clinical experts: Leave-N-Out (LNO) and Natural Hallucination (NH), which have omitted facts in different ways to exercise the capabilities of hallucination detection methods. This paper aims to address the following research questions: RQ1: How can we evaluate hallucination detection for medical tasks? RQ2: Does the performance of hallucination detection metrics on synthetic data transfer to real-world clinical summarization data? RQ3: How best can fact based hallucination detection be implemented? To investigate these questions, we evaluate many baseline metrics, including classical, entailment-based, and LLM-based approaches on datasets described in § 3 . Additionally, we develop many fact based methods for clinical summarization in § 4 . Our findings demonstrate that LLM-based methods, particularly those using fact alignment, significantly outperform traditional metrics in hallucination detection. These methods show strong generalization, maintaining high performance across both synthetic and real-world datasets. Notably, our proposed methods also excel in detecting high-severity hallucinations, crucial for ensuring patient safety in clinical applications.",
                "Language models are used to develop automated clinical text summarization systems but often result in \"hallucination\" where summaries include unverified information. This can lead to misdiagnosis and treatment decisions, so developing effective hallucination detection methods is crucial for patient safety. We created two novel datasets: Leave-N-Out (LNO) and Natural Hallucination (NH), which omit facts in different ways to test existing methods. Our findings show that LLM-based methods using fact alignment outperform traditional metrics in detecting hallucinations, including high-severity ones, and exhibit strong generalization across synthetic and real-world data."
            ],
            [
                "2 Related Work",
                "Traditional lexical and semantic overlap metrics such as ROUGE [ 4 ] , BLEU [ 5 ] , and BERT-Score [ 6 ] offer straightforward implementation but struggle with semantic understanding. More sophisticated approaches include entailment-based metrics (e.g., FactCC [ 7 ] , SummaC [ 8 ] , AlignScore [ 9 ] ), question-answering based metrics (e.g., FEQA [ 10 ] , QuestEval [ 11 ] , Q 2 [ 12 ] ), and information theory-based metrics (e.g., InfoLM [ 13 ] ). Recent developments include ensemble approaches like FENICE [ 14 ] and LLM-based metrics such as DocLens [ 15 ] . While DocLens represents an innovative approach to medical text evaluation, its methodology, which relies on claim generation and citation matching, doesn’t directly address the need for hallucination detection in clinical summaries. Furthermore, its partial dependence on reference summaries conflicts with our goal of developing reference-free evaluation methods.",
                "ROUGE, BLEU, BERT-Score, FactCC, SummaC, AlignScore, FEQA, QuestEval, Q2, InfoLM, FENICE, DocLens."
            ],
            [
                "3 Hallucination Datasets",
                "We focus our research on the ACI-Bench dataset [ 16 ] , which comprises clinician-patient conversation transcripts and their corresponding clinical summaries (SOAP Notes [ 17 ] ). A SOAP note is a structured way for healthcare providers to document patient information, including Subjective (patient reports), Objective (observable data), Assessment (provider’s evaluation), and Plan (treatment strategy). The transcripts are generally longer and use layman’s terms, while the summaries are written using medical terminology.",
                "ACI-Bench dataset is used in research, comprising clinician-patient conversation transcripts and SOAP Notes. The transcripts are longer and in layman's terms, while summaries use medical terminology."
            ],
            [
                "5 Conclusion",
                "We presented a novel fact-controlled approach to generate domain specific evaluation benchmarks using fact removal to rewrite the input in the LNO dataset. This approach makes it tractable to generate domain specific evaluation data when natural hallucinations are difficult to find in large numbers. While this dataset was helpful in developing methods, high performance on this task didn’t always translate into high performance on the real-world natural hallucination dataset. Existing baseline methods show limited effectiveness on detecting clinical hallucinations, highlighting the need for specialized approaches. We also found that existing methods didn’t generate an interpretable score as much as a count of hallucinations could and incorporated that into our approaches. We demonstrate fact based approaches offer clear explainability into what the hallucinations are as shown in Figure 2 . Our proposed LLM-based evaluation methods, particularly fact alignment, generalize well across datasets and outperform existing metrics, achieving correlations up to 0.43 on synthetic data and 0.37 on real-world data. This suggests that decomposing evaluation into atomic fact verification provides a more robust framework for hallucination detection.",
                "We presented a novel approach to generate domain-specific evaluation benchmarks using fact removal, enabling creation of evaluation data when natural hallucinations are scarce. Existing methods show limited effectiveness on detecting clinical hallucinations, highlighting the need for specialized approaches. Our proposed LLM-based evaluation methods outperform existing metrics, achieving correlations up to 0.43 (synthetic) and 0.37 (real-world)."
            ],
            [
                "6 Limitations",
                "The study primarily uses the ACI-Bench dataset, which may not represent the full complexity of clinical language. Additionally, our research focuses solely on SOAP notes, excluding other clinical documents like discharge summaries or progress reports. Fact based analysis is limited by the definition of what is a fact. While we tried to prompt models to extract atomic facts this often didn’t work leading to undercounting of hallucinations. This study also utilizes LLM-based evaluation methods which are computationally expensive, potentially limiting their practical application which may limit the utility.",
                "The study uses ACI-Bench and focuses on SOAP notes, excluding other documents. Fact analysis is limited due to definitions and model performance issues. LLM-based evaluation methods are computationally expensive."
            ]
        ],
        "general_summary": "**Research Overview**\n\nThis research addresses the critical issue of hallucinations in large language models (LLMs) during summarization of patient-clinician dialogues, which can lead to misdiagnosis and treatment decisions. The study aims to develop effective hallucination detection methods for clinical text summarization.\n\n**Methodology & Approach**\n\nThe researchers created two novel datasets: Leave-N-Out (LNO) and Natural Hallucination (NH), using fact removal to induce hallucinations in summaries. They evaluated existing methods, including general-domain detectors, and proposed LLM-based approaches that count hallucinations. The study also developed a suite of specialized metrics supported by expert-annotated datasets.\n\n**Key Contributions & Findings**\n\nThe key findings include: 1) Existing methods struggle to detect clinical hallucinations; 2) LLM-based methods using fact alignment outperform traditional metrics in detecting hallucinations, including high-severity ones; and 3) The proposed LLM-based evaluation methods exhibit strong generalization across synthetic and real-world data.\n\n**Technical Details**\n\nThe researchers used a range of existing metrics, including ROUGE, BLEU, BERT-Score, FactCC, and SummaC, to evaluate hallucination detection. They also employed an LLM-based approach that uses fact alignment to count hallucinations, offering explainability not available with existing methods.\n\n**Limitations & Future Work**\n\nThe study has some limitations, including the use of a single dataset (ACI-Bench) and the exclusion of other documents. Additionally, fact analysis is limited due to definitions and model performance issues. The LLM-based evaluation methods are also computationally expensive.\n\n**Practical Implications**\n\nThis research contributes to the development of faithful clinical summarization systems by providing specialized metrics and expert-annotated datasets. The proposed methods have the potential to improve patient safety by reducing misdiagnosis and treatment decisions due to hallucinations in LLMs."
    },
    {
        "id": "2504.17550v1",
        "title": "HalluLens: LLM Hallucination Benchmark",
        "authors": [
            "Yejin Bang",
            "Ziwei Ji",
            "Alan Schelten",
            "Anthony Hartshorn",
            "Tara Fowler",
            "Cheng Zhang",
            "Nicola Cancedda",
            "Pascale Fung"
        ],
        "url": "https://arxiv.org/abs/2504.17550v1",
        "Abstract": "Abstract Large language models (LLMs) often generate responses that deviate from user input or training data, a phenomenon known as “hallucination.” These hallucinations undermine user trust and hinder the adoption of generative AI systems. Addressing hallucinations is essential for the advancement of LLMs. This paper introduces a comprehensive hallucination benchmark, incorporating both new extrinsic and existing intrinsic evaluation tasks, built upon clear taxonomy of hallucination. A major challenge in benchmarking hallucinations is the lack of a unified framework due to inconsistent definitions and categorizations. We disentangle LLM hallucination from “factuality,” proposing a clear taxonomy that distinguishes between extrinsic and intrinsic hallucinations, to promote consistency and facilitate research. Extrinsic hallucinations, where the generated content is not consistent with the training data, are increasingly important as LLMs evolve. Our benchmark includes dynamic test set generation to mitigate data leakage and ensure robustness against such leakage. We also analyze existing benchmarks, highlighting their limitations and saturation. The work aims to: (1) establish a clear taxonomy of hallucinations, (2) introduce new extrinsic hallucination tasks, with data that can be dynamically regenerated to prevent saturation by leakage, (3) provide a comprehensive analysis of existing benchmarks, distinguishing them from factuality evaluations.",
        "Main": {
            "1 Introduction": "Large language models (LLMs) are known to generate responses that could be inconsistent with the user input, their own previous outputs, or existing knowledge – a phenomenon commonly referred to as “hallucination”. Such hallucinations pose significant challenges to user trust and acceptance of generative AI systems, particularly when they affect downstream decision making. Therefore, identifying and mitigating hallucinations is important for the broader adoption and further development of LLMs. We believe that a comprehensive, reliable, and ungameable evaluation is a first step toward effective mitigation. One of the key challenges in benchmarking hallucinations in LLMs is the lack of consensus on the definitions of various types and sources of hallucinations, leading to the absence of a unified framework for comprehensive evaluation. While several benchmarks exist for evaluating LLM hallucination (Li et al., 2024 ; Ming et al., 2024 ; Ji et al., 2024 ; Sun et al., 2024 ) , they often do not specify the types of hallucination being considered, or the categories are inconsistent with one another. This results in inconsistent coverage and a gap in research insights. Moreover, as LLMs advance, the LLM hallucination is often conflated with “factuality” (Wei et al., 2024a ; Lin et al., 2022 ; Mallen et al., 2023b ) . Although “hallucination” and “factuality” overlap, they are distinct challenges necessitating separate benchmarks and solutions Wang et al. ( 2023 ); Augenstein et al. ( 2024 ) . In particular, factuality requires an oracle external to the model or even the AI system to define a ground truth. Hallucination is defined as a model behavior in which its output is found to be inconsistent with either its training corpus or the input context. Whereas the oracle for factuality can be difficult to define and even controversial at times, an oracle for hallucination can be defined internally with respect to the model. Our first objective is to delineate and clarify different types of hallucinations by disentangling “hallucination” from “factuality” and providing a taxonomy that promotes consistency and facilitates further research. We consider there to be two main types of textual hallucinations, namely “intrinsic” and “extrinsic” hallucinations Ji et al. ( 2023 ) . Intrinsic hallucinations are generated texts that contradict the source query. This can happen during machine translation or text summarization, for example, where the generated text contains statements that either contradict or do not exist in the source query. Such hallucinations are generally easily verifiable with respect to the source query. In addition, LLMs are also capable of generating content without a direct input context (Bang et al., 2023 ; Huang et al., 2023 ; Zhang et al., 2023 ; Wang et al., 2023 , 2024 ) , but instead relying on their internal knowledge. For instance, LLM can generate free-form text based on the user task instruction, which does not necessarily include the context input. Most of today’s generation tasks are based on task instructions only. In such cases, hallucinated content is not easily verifiable as the oracle “truth” could be anywhere in the training data. This is known as “extrinsic hallucination,” and no existing benchmark adequately measures it. In this work, we introduce new evaluation tasks specifically designed for extrinsic hallucination. In addition, data leakage is a common challenge to designing effective benchmarks (Deng et al., 2024 ) . This problem is especially acute for hallucination benchmarks due to the rapid evolution of LLM development and the intensive annotation efforts ensued. Static test sets are especially vulnerable to obsolescence as new training datasets continuously update and consequently expand to incorporate such test sets. To address this, our benchmark also adopts a dynamic approach to test set generation, reducing the risk of leakage and ensuring robustness over time, while ensuring reliable evaluation of hallucination. Additionally, to address the gaps in insights from existing benchmarks, we analyze major benchmarks for hallucination and factuality, including TruthfulQA (Lin et al., 2022 ) , SimpleQA (Wang et al., 2024 ) , and HaluEval2.0 (Li et al., 2024 ) . We identify the specific challenges these benchmarks evaluate, ensuring their insights are appropriately applied for LLM development. In particular, our analysis of TruthfulQA reveals several issues: it is now saturated due to inclusion in training data, contains incorrect gold answers, and its metrics excessively penalize models. These findings underscore the need to revisit existing benchmarks. The goal of this work is therefore threefold: (1) to establish a clear taxonomy of hallucinations in LLM (§ 2 ); (2) to introduce new extrinsic hallucination evaluation tasks, with data that can be dynamically regenerated to prevent saturation by leakage (§ 3 ); (3) to provide a comprehensive analysis of existing benchmarks, distinguishing between hallucination and factuality evaluations (§ 4 - 5 ).",
            "2 Overview of LLM Hallucination": "The hallucinations of LLM have significant implications for the model’s performance, reliability, and trustworthiness. We first provide a clear taxonomy of hallucinations by explaining types of hallucination, comparing with categorizations suggested by existing LLM hallucination surveys and disentangling hallucinations from the problem of low LLM factuality. Then, we also explain potential sources of hallucination. Finally, we introduce our criteria for hallucination evaluation benchmark design.",
            "3 HalluLens (a): Extrinsic Hallucination Evaluation": "We introduce a suite of tasks for extrinsic hallucination, focused on (in-)consistency with training data. This benchmark comprises three tasks, divided into two main categories based on distinct sources of hallucination: (1) modeling errors and (2) knowledge gaps due to unseen or limited information. For modeling errors, there are two tasks: evaluating precise short answers ( PreciseWikiQA ) and ensuring consistency in detailed long-form content ( LongWiki ). We utilize Wikipedia to construct test set, assuming it is included in the training data of most advanced LLMs. To address hallucinations caused by unseen data, we assess the model’s behavior when confronted with unanswerable questions beyond its training data ( NonExistentRefusal ). Given the variability in LLM training datasets, we create questions asking for entirely nonexistent information. Ideally, the model should recognize the lack of information and refrain from answering. Two key criteria for the extrinsic hallucination benchmark are: (1) whether the knowledge scope is within the training data, and (2) whether refusal is evaluated in the metric. To reduce the risk of test sets being memorized or leaked, we dynamically generate new test questions during evaluation (i.e., no fixed test set). This ensures that the content is unpredictable and not directly accessible in any pre-existing datasets. This is not a trivial challenge as this dynamicity brings in the tension with reproducibility. Thus, the dynamic set should also coincide with low variance over different versions of the test set. Additionally, we design the tasks to cover a broad range of topics, enhancing the robustness and generalizability of the evaluation process. The three newly introduced tasks provide diverse and comprehensive coverage of various scenarios, allowing for a thorough assessment of the models’ performance under different conditions. By doing so, our benchmark offers a more complete understanding of the strengths and weaknesses of LLMs in terms of extrinsic hallucination. 2 2 2 Note, there can be certain contexts, such as creating fiction, where it is good for the model to be able to invent some unreal facts. This work is focused on understanding how much a model extrinsically hallucinates to better understand how to better prevent hallucination when appropriate for the targeted use case. 1. PreciseWikiQA : to evaluate the level of hallucination of the model on short and fact-seeking queries based on the knowledge from the training data. The questions are bounded to training data, thus, an ideal model should be able to provide accurate answers without refusal. 2. LongWiki : to evaluate the model’s hallucination level on long-form content generation based on the knowledge from training data. 3. NonExistentRefusal : to evaluate the model’s likelihood of generating hallucinated information when prompted with knowledge beyond its training data such as non-existent instances that sound natural. We create nonexistent entity names in various domains such as animal, plant, business, brand that sound natural. It consists two sub-tasks: (i) MixedEntities (ii) GeneratedEntities. We evaluated 13 instruction tuned LLMs with varying sizse and different families: 10 open-source LLMs, including Llama3.1-Instruct (8B, 70B, 405B), Llama-3.3-70B-Instruct, Qwen-2.5-Instruct (7B, 14B), Gemma-2-Instruct (9B, 27B) and Mistral-Instruct (7B, Nemo) and three commercial models, including Claude-3-haiku (2024-02-29), Claude-3-sonnet (2024-03-07), and GPT-4o (2024-08-06), on these tasks.",
            "4 HalluLens (b): Intrinsic Hallucination Evaluation": "Intrinsic hallucination occurs when a language model generates content that is inconsistent with the input context. In modern LLM case, intrinsic hallucination is evaluated against the input context provided by the user. For example, in text summarization tasks, the original content serves as the reference source. As LLMs have become more versatile and agent-like, aligning their outputs with the user’s input context has become important for maintaining faithfulness, which is why intrinsic hallucination is often referred to as “faithfulness hallucination.” When used with domain-specific data as an input context, such as in RAG, the generated content should align with the provided context, leading to the term “input-conflicting hallucination.” Intrinsic hallucination is relatively well-studied compared to extrinsic hallucination for two main reasons: (1) Even before the advancement of LLMs, language models demonstrated promising performance in NLG tasks that require a source input, and (2) the verification source is clear and well-defined. However, intrinsic hallucination is still important challenge to be tackled, especially it is likely closely tied with users’ trust for the LLM-based system. In this section, we direct the researchers to the intrinsic hallucination benchmarks that the benchmarks are not saturated and remain relevant. In our analysis of existing benchmarks, we apply the criteria outlined in Section 2.4 . Although we created dynamic test set for extrinsic hallucination to ensure robustness against unintentional data leakage, there are challenges to creating dynamic test sets for intrinsic hallucination, primarily due to evaluation challenges. While extrinsic hallucination can be equipped with automatically generated gold answers, this approach is not feasible for intrinsic hallucination, as using the LLM itself as a judge can introduce intrinsic hallucination. Developing dynamic test sets for intrinsic hallucination is a promising research direction. In the meantime, we rely on well-cited benchmarks specifically targeting intrinsic hallucination to evaluate LLMs’ performance in this area. Specifically, we include three existing benchmark (1) Hughes Hallucination Evaluation Model (HHEM) (Vectara, 2024 ) (2) ANAH 2.0 (Ji et al., 2024 ) – with reference set-up (3) FaithEval (Ming et al., 2024 ) .",
            "5 Revisiting Existing Benchmarks": "In this section, we examine frequently cited benchmarks for factuality and hallucination, such as TruthfulQA, SimpleQA, and HaluEval. These benchmarks are often referenced in both contexts. Our discussion focuses on understanding the insights their evaluation metrics provide regarding hallucination and factuality in LLMs. We also explore how they can be adapted as extrinsic hallucination benchmarks. For a detailed description of their tasks, test sets, and evaluation metrics, please refer to Appendix 10 .",
            "7 Conclusion": "In conclusion, we present a taxonomy of hallucinations by distinguishing them from the factuality of LLMs and categorizing them into extrinsic and intrinsic types. We introduce HalluLens, which includes three newly proposed extrinsic hallucination evaluation tasks alongside three existing intrinsic hallucination tasks. Our proposed tasks specifically aim to evaluate extrinsic hallucinations by assessing model generations in reference to the model’s training data. These tasks cover diverse scenarios and are robust against data leakage by dynamically generating test sets while maintaining the stability of model evaluation. We conclude our paper by revisiting existing benchmarks and emphasizing the need for the distinct extrinsic hallucination benchmark introduced in this study.",
            "Acknowledgement": "We thank Lei Yu for insightful discussions on the topic of LLM hallucination, and Whitney Meers and Austen Gregerson from the GenAI Content Engineering team for their help with data annotation and analysis of the TruthfulQA benchmark. We also appreciate Delong Chen for his comments on the paper. We also thank Carolyn Krol for her close support during the paper review. \\beginappendix Appendix includes: ( 8 ) Overview of HalluLens; ( 9 ) Discussion and implementation details for the extrinsic hallucination tasks, including PreciseWikiQA ( 9.1 ), LongWiki ( 9.2 ), and NonExistentRefusal ( 9.3 ); ( 10 ) Discussion on LLM Factuality Benchmark; ( 11 ) Prompts used for extrinsic hallucination evaluation tasks.",
            "8 Overview of HalluLens": "Table 5 provides an overview of HalluLens, our proposed hallucination benchmark, along with other LLM factuality benchmarks. Our newly proposed tasks for evaluating extrinsic hallucinations ensure high robustness against intentional data leakage through dynamic task creation. Despite the dynamic nature, we ensure strong stability, demonstrated by low variance across different trials, and high sensitivity, which distinguishes intra-model ranking, serving as a reliable benchmark. In addition, all tasks encompass various domains and diverse scenarios, enhancing the applicability in the real world. All test sets are created using open source datasets, ensuring that all evaluation work is reproducible. In terms of difficulty, PreciseWikiQA and LongWiki present a high difficulty level (i.e., challenging for most state-of-the-art models such as Llama-3.1-405B-Instruct and GPT-4o), while NonExistentRefusal presents a medium difficulty level. HalluLens also includes existing intrinsic hallucination tasks. These tasks cover various scenarios where the intrinsic hallucination of LLMs can be evaluated. Constructing dynamically created test sets for intrinsic hallucination evaluation tasks is challenging, as they require gold-standard answers that require human annotation. Furthermore, Table 5 illustrates the LLM factuality benchmarks for reference (gray rows) with more details in Appendix 10 .",
            "9 Discussion and Implementation Details": "We evaluate all models under the same decoding setup, using a temperature of zero and top-p of one, following the established practice of hallucination and factuality benchmarks. For the inference, we used a default chat template format for each model. Each experiment consists of three trials, which showed low variance across runs. The reported number in the main content is average score of three trials. We report the standard deviation for each run in the respective subsections for each task. As a summary, we listed the LLMs used in the pipeline of each tasks in Table 6 .",
            "10 LLM Factuality Benchmark": "LLM Factuality is closely related to the hallucination problem, as explained earlier in Section 2 . To briefly recap, hallucination can contribute to the factuality challenge of LLMs. However, not all hallucinations are factually incorrect. For instance, in the case of intrinsic hallucination, if a user instructs the model to generate content based on an imaginary assumption, such as “Meta is a organic product company,” and then asks what Meta is as a company, the model’s response “it is a tech company” would be factual but hallucinated, as it does not align with the user’s input context. This would be a concern of hallucination, not factuality. Additionally, not all factuality challenges are due to hallucination, especially when dealing with the most up-to-date (time-sensitive) knowledge that is beyond the model’s knowledge cutoff. This is purely due to the model’s lack of access to current knowledge. This specific type of challenge should be addressed with real-time knowledge access techniques, such as Retrieval-Augmented Generation (RAG), if the focus is to enable model to generate factual information. Moreover, one of the biggest distinction for benchmark is that evaluation of LLM factuality often requires the most up-to-date information or regularly updated human annotation. The following works address the “factuality challenge” of LLMs. There have been many efforts to evaluate the factuality challenge of LLMs. There are two main lines of work: (1) factuality level evaluation benchmarks and (2) factuality evaluation methodologies Min et al. ( 2023 ); Song et al. ( 2024 ); Wei et al. ( 2024b ) . The former involves evaluating the factuality ratio of LLM-generated answers while the later is about effort to develop automatic fact-checking evaluation methods. For a more comprehensive survey, we direct readers to the LLM factuality survey Wang et al. ( 2023 ) for additional information. In this section, we provide analysis on the mostly well-known LLM factuality evaluation benchmark that were not discussed in the main content, for the reference: (1) TruthfulQA (Lin et al., 2022 ) , (2) SimpleQA Wei et al. ( 2024a ) (3) FreshQA (Vu et al., 2024 ) (4) HaluEval 2.0 (Li et al., 2024 ) and (5) LongFact (Wei et al., 2024c ) ."
        },
        "Tuples": [
            [
                "1 Introduction",
                "Large language models (LLMs) are known to generate responses that could be inconsistent with the user input, their own previous outputs, or existing knowledge – a phenomenon commonly referred to as “hallucination”. Such hallucinations pose significant challenges to user trust and acceptance of generative AI systems, particularly when they affect downstream decision making. Therefore, identifying and mitigating hallucinations is important for the broader adoption and further development of LLMs. We believe that a comprehensive, reliable, and ungameable evaluation is a first step toward effective mitigation. One of the key challenges in benchmarking hallucinations in LLMs is the lack of consensus on the definitions of various types and sources of hallucinations, leading to the absence of a unified framework for comprehensive evaluation. While several benchmarks exist for evaluating LLM hallucination (Li et al., 2024 ; Ming et al., 2024 ; Ji et al., 2024 ; Sun et al., 2024 ) , they often do not specify the types of hallucination being considered, or the categories are inconsistent with one another. This results in inconsistent coverage and a gap in research insights. Moreover, as LLMs advance, the LLM hallucination is often conflated with “factuality” (Wei et al., 2024a ; Lin et al., 2022 ; Mallen et al., 2023b ) . Although “hallucination” and “factuality” overlap, they are distinct challenges necessitating separate benchmarks and solutions Wang et al. ( 2023 ); Augenstein et al. ( 2024 ) . In particular, factuality requires an oracle external to the model or even the AI system to define a ground truth. Hallucination is defined as a model behavior in which its output is found to be inconsistent with either its training corpus or the input context. Whereas the oracle for factuality can be difficult to define and even controversial at times, an oracle for hallucination can be defined internally with respect to the model. Our first objective is to delineate and clarify different types of hallucinations by disentangling “hallucination” from “factuality” and providing a taxonomy that promotes consistency and facilitates further research. We consider there to be two main types of textual hallucinations, namely “intrinsic” and “extrinsic” hallucinations Ji et al. ( 2023 ) . Intrinsic hallucinations are generated texts that contradict the source query. This can happen during machine translation or text summarization, for example, where the generated text contains statements that either contradict or do not exist in the source query. Such hallucinations are generally easily verifiable with respect to the source query. In addition, LLMs are also capable of generating content without a direct input context (Bang et al., 2023 ; Huang et al., 2023 ; Zhang et al., 2023 ; Wang et al., 2023 , 2024 ) , but instead relying on their internal knowledge. For instance, LLM can generate free-form text based on the user task instruction, which does not necessarily include the context input. Most of today’s generation tasks are based on task instructions only. In such cases, hallucinated content is not easily verifiable as the oracle “truth” could be anywhere in the training data. This is known as “extrinsic hallucination,” and no existing benchmark adequately measures it. In this work, we introduce new evaluation tasks specifically designed for extrinsic hallucination. In addition, data leakage is a common challenge to designing effective benchmarks (Deng et al., 2024 ) . This problem is especially acute for hallucination benchmarks due to the rapid evolution of LLM development and the intensive annotation efforts ensued. Static test sets are especially vulnerable to obsolescence as new training datasets continuously update and consequently expand to incorporate such test sets. To address this, our benchmark also adopts a dynamic approach to test set generation, reducing the risk of leakage and ensuring robustness over time, while ensuring reliable evaluation of hallucination. Additionally, to address the gaps in insights from existing benchmarks, we analyze major benchmarks for hallucination and factuality, including TruthfulQA (Lin et al., 2022 ) , SimpleQA (Wang et al., 2024 ) , and HaluEval2.0 (Li et al., 2024 ) . We identify the specific challenges these benchmarks evaluate, ensuring their insights are appropriately applied for LLM development. In particular, our analysis of TruthfulQA reveals several issues: it is now saturated due to inclusion in training data, contains incorrect gold answers, and its metrics excessively penalize models. These findings underscore the need to revisit existing benchmarks. The goal of this work is therefore threefold: (1) to establish a clear taxonomy of hallucinations in LLM (§ 2 ); (2) to introduce new extrinsic hallucination evaluation tasks, with data that can be dynamically regenerated to prevent saturation by leakage (§ 3 ); (3) to provide a comprehensive analysis of existing benchmarks, distinguishing between hallucination and factuality evaluations (§ 4 - 5 )."
            ],
            [
                "2 Overview of LLM Hallucination",
                "The hallucinations of LLM have significant implications for the model’s performance, reliability, and trustworthiness. We first provide a clear taxonomy of hallucinations by explaining types of hallucination, comparing with categorizations suggested by existing LLM hallucination surveys and disentangling hallucinations from the problem of low LLM factuality. Then, we also explain potential sources of hallucination. Finally, we introduce our criteria for hallucination evaluation benchmark design."
            ],
            [
                "3 HalluLens (a): Extrinsic Hallucination Evaluation",
                "We introduce a suite of tasks for extrinsic hallucination, focused on (in-)consistency with training data. This benchmark comprises three tasks, divided into two main categories based on distinct sources of hallucination: (1) modeling errors and (2) knowledge gaps due to unseen or limited information. For modeling errors, there are two tasks: evaluating precise short answers ( PreciseWikiQA ) and ensuring consistency in detailed long-form content ( LongWiki ). We utilize Wikipedia to construct test set, assuming it is included in the training data of most advanced LLMs. To address hallucinations caused by unseen data, we assess the model’s behavior when confronted with unanswerable questions beyond its training data ( NonExistentRefusal ). Given the variability in LLM training datasets, we create questions asking for entirely nonexistent information. Ideally, the model should recognize the lack of information and refrain from answering. Two key criteria for the extrinsic hallucination benchmark are: (1) whether the knowledge scope is within the training data, and (2) whether refusal is evaluated in the metric. To reduce the risk of test sets being memorized or leaked, we dynamically generate new test questions during evaluation (i.e., no fixed test set). This ensures that the content is unpredictable and not directly accessible in any pre-existing datasets. This is not a trivial challenge as this dynamicity brings in the tension with reproducibility. Thus, the dynamic set should also coincide with low variance over different versions of the test set. Additionally, we design the tasks to cover a broad range of topics, enhancing the robustness and generalizability of the evaluation process. The three newly introduced tasks provide diverse and comprehensive coverage of various scenarios, allowing for a thorough assessment of the models’ performance under different conditions. By doing so, our benchmark offers a more complete understanding of the strengths and weaknesses of LLMs in terms of extrinsic hallucination. 2 2 2 Note, there can be certain contexts, such as creating fiction, where it is good for the model to be able to invent some unreal facts. This work is focused on understanding how much a model extrinsically hallucinates to better understand how to better prevent hallucination when appropriate for the targeted use case. 1. PreciseWikiQA : to evaluate the level of hallucination of the model on short and fact-seeking queries based on the knowledge from the training data. The questions are bounded to training data, thus, an ideal model should be able to provide accurate answers without refusal. 2. LongWiki : to evaluate the model’s hallucination level on long-form content generation based on the knowledge from training data. 3. NonExistentRefusal : to evaluate the model’s likelihood of generating hallucinated information when prompted with knowledge beyond its training data such as non-existent instances that sound natural. We create nonexistent entity names in various domains such as animal, plant, business, brand that sound natural. It consists two sub-tasks: (i) MixedEntities (ii) GeneratedEntities. We evaluated 13 instruction tuned LLMs with varying sizse and different families: 10 open-source LLMs, including Llama3.1-Instruct (8B, 70B, 405B), Llama-3.3-70B-Instruct, Qwen-2.5-Instruct (7B, 14B), Gemma-2-Instruct (9B, 27B) and Mistral-Instruct (7B, Nemo) and three commercial models, including Claude-3-haiku (2024-02-29), Claude-3-sonnet (2024-03-07), and GPT-4o (2024-08-06), on these tasks."
            ],
            [
                "4 HalluLens (b): Intrinsic Hallucination Evaluation",
                "Intrinsic hallucination occurs when a language model generates content that is inconsistent with the input context. In modern LLM case, intrinsic hallucination is evaluated against the input context provided by the user. For example, in text summarization tasks, the original content serves as the reference source. As LLMs have become more versatile and agent-like, aligning their outputs with the user’s input context has become important for maintaining faithfulness, which is why intrinsic hallucination is often referred to as “faithfulness hallucination.” When used with domain-specific data as an input context, such as in RAG, the generated content should align with the provided context, leading to the term “input-conflicting hallucination.” Intrinsic hallucination is relatively well-studied compared to extrinsic hallucination for two main reasons: (1) Even before the advancement of LLMs, language models demonstrated promising performance in NLG tasks that require a source input, and (2) the verification source is clear and well-defined. However, intrinsic hallucination is still important challenge to be tackled, especially it is likely closely tied with users’ trust for the LLM-based system. In this section, we direct the researchers to the intrinsic hallucination benchmarks that the benchmarks are not saturated and remain relevant. In our analysis of existing benchmarks, we apply the criteria outlined in Section 2.4 . Although we created dynamic test set for extrinsic hallucination to ensure robustness against unintentional data leakage, there are challenges to creating dynamic test sets for intrinsic hallucination, primarily due to evaluation challenges. While extrinsic hallucination can be equipped with automatically generated gold answers, this approach is not feasible for intrinsic hallucination, as using the LLM itself as a judge can introduce intrinsic hallucination. Developing dynamic test sets for intrinsic hallucination is a promising research direction. In the meantime, we rely on well-cited benchmarks specifically targeting intrinsic hallucination to evaluate LLMs’ performance in this area. Specifically, we include three existing benchmark (1) Hughes Hallucination Evaluation Model (HHEM) (Vectara, 2024 ) (2) ANAH 2.0 (Ji et al., 2024 ) – with reference set-up (3) FaithEval (Ming et al., 2024 ) ."
            ],
            [
                "5 Revisiting Existing Benchmarks",
                "In this section, we examine frequently cited benchmarks for factuality and hallucination, such as TruthfulQA, SimpleQA, and HaluEval. These benchmarks are often referenced in both contexts. Our discussion focuses on understanding the insights their evaluation metrics provide regarding hallucination and factuality in LLMs. We also explore how they can be adapted as extrinsic hallucination benchmarks. For a detailed description of their tasks, test sets, and evaluation metrics, please refer to Appendix 10 ."
            ],
            [
                "7 Conclusion",
                "In conclusion, we present a taxonomy of hallucinations by distinguishing them from the factuality of LLMs and categorizing them into extrinsic and intrinsic types. We introduce HalluLens, which includes three newly proposed extrinsic hallucination evaluation tasks alongside three existing intrinsic hallucination tasks. Our proposed tasks specifically aim to evaluate extrinsic hallucinations by assessing model generations in reference to the model’s training data. These tasks cover diverse scenarios and are robust against data leakage by dynamically generating test sets while maintaining the stability of model evaluation. We conclude our paper by revisiting existing benchmarks and emphasizing the need for the distinct extrinsic hallucination benchmark introduced in this study."
            ],
            [
                "Acknowledgement",
                "We thank Lei Yu for insightful discussions on the topic of LLM hallucination, and Whitney Meers and Austen Gregerson from the GenAI Content Engineering team for their help with data annotation and analysis of the TruthfulQA benchmark. We also appreciate Delong Chen for his comments on the paper. We also thank Carolyn Krol for her close support during the paper review. \\beginappendix Appendix includes: ( 8 ) Overview of HalluLens; ( 9 ) Discussion and implementation details for the extrinsic hallucination tasks, including PreciseWikiQA ( 9.1 ), LongWiki ( 9.2 ), and NonExistentRefusal ( 9.3 ); ( 10 ) Discussion on LLM Factuality Benchmark; ( 11 ) Prompts used for extrinsic hallucination evaluation tasks."
            ],
            [
                "8 Overview of HalluLens",
                "Table 5 provides an overview of HalluLens, our proposed hallucination benchmark, along with other LLM factuality benchmarks. Our newly proposed tasks for evaluating extrinsic hallucinations ensure high robustness against intentional data leakage through dynamic task creation. Despite the dynamic nature, we ensure strong stability, demonstrated by low variance across different trials, and high sensitivity, which distinguishes intra-model ranking, serving as a reliable benchmark. In addition, all tasks encompass various domains and diverse scenarios, enhancing the applicability in the real world. All test sets are created using open source datasets, ensuring that all evaluation work is reproducible. In terms of difficulty, PreciseWikiQA and LongWiki present a high difficulty level (i.e., challenging for most state-of-the-art models such as Llama-3.1-405B-Instruct and GPT-4o), while NonExistentRefusal presents a medium difficulty level. HalluLens also includes existing intrinsic hallucination tasks. These tasks cover various scenarios where the intrinsic hallucination of LLMs can be evaluated. Constructing dynamically created test sets for intrinsic hallucination evaluation tasks is challenging, as they require gold-standard answers that require human annotation. Furthermore, Table 5 illustrates the LLM factuality benchmarks for reference (gray rows) with more details in Appendix 10 ."
            ],
            [
                "9 Discussion and Implementation Details",
                "We evaluate all models under the same decoding setup, using a temperature of zero and top-p of one, following the established practice of hallucination and factuality benchmarks. For the inference, we used a default chat template format for each model. Each experiment consists of three trials, which showed low variance across runs. The reported number in the main content is average score of three trials. We report the standard deviation for each run in the respective subsections for each task. As a summary, we listed the LLMs used in the pipeline of each tasks in Table 6 ."
            ],
            [
                "10 LLM Factuality Benchmark",
                "LLM Factuality is closely related to the hallucination problem, as explained earlier in Section 2 . To briefly recap, hallucination can contribute to the factuality challenge of LLMs. However, not all hallucinations are factually incorrect. For instance, in the case of intrinsic hallucination, if a user instructs the model to generate content based on an imaginary assumption, such as “Meta is a organic product company,” and then asks what Meta is as a company, the model’s response “it is a tech company” would be factual but hallucinated, as it does not align with the user’s input context. This would be a concern of hallucination, not factuality. Additionally, not all factuality challenges are due to hallucination, especially when dealing with the most up-to-date (time-sensitive) knowledge that is beyond the model’s knowledge cutoff. This is purely due to the model’s lack of access to current knowledge. This specific type of challenge should be addressed with real-time knowledge access techniques, such as Retrieval-Augmented Generation (RAG), if the focus is to enable model to generate factual information. Moreover, one of the biggest distinction for benchmark is that evaluation of LLM factuality often requires the most up-to-date information or regularly updated human annotation. The following works address the “factuality challenge” of LLMs. There have been many efforts to evaluate the factuality challenge of LLMs. There are two main lines of work: (1) factuality level evaluation benchmarks and (2) factuality evaluation methodologies Min et al. ( 2023 ); Song et al. ( 2024 ); Wei et al. ( 2024b ) . The former involves evaluating the factuality ratio of LLM-generated answers while the later is about effort to develop automatic fact-checking evaluation methods. For a more comprehensive survey, we direct readers to the LLM factuality survey Wang et al. ( 2023 ) for additional information. In this section, we provide analysis on the mostly well-known LLM factuality evaluation benchmark that were not discussed in the main content, for the reference: (1) TruthfulQA (Lin et al., 2022 ) , (2) SimpleQA Wei et al. ( 2024a ) (3) FreshQA (Vu et al., 2024 ) (4) HaluEval 2.0 (Li et al., 2024 ) and (5) LongFact (Wei et al., 2024c ) ."
            ]
        ],
        "section_summaries": [
            [
                "1 Introduction",
                "Large language models (LLMs) are known to generate responses that could be inconsistent with the user input, their own previous outputs, or existing knowledge – a phenomenon commonly referred to as “hallucination”. Such hallucinations pose significant challenges to user trust and acceptance of generative AI systems, particularly when they affect downstream decision making. Therefore, identifying and mitigating hallucinations is important for the broader adoption and further development of LLMs. We believe that a comprehensive, reliable, and ungameable evaluation is a first step toward effective mitigation. One of the key challenges in benchmarking hallucinations in LLMs is the lack of consensus on the definitions of various types and sources of hallucinations, leading to the absence of a unified framework for comprehensive evaluation. While several benchmarks exist for evaluating LLM hallucination (Li et al., 2024 ; Ming et al., 2024 ; Ji et al., 2024 ; Sun et al., 2024 ) , they often do not specify the types of hallucination being considered, or the categories are inconsistent with one another. This results in inconsistent coverage and a gap in research insights. Moreover, as LLMs advance, the LLM hallucination is often conflated with “factuality” (Wei et al., 2024a ; Lin et al., 2022 ; Mallen et al., 2023b ) . Although “hallucination” and “factuality” overlap, they are distinct challenges necessitating separate benchmarks and solutions Wang et al. ( 2023 ); Augenstein et al. ( 2024 ) . In particular, factuality requires an oracle external to the model or even the AI system to define a ground truth. Hallucination is defined as a model behavior in which its output is found to be inconsistent with either its training corpus or the input context. Whereas the oracle for factuality can be difficult to define and even controversial at times, an oracle for hallucination can be defined internally with respect to the model. Our first objective is to delineate and clarify different types of hallucinations by disentangling “hallucination” from “factuality” and providing a taxonomy that promotes consistency and facilitates further research. We consider there to be two main types of textual hallucinations, namely “intrinsic” and “extrinsic” hallucinations Ji et al. ( 2023 ) . Intrinsic hallucinations are generated texts that contradict the source query. This can happen during machine translation or text summarization, for example, where the generated text contains statements that either contradict or do not exist in the source query. Such hallucinations are generally easily verifiable with respect to the source query. In addition, LLMs are also capable of generating content without a direct input context (Bang et al., 2023 ; Huang et al., 2023 ; Zhang et al., 2023 ; Wang et al., 2023 , 2024 ) , but instead relying on their internal knowledge. For instance, LLM can generate free-form text based on the user task instruction, which does not necessarily include the context input. Most of today’s generation tasks are based on task instructions only. In such cases, hallucinated content is not easily verifiable as the oracle “truth” could be anywhere in the training data. This is known as “extrinsic hallucination,” and no existing benchmark adequately measures it. In this work, we introduce new evaluation tasks specifically designed for extrinsic hallucination. In addition, data leakage is a common challenge to designing effective benchmarks (Deng et al., 2024 ) . This problem is especially acute for hallucination benchmarks due to the rapid evolution of LLM development and the intensive annotation efforts ensued. Static test sets are especially vulnerable to obsolescence as new training datasets continuously update and consequently expand to incorporate such test sets. To address this, our benchmark also adopts a dynamic approach to test set generation, reducing the risk of leakage and ensuring robustness over time, while ensuring reliable evaluation of hallucination. Additionally, to address the gaps in insights from existing benchmarks, we analyze major benchmarks for hallucination and factuality, including TruthfulQA (Lin et al., 2022 ) , SimpleQA (Wang et al., 2024 ) , and HaluEval2.0 (Li et al., 2024 ) . We identify the specific challenges these benchmarks evaluate, ensuring their insights are appropriately applied for LLM development. In particular, our analysis of TruthfulQA reveals several issues: it is now saturated due to inclusion in training data, contains incorrect gold answers, and its metrics excessively penalize models. These findings underscore the need to revisit existing benchmarks. The goal of this work is therefore threefold: (1) to establish a clear taxonomy of hallucinations in LLM (§ 2 ); (2) to introduce new extrinsic hallucination evaluation tasks, with data that can be dynamically regenerated to prevent saturation by leakage (§ 3 ); (3) to provide a comprehensive analysis of existing benchmarks, distinguishing between hallucination and factuality evaluations (§ 4 - 5 ).",
                "LLMs generate responses that could be inconsistent with user input or previous outputs (\"hallucinations\"), posing challenges for user trust and adoption. Current benchmarks lack consensus on hallucination definitions, leading to inconsistent coverage and gaps in research insights. The LLM hallucination is often conflated with \"factuality\", but they are distinct challenges requiring separate benchmarks and solutions. Two main types of textual hallucinations exist: intrinsic (contradicting source queries) and extrinsic (generated content without direct input context). Existing benchmarks, such as TruthfulQA, SimpleQA, and HaluEval2.0, have issues like saturation, incorrect gold answers, and excessive penalty metrics. This work aims to establish a clear taxonomy of hallucinations, introduce new evaluation tasks for extrinsic hallucination, and provide a comprehensive analysis of existing benchmarks."
            ],
            [
                "2 Overview of LLM Hallucination",
                "The hallucinations of LLM have significant implications for the model’s performance, reliability, and trustworthiness. We first provide a clear taxonomy of hallucinations by explaining types of hallucination, comparing with categorizations suggested by existing LLM hallucination surveys and disentangling hallucinations from the problem of low LLM factuality. Then, we also explain potential sources of hallucination. Finally, we introduce our criteria for hallucination evaluation benchmark design.",
                "Hallucinations in LLM have implications for model performance and trustworthiness. We categorize hallucinations, compare with existing surveys, and disentangle them from low factuality. Potential sources of hallucination are also explained."
            ],
            [
                "3 HalluLens (a): Extrinsic Hallucination Evaluation",
                "We introduce a suite of tasks for extrinsic hallucination, focused on (in-)consistency with training data. This benchmark comprises three tasks, divided into two main categories based on distinct sources of hallucination: (1) modeling errors and (2) knowledge gaps due to unseen or limited information. For modeling errors, there are two tasks: evaluating precise short answers ( PreciseWikiQA ) and ensuring consistency in detailed long-form content ( LongWiki ). We utilize Wikipedia to construct test set, assuming it is included in the training data of most advanced LLMs. To address hallucinations caused by unseen data, we assess the model’s behavior when confronted with unanswerable questions beyond its training data ( NonExistentRefusal ). Given the variability in LLM training datasets, we create questions asking for entirely nonexistent information. Ideally, the model should recognize the lack of information and refrain from answering. Two key criteria for the extrinsic hallucination benchmark are: (1) whether the knowledge scope is within the training data, and (2) whether refusal is evaluated in the metric. To reduce the risk of test sets being memorized or leaked, we dynamically generate new test questions during evaluation (i.e., no fixed test set). This ensures that the content is unpredictable and not directly accessible in any pre-existing datasets. This is not a trivial challenge as this dynamicity brings in the tension with reproducibility. Thus, the dynamic set should also coincide with low variance over different versions of the test set. Additionally, we design the tasks to cover a broad range of topics, enhancing the robustness and generalizability of the evaluation process. The three newly introduced tasks provide diverse and comprehensive coverage of various scenarios, allowing for a thorough assessment of the models’ performance under different conditions. By doing so, our benchmark offers a more complete understanding of the strengths and weaknesses of LLMs in terms of extrinsic hallucination. 2 2 2 Note, there can be certain contexts, such as creating fiction, where it is good for the model to be able to invent some unreal facts. This work is focused on understanding how much a model extrinsically hallucinates to better understand how to better prevent hallucination when appropriate for the targeted use case. 1. PreciseWikiQA : to evaluate the level of hallucination of the model on short and fact-seeking queries based on the knowledge from the training data. The questions are bounded to training data, thus, an ideal model should be able to provide accurate answers without refusal. 2. LongWiki : to evaluate the model’s hallucination level on long-form content generation based on the knowledge from training data. 3. NonExistentRefusal : to evaluate the model’s likelihood of generating hallucinated information when prompted with knowledge beyond its training data such as non-existent instances that sound natural. We create nonexistent entity names in various domains such as animal, plant, business, brand that sound natural. It consists two sub-tasks: (i) MixedEntities (ii) GeneratedEntities. We evaluated 13 instruction tuned LLMs with varying sizse and different families: 10 open-source LLMs, including Llama3.1-Instruct (8B, 70B, 405B), Llama-3.3-70B-Instruct, Qwen-2.5-Instruct (7B, 14B), Gemma-2-Instruct (9B, 27B) and Mistral-Instruct (7B, Nemo) and three commercial models, including Claude-3-haiku (2024-02-29), Claude-3-sonnet (2024-03-07), and GPT-4o (2024-08-06), on these tasks.",
                "We introduce a suite of three extrinsic hallucination tasks, divided into two categories: (1) modeling errors and (2) knowledge gaps. Three new tasks are introduced: PreciseWikiQA for short answers, LongWiki for long-form content generation, and NonExistentRefusal to evaluate hallucinations beyond training data. The benchmark assesses models' behavior when confronted with unanswerable questions or nonexistent information. 13 instruction-tuned LLMs were evaluated on these tasks, including commercial models Claude-3-haiku, Claude-3-sonnet, and GPT-4o, as well as open-source models Llama3.1-Instruct and Qwen-2.5-Instruct. The benchmark evaluates a model's ability to recognize the lack of information and refrain from answering when faced with nonexistent or unanswerable questions. Two key criteria for the extrinsic hallucination benchmark are whether the knowledge scope is within the training data, and whether refusal is evaluated in the metric."
            ],
            [
                "4 HalluLens (b): Intrinsic Hallucination Evaluation",
                "Intrinsic hallucination occurs when a language model generates content that is inconsistent with the input context. In modern LLM case, intrinsic hallucination is evaluated against the input context provided by the user. For example, in text summarization tasks, the original content serves as the reference source. As LLMs have become more versatile and agent-like, aligning their outputs with the user’s input context has become important for maintaining faithfulness, which is why intrinsic hallucination is often referred to as “faithfulness hallucination.” When used with domain-specific data as an input context, such as in RAG, the generated content should align with the provided context, leading to the term “input-conflicting hallucination.” Intrinsic hallucination is relatively well-studied compared to extrinsic hallucination for two main reasons: (1) Even before the advancement of LLMs, language models demonstrated promising performance in NLG tasks that require a source input, and (2) the verification source is clear and well-defined. However, intrinsic hallucination is still important challenge to be tackled, especially it is likely closely tied with users’ trust for the LLM-based system. In this section, we direct the researchers to the intrinsic hallucination benchmarks that the benchmarks are not saturated and remain relevant. In our analysis of existing benchmarks, we apply the criteria outlined in Section 2.4 . Although we created dynamic test set for extrinsic hallucination to ensure robustness against unintentional data leakage, there are challenges to creating dynamic test sets for intrinsic hallucination, primarily due to evaluation challenges. While extrinsic hallucination can be equipped with automatically generated gold answers, this approach is not feasible for intrinsic hallucination, as using the LLM itself as a judge can introduce intrinsic hallucination. Developing dynamic test sets for intrinsic hallucination is a promising research direction. In the meantime, we rely on well-cited benchmarks specifically targeting intrinsic hallucination to evaluate LLMs’ performance in this area. Specifically, we include three existing benchmark (1) Hughes Hallucination Evaluation Model (HHEM) (Vectara, 2024 ) (2) ANAH 2.0 (Ji et al., 2024 ) – with reference set-up (3) FaithEval (Ming et al., 2024 ) .",
                "Intrinsic hallucination occurs when a language model generates inconsistent content with input context, and is evaluated against user-provided context. It's relatively well-studied due to promising performance in NLG tasks and clear verification sources. However, it remains an important challenge, especially regarding users' trust for LLM-based systems.\n\nNumbers:\n- 2\n- 2024 (x3)\n\nNames/Acronyms:\n- HHEM (Hughes Hallucination Evaluation Model)\n- ANAH 2.0 \n- FaithEval \n- RAG\n- LLM (Language Models)\n- NLG (Natural Language Generation)"
            ],
            [
                "5 Revisiting Existing Benchmarks",
                "In this section, we examine frequently cited benchmarks for factuality and hallucination, such as TruthfulQA, SimpleQA, and HaluEval. These benchmarks are often referenced in both contexts. Our discussion focuses on understanding the insights their evaluation metrics provide regarding hallucination and factuality in LLMs. We also explore how they can be adapted as extrinsic hallucination benchmarks. For a detailed description of their tasks, test sets, and evaluation metrics, please refer to Appendix 10 .",
                "TruthfulQA, SimpleQA, and HaluEval are frequently cited benchmarks for factuality and hallucination in LLMs, providing insights through evaluation metrics that can be adapted for extrinsic hallucination benchmarks."
            ],
            [
                "7 Conclusion",
                "In conclusion, we present a taxonomy of hallucinations by distinguishing them from the factuality of LLMs and categorizing them into extrinsic and intrinsic types. We introduce HalluLens, which includes three newly proposed extrinsic hallucination evaluation tasks alongside three existing intrinsic hallucination tasks. Our proposed tasks specifically aim to evaluate extrinsic hallucinations by assessing model generations in reference to the model’s training data. These tasks cover diverse scenarios and are robust against data leakage by dynamically generating test sets while maintaining the stability of model evaluation. We conclude our paper by revisiting existing benchmarks and emphasizing the need for the distinct extrinsic hallucination benchmark introduced in this study.",
                "We propose a taxonomy of hallucinations, distinguishing between LLM factuality and categorizing them into extrinsic and intrinsic types. We introduce HalluLens with 3 new tasks to evaluate extrinsic hallucinations and 3 existing intrinsic tasks. The tasks assess model generations in relation to training data and are robust against data leakage."
            ],
            [
                "Acknowledgement",
                "We thank Lei Yu for insightful discussions on the topic of LLM hallucination, and Whitney Meers and Austen Gregerson from the GenAI Content Engineering team for their help with data annotation and analysis of the TruthfulQA benchmark. We also appreciate Delong Chen for his comments on the paper. We also thank Carolyn Krol for her close support during the paper review. \\beginappendix Appendix includes: ( 8 ) Overview of HalluLens; ( 9 ) Discussion and implementation details for the extrinsic hallucination tasks, including PreciseWikiQA ( 9.1 ), LongWiki ( 9.2 ), and NonExistentRefusal ( 9.3 ); ( 10 ) Discussion on LLM Factuality Benchmark; ( 11 ) Prompts used for extrinsic hallucination evaluation tasks.",
                "Lei Yu, Whitney Meers, Austen Gregerson, Delong Chen, and Carolyn Krol. \nPreciseWikiQA, LongWiki, NonExistentRefusal, TruthfulQA benchmark. \nHalluLens appendix includes 8-11 sections."
            ],
            [
                "8 Overview of HalluLens",
                "Table 5 provides an overview of HalluLens, our proposed hallucination benchmark, along with other LLM factuality benchmarks. Our newly proposed tasks for evaluating extrinsic hallucinations ensure high robustness against intentional data leakage through dynamic task creation. Despite the dynamic nature, we ensure strong stability, demonstrated by low variance across different trials, and high sensitivity, which distinguishes intra-model ranking, serving as a reliable benchmark. In addition, all tasks encompass various domains and diverse scenarios, enhancing the applicability in the real world. All test sets are created using open source datasets, ensuring that all evaluation work is reproducible. In terms of difficulty, PreciseWikiQA and LongWiki present a high difficulty level (i.e., challenging for most state-of-the-art models such as Llama-3.1-405B-Instruct and GPT-4o), while NonExistentRefusal presents a medium difficulty level. HalluLens also includes existing intrinsic hallucination tasks. These tasks cover various scenarios where the intrinsic hallucination of LLMs can be evaluated. Constructing dynamically created test sets for intrinsic hallucination evaluation tasks is challenging, as they require gold-standard answers that require human annotation. Furthermore, Table 5 illustrates the LLM factuality benchmarks for reference (gray rows) with more details in Appendix 10 .",
                "Table 5 provides an overview of HalluLens, a hallucination benchmark, and other LLM factuality benchmarks. Tasks include PreciseWikiQA, LongWiki, NonExistentRefusal, and intrinsic hallucination tasks with varying difficulty levels (high for PreciseWikiQA and LongWiki, medium for NonExistentRefusal). Test sets are created using open source datasets and encompass various domains and scenarios to ensure applicability in the real world."
            ],
            [
                "9 Discussion and Implementation Details",
                "We evaluate all models under the same decoding setup, using a temperature of zero and top-p of one, following the established practice of hallucination and factuality benchmarks. For the inference, we used a default chat template format for each model. Each experiment consists of three trials, which showed low variance across runs. The reported number in the main content is average score of three trials. We report the standard deviation for each run in the respective subsections for each task. As a summary, we listed the LLMs used in the pipeline of each tasks in Table 6 .",
                "Temperature: zero, top-p: one, \nDefault chat template format,\nThree trials with low variance,\nAverage score reported, \nStandard deviation reported for each run."
            ],
            [
                "10 LLM Factuality Benchmark",
                "LLM Factuality is closely related to the hallucination problem, as explained earlier in Section 2 . To briefly recap, hallucination can contribute to the factuality challenge of LLMs. However, not all hallucinations are factually incorrect. For instance, in the case of intrinsic hallucination, if a user instructs the model to generate content based on an imaginary assumption, such as “Meta is a organic product company,” and then asks what Meta is as a company, the model’s response “it is a tech company” would be factual but hallucinated, as it does not align with the user’s input context. This would be a concern of hallucination, not factuality. Additionally, not all factuality challenges are due to hallucination, especially when dealing with the most up-to-date (time-sensitive) knowledge that is beyond the model’s knowledge cutoff. This is purely due to the model’s lack of access to current knowledge. This specific type of challenge should be addressed with real-time knowledge access techniques, such as Retrieval-Augmented Generation (RAG), if the focus is to enable model to generate factual information. Moreover, one of the biggest distinction for benchmark is that evaluation of LLM factuality often requires the most up-to-date information or regularly updated human annotation. The following works address the “factuality challenge” of LLMs. There have been many efforts to evaluate the factuality challenge of LLMs. There are two main lines of work: (1) factuality level evaluation benchmarks and (2) factuality evaluation methodologies Min et al. ( 2023 ); Song et al. ( 2024 ); Wei et al. ( 2024b ) . The former involves evaluating the factuality ratio of LLM-generated answers while the later is about effort to develop automatic fact-checking evaluation methods. For a more comprehensive survey, we direct readers to the LLM factuality survey Wang et al. ( 2023 ) for additional information. In this section, we provide analysis on the mostly well-known LLM factuality evaluation benchmark that were not discussed in the main content, for the reference: (1) TruthfulQA (Lin et al., 2022 ) , (2) SimpleQA Wei et al. ( 2024a ) (3) FreshQA (Vu et al., 2024 ) (4) HaluEval 2.0 (Li et al., 2024 ) and (5) LongFact (Wei et al., 2024c ) .",
                "LLM Factuality is related to hallucination, but not all hallucinations are factually incorrect. The model's lack of access to current knowledge can also lead to factuality challenges. Evaluation of LLM factuality often requires up-to-date information and human annotation. There are two main lines of work: (1) factuality level evaluation benchmarks and (2) automatic fact-checking evaluation methods. Examples of popular LLM factuality evaluation benchmarks include TruthfulQA, SimpleQA, FreshQA, HaluEval 2.0, and LongFact."
            ]
        ],
        "general_summary": "**Research Overview**\nThis paper introduces HalluLens, a comprehensive benchmark for evaluating hallucinations in large language models (LLMs), which deviate from user input or training data. The research aims to establish a clear taxonomy of hallucinations, introduce new extrinsic evaluation tasks, and provide a comprehensive analysis of existing benchmarks.\n\n**Methodology & Approach**\nThe proposed solution, HalluLens, includes three new extrinsic hallucination tasks: PreciseWikiQA, LongWiki, and NonExistentRefusal. These tasks assess models' behavior when confronted with unanswerable questions or nonexistent information. The benchmark also incorporates intrinsic evaluation tasks from existing benchmarks. A clear taxonomy is presented to distinguish between extrinsic and intrinsic hallucinations.\n\n**Key Contributions & Findings**\nHalluLens introduces a unified framework for evaluating hallucinations, which was previously fragmented due to inconsistent definitions. 13 instruction-tuned LLMs were evaluated on HalluLens tasks, demonstrating the benchmark's robustness against data leakage. The study highlights limitations in existing benchmarks and suggests directions for future research.\n\n**Technical Details**\nThe three extrinsic tasks are designed to evaluate models' ability to recognize knowledge gaps or lack of information. PreciseWikiQA assesses short answers, LongWiki evaluates long-form content generation, and NonExistentRefusal evaluates hallucinations beyond training data.\n\n**Limitations & Future Work**\nExisting benchmarks have limitations such as saturation, incorrect gold answers, and excessive penalty metrics. The study suggests revisiting these benchmarks to address their shortcomings and encourages further research on evaluating LLMs' factuality.\n\n**Practical Implications**\nHalluLens can be applied in practice by improving the evaluation of LLMs' hallucinations, which is crucial for model trustworthiness and adoption. The benchmark's results can inform developers to improve models' ability to recognize knowledge gaps or lack of information, leading to more reliable AI systems.\n\n**Technical Details (continued)**\nThe benchmark uses a dynamic test set generation method to prevent data leakage and ensure robustness. Two key criteria are used for evaluating extrinsic hallucination: whether the knowledge scope is within the training data, and whether refusal is evaluated in the metric.\n\n**Limitations & Future Work (continued)**\nHalluLens also highlights the importance of factuality evaluation in LLMs, which requires up-to-date information and human annotation. The study suggests exploring automatic fact-checking methods and revisiting existing benchmarks to improve their coverage and accuracy.\n\n**Practical Implications (continued)**\nThe development of HalluLens can contribute to the advancement of LLMs by providing a standardized evaluation framework for hallucinations, which is essential for model trustworthiness and adoption. The benchmark's results can inform developers to improve models' ability to recognize knowledge gaps or lack of information, leading to more reliable AI systems."
    },
    {
        "id": "2501.13824v2",
        "title": "Can Hallucinations Help? Boosting LLMs for Drug Discovery",
        "authors": [
            "Shuzhou Yuan",
            "Zhan Qu",
            "Ashish Yashwanth Kangen",
            "Michael Färber"
        ],
        "url": "https://arxiv.org/abs/2501.13824v2",
        "Abstract": "Abstract Hallucinations in large language models (LLMs), plausible but factually inaccurate text, are often viewed as undesirable. However, recent work suggests that such outputs may hold creative potential. In this paper, we investigate whether hallucinations can improve LLMs on molecule property prediction, a key task in early-stage drug discovery. We prompt LLMs to generate natural language descriptions from molecular SMILES strings and incorporate these often hallucinated descriptions into downstream classification tasks. Evaluating seven instruction-tuned LLMs across five datasets, we find that hallucinations significantly improve predictive accuracy for some models. Notably, Falcon3-Mamba-7B outperforms all baselines when hallucinated text is included, while hallucinations generated by GPT-4o consistently yield the greatest gains between models. We further identify and categorize over 18,000 beneficial hallucinations, with structural misdescriptions emerging as the most impactful type, suggesting that hallucinated statements about molecular structure may increase model confidence. Ablation studies show that larger models benefit more from hallucinations, while temperature has a limited effect. Our findings challenge conventional views of hallucination as purely problematic and suggest new directions for leveraging hallucinations as a useful signal in scientific modeling tasks like drug discovery.",
        "Main": {
            "Introduction": "Large language models (LLMs) have demonstrated impressive capabilities across diverse domains, including everyday applications (Ge et al. 2023 ; Yao et al. 2024 ) , scientific discovery (Zhang et al. 2024b ; Madani et al. 2023 ) , and chemistry (Boiko et al. 2023 ; Zhang et al. 2024a ) . However, LLMs are known to produce hallucinations, text that is plausible but factually incorrect or misaligned with the input (Huang et al. 2023 ; Bang et al. 2023 ; Rawte et al. 2023 ; Yuan and Faerber 2023 ) . While much work has focused on mitigating hallucinations (Ji et al. 2023 ; Dhuliawala et al. 2024 ) , recent perspectives suggest that hallucination may also be beneficial, for example, it may enable creativity (Lee 2023 ; Ye et al. 2023 ; Wang 2024 ) . In domains like drug discovery, creativity is a vital asset. Evaluating molecular properties for potential drug candidates involves reasoning over vast chemical spaces and abstract functional traits (Hughes et al. 2011 ) . LLMs are beginning to assist in this process (Chakraborty, Bhattacharya, and Lee 2023 ; Edwards, Wang, and Ji 2024 ) , and textual molecule descriptions have been shown to help improve generalization (Liu et al. 2023 ) . Yet, many LLM-generated descriptions contain hallucinations, as illustrated in Figure 1 , where models deviate significantly from domain-specific references such as MolT5 (Edwards et al. 2022 ) . This raises an underexplored question: Can hallucinations improve LLMs’ performance in molecule property prediction? While hallucinations are often treated as failure cases, we hypothesize that they may act as implicit counterfactuals, offering speculative, high-level interpretations that help LLMs generalize over unseen compounds. This idea is particularly relevant in early-stage drug discovery, where structured knowledge is sparse and creativity may be an advantage. To investigate this, we conduct a comprehensive evaluation of seven instruction-tuned LLMs across five molecular property prediction tasks from MoleculeNet (Wu et al. 2018 ) . Given a molecule’s SMILES string (Weininger 1988 ) , we generate natural language descriptions using each LLM, then include the description in the prompt alongside a task-specific instruction (Figure 2 ). We compare the resulting performance against three baselines: • SMILES : using only the molecule string, • MolT5 : domain-specific reference descriptions, • PubChem : rule-based text derived from authoritative chemical metadata (Kim et al. 2025 ) . Our results show that LLM-generated hallucinations can enhance performance. For example, Falcon3-Mamba-7B surpasses the PubChem baseline by 8.22% in ROC-AUC. Llama-3.1-8B achieves 15.8% and 11.2% improvements over the SMILES and MolT5 baselines, respectively. Hallucinations from GPT-4o deliver the most consistent gains across models. We further conduct ablation studies and analyze the effects of model size and generation temperature. Larger models benefit more from hallucinations, while temperature affects factuality (hallucination rate) but has limited influence on downstream performance. To understand the nature of these helpful hallucinations, we collect instances that lead to improved predictions and categorize them into four types: structural misdescription , functional hallucination , analogical hallucination , and generic fluff . Surprisingly, the dominant type is structural misdescription , suggesting that factual deviation is not necessarily detrimental, and may help LLMs generalize. Our contributions are as follows: • We conduct the first systematic investigation into whether hallucinations can improve LLMs for molecular property prediction, a core subtask in early-stage drug discovery. • We evaluate seven LLMs across five benchmark datasets, demonstrating that certain models benefit significantly from hallucinated input. • We analyze model size, generation temperature, and hallucination types to understand how and when hallucinations contribute positively to performance.",
            "Hallucination Generation": "Large language models (LLMs), when prompted to describe molecules, often produce fluent yet factually incorrect descriptions, commonly referred to as hallucinations. While typically regarded as undesirable, we explore an alternative view: these hallucinations can be seen as implicit counterfactuals, imaginative variations on molecular structure or function that may enhance model performance in downstream prediction tasks. In this section, we detail how we generate these descriptions and analyze their properties.",
            "Task Formulation": "We define molecule property prediction as a prompt-based decision task for large language models (LLMs). Each instance consists of a molecule represented by a SMILES string [ S M I L E S ] [SMILES] , optionally augmented with a natural language description [ D e s c r i p t i o n ] [Description] , followed by a task-specific instruction [ I n s t r u c t ] [Instruct] . System : You are an expert in chemistry. User : [ S M I L E S ] [SMILES] [ D e s c r i p t i o n ] [Description] [ I n s t r u c t ] [Instruct] The model is then asked to answer a binary yes/no question, such as: “Does this molecule inhibit HIV replication? Only answer ‘Yes’ or ‘No’.” We decode the model’s prediction by selecting the highest-probability token: y ^ = arg ⁡ max y ∈ V ⁡ P ϕ ( y ) , V = { Yes , No } \\hat{y}=\\arg\\max_{y\\in V}P_{\\phi}(y),\\quad V=\\{\\text{Yes},\\text{No}\\} (1) Textual Conditioning as Counterfactual Context. Our central hypothesis is that the presence of [ D e s c r i p t i o n ] [Description] , regardless of factual accuracy, provides useful inductive bias for the model. These descriptions vary across three settings: • None (SMILES-only) : The model receives no textual context beyond the SMILES string. • Factual : A structured, rule-based description from PubChem is included. • LLM-generated : A free-text description is generated as described in the previous section, and may include hallucinations. Rather than treating hallucinated descriptions as noise, we consider them as implicit counterfactuals , alternative narratives about molecular properties that may help the model distinguish subtle patterns. This setup enables us to investigate how speculative or misaligned information interacts with LLM decision-making in molecular tasks.",
            "Experimental Design": "We investigate how molecule-level hallucinations generated by different LLMs affect downstream classification performance. The prompt format is constructed as [ S M I L E S ] [ D e s c r i p t i o n ] [ I n s t r u c t ] [SMILES][Description][Instruct] , where [ D e s c r i p t i o n ] [Description] is varied under four different settings: SMILES : [ D e s c r i p t i o n ] [Description] is set to ϵ \\epsilon (empty), and the model must make predictions based solely on the SMILES string. MolT5 : We use descriptions generated by MolT5 (Edwards et al. 2022 ) , a pretrained molecule-to-text model. This provides a reference point with high-quality factual descriptions aligned to molecular structure. PubChem : We extract rule-based factual information from the PubChem database (Kim et al. 2025 ) , including molecular formula, atom count, synonyms, and molecular weight. This metadata is formatted into a natural language description, e.g., “This compound, known as X, has the molecular formula C 22 H 23 N 7 O 2 …” . While this setting reflects gold-standard chemical knowledge, it is limited to molecules present in PubChem (88% coverage). As such, this baseline is not intended for practical deployment but to assess model behavior under ideal factual conditions. LLMs : To study the effects of hallucinations, we replace [ D e s c r i p t i o n ] [Description] with text generated by different LLMs. Each model is evaluated using (a) its own hallucinated descriptions and (b) hallucinations generated by other LLMs. This setup allows us to analyze if performance is influenced more by the source of the hallucination or the model that interprets it.",
            "Main Results and Analysis": "We conduct experiments to address three primary questions: i) Can hallucinated descriptions improve LLM performance compared to standard baselines? ii) Which LLMs generate the most beneficial hallucinations? iii) What types of hallucinations are most likely to improve prediction?",
            "Ablation Studies": "To better understand when and why hallucinated text improves molecular prediction, we conduct ablation studies focusing on two factors: model size and generation temperature. We use the Llama-3 family due to its consistent architecture and multiple available sizes. As PubChem coverage is incomplete, we report improvements relative to SMILES and MolT5 baselines.",
            "Conclusion": "This paper investigates whether hallucinations can improve the performance of Large Language Models (LLMs) in predicting molecule properties. We evaluate seven LLMs across five datasets by incorporating natural language descriptions of molecules, which often contain hallucinations, into the input prompts. Our results show that hallucinated input can improve classification performance for most LLMs, even outperforming accurate baselines like MolT5 and PubChem in some cases. In particular, Falcon3-Mamba-7B and hallucinations generated by GPT-4o yield the most consistent improvements across datasets. We further categorize the beneficial hallucinations and find that structural misdescription is the most common and impactful type, suggesting that factual deviation does not necessarily harm, and may even help, LLM decision-making. Ablation studies reveal that larger models are better at leveraging hallucinated descriptions, while variation in generation temperature has limited impact on downstream performance. Our findings offer a new perspective on the role of hallucinations in LLM-based scientific tasks, highlighting their potential to act as implicit counterfactuals that encourage flexible reasoning and robust prediction. Future work may further investigate the mechanisms by which LLMs benefit from such hallucinated input and how to leverage this creative potential in a safe and intentional manner, especially in high-stakes domains."
        },
        "Tuples": [
            [
                "Introduction",
                "Large language models (LLMs) have demonstrated impressive capabilities across diverse domains, including everyday applications (Ge et al. 2023 ; Yao et al. 2024 ) , scientific discovery (Zhang et al. 2024b ; Madani et al. 2023 ) , and chemistry (Boiko et al. 2023 ; Zhang et al. 2024a ) . However, LLMs are known to produce hallucinations, text that is plausible but factually incorrect or misaligned with the input (Huang et al. 2023 ; Bang et al. 2023 ; Rawte et al. 2023 ; Yuan and Faerber 2023 ) . While much work has focused on mitigating hallucinations (Ji et al. 2023 ; Dhuliawala et al. 2024 ) , recent perspectives suggest that hallucination may also be beneficial, for example, it may enable creativity (Lee 2023 ; Ye et al. 2023 ; Wang 2024 ) . In domains like drug discovery, creativity is a vital asset. Evaluating molecular properties for potential drug candidates involves reasoning over vast chemical spaces and abstract functional traits (Hughes et al. 2011 ) . LLMs are beginning to assist in this process (Chakraborty, Bhattacharya, and Lee 2023 ; Edwards, Wang, and Ji 2024 ) , and textual molecule descriptions have been shown to help improve generalization (Liu et al. 2023 ) . Yet, many LLM-generated descriptions contain hallucinations, as illustrated in Figure 1 , where models deviate significantly from domain-specific references such as MolT5 (Edwards et al. 2022 ) . This raises an underexplored question: Can hallucinations improve LLMs’ performance in molecule property prediction? While hallucinations are often treated as failure cases, we hypothesize that they may act as implicit counterfactuals, offering speculative, high-level interpretations that help LLMs generalize over unseen compounds. This idea is particularly relevant in early-stage drug discovery, where structured knowledge is sparse and creativity may be an advantage. To investigate this, we conduct a comprehensive evaluation of seven instruction-tuned LLMs across five molecular property prediction tasks from MoleculeNet (Wu et al. 2018 ) . Given a molecule’s SMILES string (Weininger 1988 ) , we generate natural language descriptions using each LLM, then include the description in the prompt alongside a task-specific instruction (Figure 2 ). We compare the resulting performance against three baselines: • SMILES : using only the molecule string, • MolT5 : domain-specific reference descriptions, • PubChem : rule-based text derived from authoritative chemical metadata (Kim et al. 2025 ) . Our results show that LLM-generated hallucinations can enhance performance. For example, Falcon3-Mamba-7B surpasses the PubChem baseline by 8.22% in ROC-AUC. Llama-3.1-8B achieves 15.8% and 11.2% improvements over the SMILES and MolT5 baselines, respectively. Hallucinations from GPT-4o deliver the most consistent gains across models. We further conduct ablation studies and analyze the effects of model size and generation temperature. Larger models benefit more from hallucinations, while temperature affects factuality (hallucination rate) but has limited influence on downstream performance. To understand the nature of these helpful hallucinations, we collect instances that lead to improved predictions and categorize them into four types: structural misdescription , functional hallucination , analogical hallucination , and generic fluff . Surprisingly, the dominant type is structural misdescription , suggesting that factual deviation is not necessarily detrimental, and may help LLMs generalize. Our contributions are as follows: • We conduct the first systematic investigation into whether hallucinations can improve LLMs for molecular property prediction, a core subtask in early-stage drug discovery. • We evaluate seven LLMs across five benchmark datasets, demonstrating that certain models benefit significantly from hallucinated input. • We analyze model size, generation temperature, and hallucination types to understand how and when hallucinations contribute positively to performance."
            ],
            [
                "Hallucination Generation",
                "Large language models (LLMs), when prompted to describe molecules, often produce fluent yet factually incorrect descriptions, commonly referred to as hallucinations. While typically regarded as undesirable, we explore an alternative view: these hallucinations can be seen as implicit counterfactuals, imaginative variations on molecular structure or function that may enhance model performance in downstream prediction tasks. In this section, we detail how we generate these descriptions and analyze their properties."
            ],
            [
                "Task Formulation",
                "We define molecule property prediction as a prompt-based decision task for large language models (LLMs). Each instance consists of a molecule represented by a SMILES string [ S M I L E S ] [SMILES] , optionally augmented with a natural language description [ D e s c r i p t i o n ] [Description] , followed by a task-specific instruction [ I n s t r u c t ] [Instruct] . System : You are an expert in chemistry. User : [ S M I L E S ] [SMILES] [ D e s c r i p t i o n ] [Description] [ I n s t r u c t ] [Instruct] The model is then asked to answer a binary yes/no question, such as: “Does this molecule inhibit HIV replication? Only answer ‘Yes’ or ‘No’.” We decode the model’s prediction by selecting the highest-probability token: y ^ = arg ⁡ max y ∈ V ⁡ P ϕ ( y ) , V = { Yes , No } \\hat{y}=\\arg\\max_{y\\in V}P_{\\phi}(y),\\quad V=\\{\\text{Yes},\\text{No}\\} (1) Textual Conditioning as Counterfactual Context. Our central hypothesis is that the presence of [ D e s c r i p t i o n ] [Description] , regardless of factual accuracy, provides useful inductive bias for the model. These descriptions vary across three settings: • None (SMILES-only) : The model receives no textual context beyond the SMILES string. • Factual : A structured, rule-based description from PubChem is included. • LLM-generated : A free-text description is generated as described in the previous section, and may include hallucinations. Rather than treating hallucinated descriptions as noise, we consider them as implicit counterfactuals , alternative narratives about molecular properties that may help the model distinguish subtle patterns. This setup enables us to investigate how speculative or misaligned information interacts with LLM decision-making in molecular tasks."
            ],
            [
                "Experimental Design",
                "We investigate how molecule-level hallucinations generated by different LLMs affect downstream classification performance. The prompt format is constructed as [ S M I L E S ] [ D e s c r i p t i o n ] [ I n s t r u c t ] [SMILES][Description][Instruct] , where [ D e s c r i p t i o n ] [Description] is varied under four different settings: SMILES : [ D e s c r i p t i o n ] [Description] is set to ϵ \\epsilon (empty), and the model must make predictions based solely on the SMILES string. MolT5 : We use descriptions generated by MolT5 (Edwards et al. 2022 ) , a pretrained molecule-to-text model. This provides a reference point with high-quality factual descriptions aligned to molecular structure. PubChem : We extract rule-based factual information from the PubChem database (Kim et al. 2025 ) , including molecular formula, atom count, synonyms, and molecular weight. This metadata is formatted into a natural language description, e.g., “This compound, known as X, has the molecular formula C 22 H 23 N 7 O 2 …” . While this setting reflects gold-standard chemical knowledge, it is limited to molecules present in PubChem (88% coverage). As such, this baseline is not intended for practical deployment but to assess model behavior under ideal factual conditions. LLMs : To study the effects of hallucinations, we replace [ D e s c r i p t i o n ] [Description] with text generated by different LLMs. Each model is evaluated using (a) its own hallucinated descriptions and (b) hallucinations generated by other LLMs. This setup allows us to analyze if performance is influenced more by the source of the hallucination or the model that interprets it."
            ],
            [
                "Main Results and Analysis",
                "We conduct experiments to address three primary questions: i) Can hallucinated descriptions improve LLM performance compared to standard baselines? ii) Which LLMs generate the most beneficial hallucinations? iii) What types of hallucinations are most likely to improve prediction?"
            ],
            [
                "Ablation Studies",
                "To better understand when and why hallucinated text improves molecular prediction, we conduct ablation studies focusing on two factors: model size and generation temperature. We use the Llama-3 family due to its consistent architecture and multiple available sizes. As PubChem coverage is incomplete, we report improvements relative to SMILES and MolT5 baselines."
            ],
            [
                "Conclusion",
                "This paper investigates whether hallucinations can improve the performance of Large Language Models (LLMs) in predicting molecule properties. We evaluate seven LLMs across five datasets by incorporating natural language descriptions of molecules, which often contain hallucinations, into the input prompts. Our results show that hallucinated input can improve classification performance for most LLMs, even outperforming accurate baselines like MolT5 and PubChem in some cases. In particular, Falcon3-Mamba-7B and hallucinations generated by GPT-4o yield the most consistent improvements across datasets. We further categorize the beneficial hallucinations and find that structural misdescription is the most common and impactful type, suggesting that factual deviation does not necessarily harm, and may even help, LLM decision-making. Ablation studies reveal that larger models are better at leveraging hallucinated descriptions, while variation in generation temperature has limited impact on downstream performance. Our findings offer a new perspective on the role of hallucinations in LLM-based scientific tasks, highlighting their potential to act as implicit counterfactuals that encourage flexible reasoning and robust prediction. Future work may further investigate the mechanisms by which LLMs benefit from such hallucinated input and how to leverage this creative potential in a safe and intentional manner, especially in high-stakes domains."
            ]
        ],
        "section_summaries": [
            [
                "Introduction",
                "Large language models (LLMs) have demonstrated impressive capabilities across diverse domains, including everyday applications (Ge et al. 2023 ; Yao et al. 2024 ) , scientific discovery (Zhang et al. 2024b ; Madani et al. 2023 ) , and chemistry (Boiko et al. 2023 ; Zhang et al. 2024a ) . However, LLMs are known to produce hallucinations, text that is plausible but factually incorrect or misaligned with the input (Huang et al. 2023 ; Bang et al. 2023 ; Rawte et al. 2023 ; Yuan and Faerber 2023 ) . While much work has focused on mitigating hallucinations (Ji et al. 2023 ; Dhuliawala et al. 2024 ) , recent perspectives suggest that hallucination may also be beneficial, for example, it may enable creativity (Lee 2023 ; Ye et al. 2023 ; Wang 2024 ) . In domains like drug discovery, creativity is a vital asset. Evaluating molecular properties for potential drug candidates involves reasoning over vast chemical spaces and abstract functional traits (Hughes et al. 2011 ) . LLMs are beginning to assist in this process (Chakraborty, Bhattacharya, and Lee 2023 ; Edwards, Wang, and Ji 2024 ) , and textual molecule descriptions have been shown to help improve generalization (Liu et al. 2023 ) . Yet, many LLM-generated descriptions contain hallucinations, as illustrated in Figure 1 , where models deviate significantly from domain-specific references such as MolT5 (Edwards et al. 2022 ) . This raises an underexplored question: Can hallucinations improve LLMs’ performance in molecule property prediction? While hallucinations are often treated as failure cases, we hypothesize that they may act as implicit counterfactuals, offering speculative, high-level interpretations that help LLMs generalize over unseen compounds. This idea is particularly relevant in early-stage drug discovery, where structured knowledge is sparse and creativity may be an advantage. To investigate this, we conduct a comprehensive evaluation of seven instruction-tuned LLMs across five molecular property prediction tasks from MoleculeNet (Wu et al. 2018 ) . Given a molecule’s SMILES string (Weininger 1988 ) , we generate natural language descriptions using each LLM, then include the description in the prompt alongside a task-specific instruction (Figure 2 ). We compare the resulting performance against three baselines: • SMILES : using only the molecule string, • MolT5 : domain-specific reference descriptions, • PubChem : rule-based text derived from authoritative chemical metadata (Kim et al. 2025 ) . Our results show that LLM-generated hallucinations can enhance performance. For example, Falcon3-Mamba-7B surpasses the PubChem baseline by 8.22% in ROC-AUC. Llama-3.1-8B achieves 15.8% and 11.2% improvements over the SMILES and MolT5 baselines, respectively. Hallucinations from GPT-4o deliver the most consistent gains across models. We further conduct ablation studies and analyze the effects of model size and generation temperature. Larger models benefit more from hallucinations, while temperature affects factuality (hallucination rate) but has limited influence on downstream performance. To understand the nature of these helpful hallucinations, we collect instances that lead to improved predictions and categorize them into four types: structural misdescription , functional hallucination , analogical hallucination , and generic fluff . Surprisingly, the dominant type is structural misdescription , suggesting that factual deviation is not necessarily detrimental, and may help LLMs generalize. Our contributions are as follows: • We conduct the first systematic investigation into whether hallucinations can improve LLMs for molecular property prediction, a core subtask in early-stage drug discovery. • We evaluate seven LLMs across five benchmark datasets, demonstrating that certain models benefit significantly from hallucinated input. • We analyze model size, generation temperature, and hallucination types to understand how and when hallucinations contribute positively to performance.",
                "Ge et al. (2023), Yao et al. (2024), Zhang et al. (2024b, 2024a), Madani et al. (2023), Boiko et al. (2023), Huang et al. (2023), Bang et al. (2023), Rawte et al. (2023), Yuan and Faerber (2023), Ji et al. (2023), Dhuliawala et al. (2024), Lee (2023), Ye et al. (2023), Wang (2024)\nHughes et al. (2011), Chakraborty, Bhattacharya, and Lee (2023), Edwards, Wang, and Ji (2024), Liu et al. (2023), Edwards et al. (2022), MolT5\nMoleculeNet, Wu et al. (2018)\nSMILES string: Weininger (1988), PubChem: Kim et al. (2025)\n7 LLMs: Falcon3-Mamba-7B, Llama-3.1-8B, GPT-4o\nROC-AUC improvements: 8.22%, 15.8%, 11.2%\nModel size and generation temperature\nFour types of hallucinations: structural misdescription, functional hallucination, analogical hallucination, generic fluff"
            ],
            [
                "Hallucination Generation",
                "Large language models (LLMs), when prompted to describe molecules, often produce fluent yet factually incorrect descriptions, commonly referred to as hallucinations. While typically regarded as undesirable, we explore an alternative view: these hallucinations can be seen as implicit counterfactuals, imaginative variations on molecular structure or function that may enhance model performance in downstream prediction tasks. In this section, we detail how we generate these descriptions and analyze their properties.",
                "Large language models produce fluent yet factually incorrect molecular descriptions (hallucinations), which can be viewed as implicit counterfactuals enhancing model performance in downstream tasks."
            ],
            [
                "Task Formulation",
                "We define molecule property prediction as a prompt-based decision task for large language models (LLMs). Each instance consists of a molecule represented by a SMILES string [ S M I L E S ] [SMILES] , optionally augmented with a natural language description [ D e s c r i p t i o n ] [Description] , followed by a task-specific instruction [ I n s t r u c t ] [Instruct] . System : You are an expert in chemistry. User : [ S M I L E S ] [SMILES] [ D e s c r i p t i o n ] [Description] [ I n s t r u c t ] [Instruct] The model is then asked to answer a binary yes/no question, such as: “Does this molecule inhibit HIV replication? Only answer ‘Yes’ or ‘No’.” We decode the model’s prediction by selecting the highest-probability token: y ^ = arg ⁡ max y ∈ V ⁡ P ϕ ( y ) , V = { Yes , No } \\hat{y}=\\arg\\max_{y\\in V}P_{\\phi}(y),\\quad V=\\{\\text{Yes},\\text{No}\\} (1) Textual Conditioning as Counterfactual Context. Our central hypothesis is that the presence of [ D e s c r i p t i o n ] [Description] , regardless of factual accuracy, provides useful inductive bias for the model. These descriptions vary across three settings: • None (SMILES-only) : The model receives no textual context beyond the SMILES string. • Factual : A structured, rule-based description from PubChem is included. • LLM-generated : A free-text description is generated as described in the previous section, and may include hallucinations. Rather than treating hallucinated descriptions as noise, we consider them as implicit counterfactuals , alternative narratives about molecular properties that may help the model distinguish subtle patterns. This setup enables us to investigate how speculative or misaligned information interacts with LLM decision-making in molecular tasks.",
                "We define molecule property prediction as a prompt-based decision task for large language models (LLMs). The task involves representing molecules by SMILES strings, optionally augmented with natural language descriptions, and providing task-specific instructions. The model answers binary yes/no questions about molecular properties. We test three settings: None (SMILES-only), Factual (structured PubChem description), and LLM-generated (free-text description that may include hallucinations)."
            ],
            [
                "Experimental Design",
                "We investigate how molecule-level hallucinations generated by different LLMs affect downstream classification performance. The prompt format is constructed as [ S M I L E S ] [ D e s c r i p t i o n ] [ I n s t r u c t ] [SMILES][Description][Instruct] , where [ D e s c r i p t i o n ] [Description] is varied under four different settings: SMILES : [ D e s c r i p t i o n ] [Description] is set to ϵ \\epsilon (empty), and the model must make predictions based solely on the SMILES string. MolT5 : We use descriptions generated by MolT5 (Edwards et al. 2022 ) , a pretrained molecule-to-text model. This provides a reference point with high-quality factual descriptions aligned to molecular structure. PubChem : We extract rule-based factual information from the PubChem database (Kim et al. 2025 ) , including molecular formula, atom count, synonyms, and molecular weight. This metadata is formatted into a natural language description, e.g., “This compound, known as X, has the molecular formula C 22 H 23 N 7 O 2 …” . While this setting reflects gold-standard chemical knowledge, it is limited to molecules present in PubChem (88% coverage). As such, this baseline is not intended for practical deployment but to assess model behavior under ideal factual conditions. LLMs : To study the effects of hallucinations, we replace [ D e s c r i p t i o n ] [Description] with text generated by different LLMs. Each model is evaluated using (a) its own hallucinated descriptions and (b) hallucinations generated by other LLMs. This setup allows us to analyze if performance is influenced more by the source of the hallucination or the model that interprets it.",
                "The prompt format is [ S M I L E S ] [ D e s c r i p t i o n ] [ I n s t r u c t ] . Four different settings are used: SMILES (empty description), MolT5 (pretrained model descriptions, 2022), PubChem (rule-based factual information, 88% coverage, Kim et al. 2025), and LLMs (hallucinated descriptions from various models)."
            ],
            [
                "Main Results and Analysis",
                "We conduct experiments to address three primary questions: i) Can hallucinated descriptions improve LLM performance compared to standard baselines? ii) Which LLMs generate the most beneficial hallucinations? iii) What types of hallucinations are most likely to improve prediction?",
                "i) Baseline comparison with hallucinated descriptions, \nii) Most beneficial LLMs for hallucinations,\niii) Beneficial hallucination types."
            ],
            [
                "Ablation Studies",
                "To better understand when and why hallucinated text improves molecular prediction, we conduct ablation studies focusing on two factors: model size and generation temperature. We use the Llama-3 family due to its consistent architecture and multiple available sizes. As PubChem coverage is incomplete, we report improvements relative to SMILES and MolT5 baselines.",
                "Llama-3 family models with varying sizes are used for ablation studies on model size and generation temperature effects on hallucinated text improvement. Improvements are reported relative to SMILES and MolT5 baselines due to PubChem coverage limitations."
            ],
            [
                "Conclusion",
                "This paper investigates whether hallucinations can improve the performance of Large Language Models (LLMs) in predicting molecule properties. We evaluate seven LLMs across five datasets by incorporating natural language descriptions of molecules, which often contain hallucinations, into the input prompts. Our results show that hallucinated input can improve classification performance for most LLMs, even outperforming accurate baselines like MolT5 and PubChem in some cases. In particular, Falcon3-Mamba-7B and hallucinations generated by GPT-4o yield the most consistent improvements across datasets. We further categorize the beneficial hallucinations and find that structural misdescription is the most common and impactful type, suggesting that factual deviation does not necessarily harm, and may even help, LLM decision-making. Ablation studies reveal that larger models are better at leveraging hallucinated descriptions, while variation in generation temperature has limited impact on downstream performance. Our findings offer a new perspective on the role of hallucinations in LLM-based scientific tasks, highlighting their potential to act as implicit counterfactuals that encourage flexible reasoning and robust prediction. Future work may further investigate the mechanisms by which LLMs benefit from such hallucinated input and how to leverage this creative potential in a safe and intentional manner, especially in high-stakes domains.",
                "Seven LLMs were evaluated across five datasets. Hallucinations improved classification performance for most models, outperforming baselines like MolT5 and PubChem in some cases. Falcon3-Mamba-7B and GPT-4o generated the most consistent improvements. Structural misdescription was the most common and impactful type of beneficial hallucination. Larger models leverage hallucinated descriptions better, while generation temperature has limited impact."
            ]
        ],
        "general_summary": "**Research Overview**\nThis research investigates whether hallucinations in large language models (LLMs) can improve predictive accuracy for molecule property prediction in early-stage drug discovery.\n\n**Methodology & Approach**\nThe authors prompt LLMs to generate natural language descriptions from molecular SMILES strings, which may include factually inaccurate but plausible text (hallucinations). These descriptions are incorporated into downstream classification tasks to evaluate the effectiveness of hallucinations. Seven instruction-tuned LLMs across five datasets are evaluated.\n\n**Key Contributions & Findings**\nHallucinations significantly improve predictive accuracy for some models, with Falcon3-Mamba-7B outperforming all baselines when hallucinated text is included. GPT-4o consistently yields the greatest gains between models. Over 18,000 beneficial hallucinations were identified, with structural misdescriptions emerging as the most impactful type.\n\n**Technical Details**\nThe authors use a prompt format of [ S M I L E S ] [ D e s c r i p t i o n ] [ I n s t r u c t ] and test four different settings: SMILES (empty description), MolT5, PubChem, and LLMs. Ablation studies show that larger models benefit more from hallucinations.\n\n**Limitations & Future Work**\nThe current limitations include the reliance on specific model architectures and datasets. Future work should explore how to leverage hallucinations in different modeling tasks and domains.\n\n**Practical Implications**\nThis research suggests that hallucinations can be a useful signal in scientific modeling tasks like drug discovery, challenging conventional views of hallucination as purely problematic. The findings have implications for the development of more effective LLMs for molecule property prediction."
    },
    {
        "id": "2503.12908v4",
        "title": "HICD: Hallucination-Inducing via Attention Dispersion for Contrastive\n  Decoding to Mitigate Hallucinations in Large Language Models",
        "authors": [
            "Xinyan Jiang",
            "Hang Ye",
            "Yongxin Zhu",
            "Xiaoying Zheng",
            "Zikang Chen",
            "Jun Gong"
        ],
        "url": "https://arxiv.org/abs/2503.12908v4",
        "Abstract": "Abstract Large Language Models (LLMs) often generate hallucinations, producing outputs that are contextually inaccurate or factually incorrect. We introduce HICD , a novel method designed to induce hallucinations for contrastive decoding to mitigate hallucinations. Unlike existing contrastive decoding methods, HICD selects attention heads crucial to the model’s prediction as inducing heads, then induces hallucinations by dispersing attention of these inducing heads and compares the hallucinated outputs with the original outputs to obtain the final result. Our approach significantly improves performance on tasks requiring contextual faithfulness, such as context completion, reading comprehension, and question answering. It also improves factuality in tasks requiring accurate knowledge recall. We demonstrate that our inducing heads selection and attention dispersion method leads to more \"contrast-effective\" hallucinations for contrastive decoding, outperforming other hallucination-inducing methods. Our findings provide a promising strategy for reducing hallucinations by inducing hallucinations in a controlled manner, enhancing the performance of LLMs in a wide range of tasks. 1 1 1 https://github.com/waitxian/HICD.git",
        "Main": {
            "1 Introduction": "Large language models(LLMs) have demonstrated exceptional performance across a wide range of NLP tasks Brown et al. ( 2020 ); Wang et al. ( 2024 ) . However, they are prone to hallucinations, where they generate content that deviates from facts or relevant contexts, hindering their practical applications in real-world scenarios. To address this challenge, efforts have been devoted to mitigate knowledge hallucinations in LLMs Kojima et al. ( 2022 ); Dhuliawala et al. ( 2023 ) . In this work, we focus on mitigating hallucinations during inference generation Li et al. ( 2024 ) . To address this, some studies have focused on developing effective inference-time decoding strategies. Among these, contrastive decoding based approaches have demonstrated strong performance Shi et al. ( 2024 ) . However, current contrastive decoding methods typically compare the model’s inherent outputs, such as those from earlier layers or smaller models, with the original outputs Chuang et al. ( 2024b ); Li et al. ( 2023c ) . Existing contrastive decoding approaches have rarely explored constructing hallucinated outputs to improve their efficacy in hallucination mitigation Sahoo et al. ( 2024 ) . Previous work has highlighted that current contrastive decoding methods, due to their coarse contrast and simplistic subtraction operations, may disrupt the original output distribution of the LLM Chen et al. ( 2024 ) . Therefore, investigating the construction of hallucinated outputs for more effective contrast with original outputs warrants further research. Building on this, Zhang et al. ( 2023b ) proposed inducing hallucinations in LLMs via slight fine-tuning or zero-shot prompting, and mitigating them through contrastive decoding with the original outputs. And there’s a method that prunes retrieval heads to generate hallucinated outputs for comparison with the original outputs Gema et al. ( 2024 ) . However, these hallucination-inducing methods require additional fine-tuning or rely on the inherent properties of the model post-pretraining, limiting their adaptability in different datasets. Moreover, the plausibility of the hallucinations and their effectiveness for contrastive decoding have not been validated. Other works have addressed the issue of hallucinations by focusing on model interpretability. Some studies examined attention heads that play a key role in output quality Bansal et al. ( 2023 ) . Another study revealed that key points causing hallucinations in LLMs are the inconsistencies in the information flow integration between memory heads and context heads, and effectively mitigated hallucinations by pruning conflicting attention heads Jin et al. ( 2024 ) . This suggests that targeting the attention heads critical to hallucinated outputs can effectively control hallucination generation. Inspired by these studies,we propose HICD , a method that induces hallucinations through attention dispersion on inducing heads for contrastive decoding to mitigate hallucinations . To address the limitation that existing hallucination-inducing methods rely on model’s internal parameters, restricting adaptability to different datasets, we construct correct and incorrect (adversarial) samples by pairing questions with corresponding right and wrong answers. We then compute task-relevant importance scores for attention heads that are critical to generating correct outputs ( right heads ) and incorrect outputs ( wrong heads ). Finally, we select heads that contribute to correct outputs while suppressing those leading to incorrect outputs, resulting in a set of inducing heads . To improve the effectiveness of contrastive decoding methods, the attention maps of the inducing heads are averaged, ensuring attention values are equalized across all tokens within each head. This redistribution disperses attention, effectively inducing hallucinated outputs optimized for contrastive decoding, as demonstrated by experiments. Finally, these hallucinated outputs are compared with the original model’s outputs to mitigate hallucinations. Our experiments are primarily conducted using models from the LLaMA Touvron et al. ( 2023b ) , Qwen Bai et al. ( 2023 ) , and Mistral Jiang et al. ( 2023 ) families. Our findings show that compared to existing contrastive decoding methods, HICD significantly improves faithfulness in tasks requiring contextual understanding, such as HellaSwag Zellers et al. ( 2019 ) , RACE Lai et al. ( 2017 ) , OpenBookQA Mihaylov et al. ( 2018 ) . Furthermore, HICD also enhances the model’s accuracy in factual recall tasks like TruthfulQA Lin et al. ( 2022 ) and Factor Muhlgay et al. ( 2024 ) , as well as generation tasks on XSum Chuang et al. ( 2024a ) and NQ-Swap Longpre et al. ( 2021 ) . Our contributions are as follows: • Task-Driven Inducing Head Selection: Inducing heads selected based on task, yield more effective hallucination induction than task-irrelevant selecting methods. • Attention Dispersion: Averaging the attention maps of inducing heads increases the effectiveness of hallucinated outputs by allowing context with lower relevance to the prediction to influence the results. • Contrast Effective: HICD leads to more effective hallucination outputs and better mitigation during contrastive decoding.",
            "3 Method": "The overall algorithm of HICD is shown in Figure 1 . First, we identify the inducing heads that are closely associated with generating hallucinations( 3.1 ). Next, we apply attention dispersion to these inducing heads to induce task-relevant hallucinations ( 3.2 ). Finally, these hallucinated outputs are compared with the original model outputs through contrastive decoding to alleviate hallucinations ( 3.3 ).",
            "5 Conclusion": "In this paper, HICD are introduced to induce hallucinations on inducing heads for contrastive decoding to mitigate hallucinations. Experiments on several tasks show that HICD outperforms existing methods in contextual tasks and achieves competitive results in factual consistency tasks. We also find that selecting task-relevant inducing heads improves performance compared to out-of-domain selections. And attention averaging induces more contrast-effective hallucinations compared to other methods. Our work opens new directions for hallucination induction and mitigation, providing a promising strategy to reduce hallucinations and enhance LLM robustness across tasks.",
            "6 Limitations": "The HICD method shows strong improvements in hallucination mitigation, but it has several limitations. First, its effectiveness depends on task-relevant induced head selection, which may not generalize well to all tasks, especially those underrepresented in training data. Second, attention map averaging for hallucination induction can be computationally expensive, particularly for larger models and datasets, making scalability a concern for real-time or resource-limited applications. Lastly, the method’s performance relies on the quality of adversarial data, and future work should explore how different adversarial data construction methods impact performance across various tasks and domains.",
            "7 Acknowledgements": "This work was supported by the National Natural Science Foundation of China (NSFC) under grant no. 12475196 and 12373113."
        },
        "Tuples": [
            [
                "1 Introduction",
                "Large language models(LLMs) have demonstrated exceptional performance across a wide range of NLP tasks Brown et al. ( 2020 ); Wang et al. ( 2024 ) . However, they are prone to hallucinations, where they generate content that deviates from facts or relevant contexts, hindering their practical applications in real-world scenarios. To address this challenge, efforts have been devoted to mitigate knowledge hallucinations in LLMs Kojima et al. ( 2022 ); Dhuliawala et al. ( 2023 ) . In this work, we focus on mitigating hallucinations during inference generation Li et al. ( 2024 ) . To address this, some studies have focused on developing effective inference-time decoding strategies. Among these, contrastive decoding based approaches have demonstrated strong performance Shi et al. ( 2024 ) . However, current contrastive decoding methods typically compare the model’s inherent outputs, such as those from earlier layers or smaller models, with the original outputs Chuang et al. ( 2024b ); Li et al. ( 2023c ) . Existing contrastive decoding approaches have rarely explored constructing hallucinated outputs to improve their efficacy in hallucination mitigation Sahoo et al. ( 2024 ) . Previous work has highlighted that current contrastive decoding methods, due to their coarse contrast and simplistic subtraction operations, may disrupt the original output distribution of the LLM Chen et al. ( 2024 ) . Therefore, investigating the construction of hallucinated outputs for more effective contrast with original outputs warrants further research. Building on this, Zhang et al. ( 2023b ) proposed inducing hallucinations in LLMs via slight fine-tuning or zero-shot prompting, and mitigating them through contrastive decoding with the original outputs. And there’s a method that prunes retrieval heads to generate hallucinated outputs for comparison with the original outputs Gema et al. ( 2024 ) . However, these hallucination-inducing methods require additional fine-tuning or rely on the inherent properties of the model post-pretraining, limiting their adaptability in different datasets. Moreover, the plausibility of the hallucinations and their effectiveness for contrastive decoding have not been validated. Other works have addressed the issue of hallucinations by focusing on model interpretability. Some studies examined attention heads that play a key role in output quality Bansal et al. ( 2023 ) . Another study revealed that key points causing hallucinations in LLMs are the inconsistencies in the information flow integration between memory heads and context heads, and effectively mitigated hallucinations by pruning conflicting attention heads Jin et al. ( 2024 ) . This suggests that targeting the attention heads critical to hallucinated outputs can effectively control hallucination generation. Inspired by these studies,we propose HICD , a method that induces hallucinations through attention dispersion on inducing heads for contrastive decoding to mitigate hallucinations . To address the limitation that existing hallucination-inducing methods rely on model’s internal parameters, restricting adaptability to different datasets, we construct correct and incorrect (adversarial) samples by pairing questions with corresponding right and wrong answers. We then compute task-relevant importance scores for attention heads that are critical to generating correct outputs ( right heads ) and incorrect outputs ( wrong heads ). Finally, we select heads that contribute to correct outputs while suppressing those leading to incorrect outputs, resulting in a set of inducing heads . To improve the effectiveness of contrastive decoding methods, the attention maps of the inducing heads are averaged, ensuring attention values are equalized across all tokens within each head. This redistribution disperses attention, effectively inducing hallucinated outputs optimized for contrastive decoding, as demonstrated by experiments. Finally, these hallucinated outputs are compared with the original model’s outputs to mitigate hallucinations. Our experiments are primarily conducted using models from the LLaMA Touvron et al. ( 2023b ) , Qwen Bai et al. ( 2023 ) , and Mistral Jiang et al. ( 2023 ) families. Our findings show that compared to existing contrastive decoding methods, HICD significantly improves faithfulness in tasks requiring contextual understanding, such as HellaSwag Zellers et al. ( 2019 ) , RACE Lai et al. ( 2017 ) , OpenBookQA Mihaylov et al. ( 2018 ) . Furthermore, HICD also enhances the model’s accuracy in factual recall tasks like TruthfulQA Lin et al. ( 2022 ) and Factor Muhlgay et al. ( 2024 ) , as well as generation tasks on XSum Chuang et al. ( 2024a ) and NQ-Swap Longpre et al. ( 2021 ) . Our contributions are as follows: • Task-Driven Inducing Head Selection: Inducing heads selected based on task, yield more effective hallucination induction than task-irrelevant selecting methods. • Attention Dispersion: Averaging the attention maps of inducing heads increases the effectiveness of hallucinated outputs by allowing context with lower relevance to the prediction to influence the results. • Contrast Effective: HICD leads to more effective hallucination outputs and better mitigation during contrastive decoding."
            ],
            [
                "3 Method",
                "The overall algorithm of HICD is shown in Figure 1 . First, we identify the inducing heads that are closely associated with generating hallucinations( 3.1 ). Next, we apply attention dispersion to these inducing heads to induce task-relevant hallucinations ( 3.2 ). Finally, these hallucinated outputs are compared with the original model outputs through contrastive decoding to alleviate hallucinations ( 3.3 )."
            ],
            [
                "5 Conclusion",
                "In this paper, HICD are introduced to induce hallucinations on inducing heads for contrastive decoding to mitigate hallucinations. Experiments on several tasks show that HICD outperforms existing methods in contextual tasks and achieves competitive results in factual consistency tasks. We also find that selecting task-relevant inducing heads improves performance compared to out-of-domain selections. And attention averaging induces more contrast-effective hallucinations compared to other methods. Our work opens new directions for hallucination induction and mitigation, providing a promising strategy to reduce hallucinations and enhance LLM robustness across tasks."
            ],
            [
                "6 Limitations",
                "The HICD method shows strong improvements in hallucination mitigation, but it has several limitations. First, its effectiveness depends on task-relevant induced head selection, which may not generalize well to all tasks, especially those underrepresented in training data. Second, attention map averaging for hallucination induction can be computationally expensive, particularly for larger models and datasets, making scalability a concern for real-time or resource-limited applications. Lastly, the method’s performance relies on the quality of adversarial data, and future work should explore how different adversarial data construction methods impact performance across various tasks and domains."
            ],
            [
                "7 Acknowledgements",
                "This work was supported by the National Natural Science Foundation of China (NSFC) under grant no. 12475196 and 12373113."
            ]
        ],
        "section_summaries": [
            [
                "1 Introduction",
                "Large language models(LLMs) have demonstrated exceptional performance across a wide range of NLP tasks Brown et al. ( 2020 ); Wang et al. ( 2024 ) . However, they are prone to hallucinations, where they generate content that deviates from facts or relevant contexts, hindering their practical applications in real-world scenarios. To address this challenge, efforts have been devoted to mitigate knowledge hallucinations in LLMs Kojima et al. ( 2022 ); Dhuliawala et al. ( 2023 ) . In this work, we focus on mitigating hallucinations during inference generation Li et al. ( 2024 ) . To address this, some studies have focused on developing effective inference-time decoding strategies. Among these, contrastive decoding based approaches have demonstrated strong performance Shi et al. ( 2024 ) . However, current contrastive decoding methods typically compare the model’s inherent outputs, such as those from earlier layers or smaller models, with the original outputs Chuang et al. ( 2024b ); Li et al. ( 2023c ) . Existing contrastive decoding approaches have rarely explored constructing hallucinated outputs to improve their efficacy in hallucination mitigation Sahoo et al. ( 2024 ) . Previous work has highlighted that current contrastive decoding methods, due to their coarse contrast and simplistic subtraction operations, may disrupt the original output distribution of the LLM Chen et al. ( 2024 ) . Therefore, investigating the construction of hallucinated outputs for more effective contrast with original outputs warrants further research. Building on this, Zhang et al. ( 2023b ) proposed inducing hallucinations in LLMs via slight fine-tuning or zero-shot prompting, and mitigating them through contrastive decoding with the original outputs. And there’s a method that prunes retrieval heads to generate hallucinated outputs for comparison with the original outputs Gema et al. ( 2024 ) . However, these hallucination-inducing methods require additional fine-tuning or rely on the inherent properties of the model post-pretraining, limiting their adaptability in different datasets. Moreover, the plausibility of the hallucinations and their effectiveness for contrastive decoding have not been validated. Other works have addressed the issue of hallucinations by focusing on model interpretability. Some studies examined attention heads that play a key role in output quality Bansal et al. ( 2023 ) . Another study revealed that key points causing hallucinations in LLMs are the inconsistencies in the information flow integration between memory heads and context heads, and effectively mitigated hallucinations by pruning conflicting attention heads Jin et al. ( 2024 ) . This suggests that targeting the attention heads critical to hallucinated outputs can effectively control hallucination generation. Inspired by these studies,we propose HICD , a method that induces hallucinations through attention dispersion on inducing heads for contrastive decoding to mitigate hallucinations . To address the limitation that existing hallucination-inducing methods rely on model’s internal parameters, restricting adaptability to different datasets, we construct correct and incorrect (adversarial) samples by pairing questions with corresponding right and wrong answers. We then compute task-relevant importance scores for attention heads that are critical to generating correct outputs ( right heads ) and incorrect outputs ( wrong heads ). Finally, we select heads that contribute to correct outputs while suppressing those leading to incorrect outputs, resulting in a set of inducing heads . To improve the effectiveness of contrastive decoding methods, the attention maps of the inducing heads are averaged, ensuring attention values are equalized across all tokens within each head. This redistribution disperses attention, effectively inducing hallucinated outputs optimized for contrastive decoding, as demonstrated by experiments. Finally, these hallucinated outputs are compared with the original model’s outputs to mitigate hallucinations. Our experiments are primarily conducted using models from the LLaMA Touvron et al. ( 2023b ) , Qwen Bai et al. ( 2023 ) , and Mistral Jiang et al. ( 2023 ) families. Our findings show that compared to existing contrastive decoding methods, HICD significantly improves faithfulness in tasks requiring contextual understanding, such as HellaSwag Zellers et al. ( 2019 ) , RACE Lai et al. ( 2017 ) , OpenBookQA Mihaylov et al. ( 2018 ) . Furthermore, HICD also enhances the model’s accuracy in factual recall tasks like TruthfulQA Lin et al. ( 2022 ) and Factor Muhlgay et al. ( 2024 ) , as well as generation tasks on XSum Chuang et al. ( 2024a ) and NQ-Swap Longpre et al. ( 2021 ) . Our contributions are as follows: • Task-Driven Inducing Head Selection: Inducing heads selected based on task, yield more effective hallucination induction than task-irrelevant selecting methods. • Attention Dispersion: Averaging the attention maps of inducing heads increases the effectiveness of hallucinated outputs by allowing context with lower relevance to the prediction to influence the results. • Contrast Effective: HICD leads to more effective hallucination outputs and better mitigation during contrastive decoding.",
                "Brown et al. (2020) and Wang et al. (2024) demonstrated exceptional performance in NLP tasks, but LLMs are prone to hallucinations. Kojima et al. (2022), Dhuliawala et al. (2023), Li et al. (2024) addressed this challenge with inference-time decoding strategies like contrastive decoding. Shi et al. (2024) showed strong performance in contrastive decoding, but existing methods rarely explored constructing hallucinated outputs for better efficacy. Chen et al. (2024) highlighted the limitations of current contrastive decoding methods due to coarse contrast and simplistic subtraction operations.\n\nBuilding on Zhang et al. (2023b) and Gema et al. (2024), a new method called HICD induces hallucinations through attention dispersion on inducing heads for contrastive decoding to mitigate hallucinations. HICD constructs correct and incorrect samples, computes task-relevant importance scores, and selects inducing heads that contribute to correct outputs while suppressing those leading to incorrect outputs.\n\nOur experiments using LLaMA, Qwen Bai, and Mistral models show that HICD significantly improves faithfulness in tasks requiring contextual understanding like HellaSwag, RACE, OpenBookQA. It also enhances the model's accuracy in factual recall tasks like TruthfulQA and Factor, as well as generation tasks on XSum and NQ-Swap.\n\nHICD contributes to task-driven inducing head selection, attention dispersion, and contrast effective hallucination outputs and mitigation during contrastive decoding."
            ],
            [
                "3 Method",
                "The overall algorithm of HICD is shown in Figure 1 . First, we identify the inducing heads that are closely associated with generating hallucinations( 3.1 ). Next, we apply attention dispersion to these inducing heads to induce task-relevant hallucinations ( 3.2 ). Finally, these hallucinated outputs are compared with the original model outputs through contrastive decoding to alleviate hallucinations ( 3.3 ).",
                "HICD's algorithm involves identifying inducing heads associated with hallucinations (3.1), applying attention dispersion to induce relevant hallucinations (3.2), and comparing with original model outputs through contrastive decoding (3.3)."
            ],
            [
                "5 Conclusion",
                "In this paper, HICD are introduced to induce hallucinations on inducing heads for contrastive decoding to mitigate hallucinations. Experiments on several tasks show that HICD outperforms existing methods in contextual tasks and achieves competitive results in factual consistency tasks. We also find that selecting task-relevant inducing heads improves performance compared to out-of-domain selections. And attention averaging induces more contrast-effective hallucinations compared to other methods. Our work opens new directions for hallucination induction and mitigation, providing a promising strategy to reduce hallucinations and enhance LLM robustness across tasks.",
                "HICD outperforms existing methods in contextual tasks, achieves competitive results in factual consistency tasks, and reduces hallucinations with attention averaging and task-relevant inducing heads."
            ],
            [
                "6 Limitations",
                "The HICD method shows strong improvements in hallucination mitigation, but it has several limitations. First, its effectiveness depends on task-relevant induced head selection, which may not generalize well to all tasks, especially those underrepresented in training data. Second, attention map averaging for hallucination induction can be computationally expensive, particularly for larger models and datasets, making scalability a concern for real-time or resource-limited applications. Lastly, the method’s performance relies on the quality of adversarial data, and future work should explore how different adversarial data construction methods impact performance across various tasks and domains.",
                "The HICD method has limitations: (1) effectiveness depends on task-relevant induced head selection, (2) attention map averaging can be computationally expensive, and (3) performance relies on adversarial data quality."
            ],
            [
                "7 Acknowledgements",
                "This work was supported by the National Natural Science Foundation of China (NSFC) under grant no. 12475196 and 12373113.",
                "National Natural Science Foundation of China (NSFC) grants: 12475196, 12373113."
            ]
        ],
        "general_summary": "**Research Overview**\n\nThis research addresses the problem of hallucinations in Large Language Models (LLMs), which often produce contextually inaccurate or factually incorrect outputs. The proposed solution is HICD, a novel method that induces hallucinations for contrastive decoding to mitigate these inaccuracies.\n\n**Methodology & Approach**\n\nHICD constructs hallucinated outputs by selecting attention heads crucial to the model's prediction as inducing heads and dispersing their attention to induce relevant hallucinations. This approach is compared with original model outputs through contrastive decoding, resulting in a final output. The method improves faithfulness in tasks requiring contextual understanding and accuracy in factual recall tasks.\n\n**Key Contributions & Findings**\n\nHICD outperforms existing methods in contextual tasks, achieving significant improvements on tasks such as context completion, reading comprehension, and question answering. It also enhances factuality in tasks requiring accurate knowledge recall. The inducing heads selection and attention dispersion method leads to more \"contrast-effective\" hallucinations for contrastive decoding.\n\n**Technical Details**\n\nThe HICD algorithm involves three main steps: identifying inducing heads associated with hallucinations (3.1), applying attention dispersion to induce relevant hallucinations (3.2), and comparing with original model outputs through contrastive decoding (3.3). Attention map averaging can be computationally expensive, but it is a crucial step in the method.\n\n**Limitations & Future Work**\n\nThe HICD method has limitations: (1) effectiveness depends on task-relevant induced head selection, (2) attention map averaging can be computationally expensive, and (3) performance relies on adversarial data quality. Future research should focus on addressing these limitations and exploring the potential of HICD in other applications.\n\n**Practical Implications**\n\nHICD has significant practical implications for improving the performance of LLMs in a wide range of tasks. By inducing hallucinations in a controlled manner, HICD can enhance faithfulness and factuality in outputs, making it a promising strategy for reducing hallucinations in LLMs."
    },
    {
        "id": "2503.00436v2",
        "title": "HalCECE: A Framework for Explainable Hallucination Detection through\n  Conceptual Counterfactuals in Image Captioning",
        "authors": [
            "Maria Lymperaiou",
            "Giorgos Filandrianos",
            "Angeliki Dimitriou",
            "Athanasios Voulodimos",
            "Giorgos Stamou"
        ],
        "url": "https://arxiv.org/abs/2503.00436v2",
        "Abstract": "Abstract In the dynamic landscape of artificial intelligence, the exploration of hallucinations within vision-language (VL) models emerges as a critical frontier. This work delves into the intricacies of hallucinatory phenomena exhibited by widely used image captioners, unraveling interesting patterns. Specifically, we step upon previously introduced techniques of conceptual counterfactual explanations to address VL hallucinations. The deterministic and efficient nature of the employed conceptual counterfactuals backbone is able to suggest semantically minimal edits driven by hierarchical knowledge, so that the transition from a hallucinated caption to a non-hallucinated one is performed in a black-box manner. HalCECE, our proposed hallucination detection framework is highly interpretable, by providing semantically meaningful edits apart from standalone numbers, while the hierarchical decomposition of hallucinated concepts leads to a thorough hallucination analysis. Another novelty tied to the current work is the investigation of role hallucinations, being one of the first works to involve interconnections between visual concepts in hallucination detection. Overall, HalCECE recommends an explainable direction to the crucial field of VL hallucination detection, thus fostering trustworthy evaluation of current and future VL systems.",
        "Main": {
            "1 Introduction": "In the ever-evolving landscape of artificial intelligence, the appearance of hallucinations has emerged as a significant concern. While neural models showcase remarkable linguistic and/or visual prowess and creativity, their outputs occasionally veer into unpredictable directions, blurring the line between factual accuracy and imaginative fabrication. Hallucinations as a research topic have recently received attention in NLP, with Large Language Models (LLMs) producing unfaithful and inaccurate outputs, despite their sheer size in terms of trainable parameters and training data volume [ 29 , 55 , 44 , 2 ] . The nature of hallucination is tied with difficulty in their detection for several reasons, one of them being the variability in hallucination types. [ 55 ] recognize three hallucination categories: Input-Conflicting Hallucinations refer to unfaithful LLM generation in comparison to what the input prompt requested. Context-Conflicting Hallucinations involve inconsistencies within the generated output itself. Finally, Fact-Conflicting Hallucinations violate factuality, providing false information in the output. Even though LLM hallucinations have captivated significant interest in literature, multimodal settings, such as vision-language (VL) hallucinations, have not been adequately explored yet. Especially during the timely transition towards Large VL Models (LVLMs) [ 31 , 37 , 3 , 1 ] , impressive capabilities in VL understanding and generation are unavoidably accompanied by unfaithful outputs that are even more difficult in detection compared to LLM hallucinations due to intra-modality ambiguity and alignment challenges. The literature on hallucinations of VL models so far addresses some fundamental research questions regarding evaluation [ 43 , 25 , 48 , 14 , 32 , 53 ] and mitigation [ 56 , 44 , 21 , 28 , 41 , 35 ] . Nevertheless, it grapples with inherent limitations, notably in terms of the interpretability and granularity of metrics employed, hindering a comprehensive understanding of the nuanced challenges posed by hallucinatory phenomena in VL models. We argue that the current VL hallucinations research gaps emphasize the need for an explainable evaluation strategy [ 34 ] , which not only interprets inner workings behind hallucination occurrence, but also paves the way towards effective hallucination mitigation approaches. At the same time, we recognize some related endeavors in recent VL evaluation literature [ 33 ] , even though the term \"hallucination\" is not explicitly used. In this work, we set the scene for an explainable evaluation framework of VL hallucinations by applying our approach to image captioning, a task associated with hallucination challenges, as in Figure 1 . We borrow techniques from prior work in VL hallucination evaluation, specifically targeting image generation from language [ 33 ] , showcasing their effortless applicability in the reverse task of language generation from text. Since most current hallucination evaluation research focuses on object hallucination, i.e. the appearance of extraneous objects or, on the contrary, objects consistently missing in the generated output, while only a few assess role hallucinations [ 50 ] , referring to spatial relationships or actions, we aim to construct a unified framework named HalCECE, incorporating both of them through their projection on graph edits . In our proposed HalCECE framework, we retain fundamental properties of conceptual counterfactuals [ 10 ] and graph-driven edits [ 8 ] , which will be analyzed in subsequent sections. Overall, we present the following contributions: • We propose the adoption of explainable evaluation in image captioning hallucination detection contrasting typical captioning evaluation metrics. • We decompose concepts existing in captions to allow fine-grained evaluation and quantification of hallucination. • We substantiate our findings by applying our proposed evaluation framework to various image captioners of increasing model size.",
            "3 Explainable hallucinations evaluation": "Many of the contributions analyzed above harness LLMs at some point of the hallucination evaluation process. These approaches inevitably induce uncertainty related to the prompt used, while simultaneously facing the possibility of LLMs also hallucinating and ultimately hindering the robustness and trustworthiness of the affected module, and thus the evaluation framework itself. In our framework, we deviate from the usage of LLMs, sacrificing the simplicity they provide in order to enhance the determinism and reliability of the evaluation process. Other than that, both metrics evaluating linguistic quality, as well as metrics for VL hallucinations lack explainability aspects, since they do not suggest the direction of change towards dehallucinated generation. This direction of change should primarily be measurable and meaningful , while its optimal usage prescribes notions of optimality , translated to semantically minimal changes , as well as the fewest possible number of edits leading to the desired outcome. We will analyze these desiderata:",
            "6 Conclusion": "In conclusion, our novel HalCECE framework designed for detecting hallucinations in image captioning represents a pioneering stride towards explainable evaluation of ever evolving VL models. By delving into the hallucination mechanisms, we decompose related phenomena based on conceptual properties enabled by the incorporation of external hierarchical knowledge. Our proposed method imposes semantically minimal and meaningful edits to transit from hallucinated concepts present in captions to non-hallucinated ground truth ones, employing the explanatory power of conceptual counterfactuals. Moreover, previously overlooked role hallucinations are analyzed, revealing that widely-used image captioners tend to generate erroneous object interconnections more often than not. Overall, we view our current analysis as a crucial first step in the direction of accurately detecting hallucinations in VL models in a conceptual and explainable manner, paving the way for future hallucination mitigation strategies. {credits}"
        },
        "Tuples": [
            [
                "1 Introduction",
                "In the ever-evolving landscape of artificial intelligence, the appearance of hallucinations has emerged as a significant concern. While neural models showcase remarkable linguistic and/or visual prowess and creativity, their outputs occasionally veer into unpredictable directions, blurring the line between factual accuracy and imaginative fabrication. Hallucinations as a research topic have recently received attention in NLP, with Large Language Models (LLMs) producing unfaithful and inaccurate outputs, despite their sheer size in terms of trainable parameters and training data volume [ 29 , 55 , 44 , 2 ] . The nature of hallucination is tied with difficulty in their detection for several reasons, one of them being the variability in hallucination types. [ 55 ] recognize three hallucination categories: Input-Conflicting Hallucinations refer to unfaithful LLM generation in comparison to what the input prompt requested. Context-Conflicting Hallucinations involve inconsistencies within the generated output itself. Finally, Fact-Conflicting Hallucinations violate factuality, providing false information in the output. Even though LLM hallucinations have captivated significant interest in literature, multimodal settings, such as vision-language (VL) hallucinations, have not been adequately explored yet. Especially during the timely transition towards Large VL Models (LVLMs) [ 31 , 37 , 3 , 1 ] , impressive capabilities in VL understanding and generation are unavoidably accompanied by unfaithful outputs that are even more difficult in detection compared to LLM hallucinations due to intra-modality ambiguity and alignment challenges. The literature on hallucinations of VL models so far addresses some fundamental research questions regarding evaluation [ 43 , 25 , 48 , 14 , 32 , 53 ] and mitigation [ 56 , 44 , 21 , 28 , 41 , 35 ] . Nevertheless, it grapples with inherent limitations, notably in terms of the interpretability and granularity of metrics employed, hindering a comprehensive understanding of the nuanced challenges posed by hallucinatory phenomena in VL models. We argue that the current VL hallucinations research gaps emphasize the need for an explainable evaluation strategy [ 34 ] , which not only interprets inner workings behind hallucination occurrence, but also paves the way towards effective hallucination mitigation approaches. At the same time, we recognize some related endeavors in recent VL evaluation literature [ 33 ] , even though the term \"hallucination\" is not explicitly used. In this work, we set the scene for an explainable evaluation framework of VL hallucinations by applying our approach to image captioning, a task associated with hallucination challenges, as in Figure 1 . We borrow techniques from prior work in VL hallucination evaluation, specifically targeting image generation from language [ 33 ] , showcasing their effortless applicability in the reverse task of language generation from text. Since most current hallucination evaluation research focuses on object hallucination, i.e. the appearance of extraneous objects or, on the contrary, objects consistently missing in the generated output, while only a few assess role hallucinations [ 50 ] , referring to spatial relationships or actions, we aim to construct a unified framework named HalCECE, incorporating both of them through their projection on graph edits . In our proposed HalCECE framework, we retain fundamental properties of conceptual counterfactuals [ 10 ] and graph-driven edits [ 8 ] , which will be analyzed in subsequent sections. Overall, we present the following contributions: • We propose the adoption of explainable evaluation in image captioning hallucination detection contrasting typical captioning evaluation metrics. • We decompose concepts existing in captions to allow fine-grained evaluation and quantification of hallucination. • We substantiate our findings by applying our proposed evaluation framework to various image captioners of increasing model size."
            ],
            [
                "3 Explainable hallucinations evaluation",
                "Many of the contributions analyzed above harness LLMs at some point of the hallucination evaluation process. These approaches inevitably induce uncertainty related to the prompt used, while simultaneously facing the possibility of LLMs also hallucinating and ultimately hindering the robustness and trustworthiness of the affected module, and thus the evaluation framework itself. In our framework, we deviate from the usage of LLMs, sacrificing the simplicity they provide in order to enhance the determinism and reliability of the evaluation process. Other than that, both metrics evaluating linguistic quality, as well as metrics for VL hallucinations lack explainability aspects, since they do not suggest the direction of change towards dehallucinated generation. This direction of change should primarily be measurable and meaningful , while its optimal usage prescribes notions of optimality , translated to semantically minimal changes , as well as the fewest possible number of edits leading to the desired outcome. We will analyze these desiderata:"
            ],
            [
                "6 Conclusion",
                "In conclusion, our novel HalCECE framework designed for detecting hallucinations in image captioning represents a pioneering stride towards explainable evaluation of ever evolving VL models. By delving into the hallucination mechanisms, we decompose related phenomena based on conceptual properties enabled by the incorporation of external hierarchical knowledge. Our proposed method imposes semantically minimal and meaningful edits to transit from hallucinated concepts present in captions to non-hallucinated ground truth ones, employing the explanatory power of conceptual counterfactuals. Moreover, previously overlooked role hallucinations are analyzed, revealing that widely-used image captioners tend to generate erroneous object interconnections more often than not. Overall, we view our current analysis as a crucial first step in the direction of accurately detecting hallucinations in VL models in a conceptual and explainable manner, paving the way for future hallucination mitigation strategies. {credits}"
            ]
        ],
        "section_summaries": [
            [
                "1 Introduction",
                "In the ever-evolving landscape of artificial intelligence, the appearance of hallucinations has emerged as a significant concern. While neural models showcase remarkable linguistic and/or visual prowess and creativity, their outputs occasionally veer into unpredictable directions, blurring the line between factual accuracy and imaginative fabrication. Hallucinations as a research topic have recently received attention in NLP, with Large Language Models (LLMs) producing unfaithful and inaccurate outputs, despite their sheer size in terms of trainable parameters and training data volume [ 29 , 55 , 44 , 2 ] . The nature of hallucination is tied with difficulty in their detection for several reasons, one of them being the variability in hallucination types. [ 55 ] recognize three hallucination categories: Input-Conflicting Hallucinations refer to unfaithful LLM generation in comparison to what the input prompt requested. Context-Conflicting Hallucinations involve inconsistencies within the generated output itself. Finally, Fact-Conflicting Hallucinations violate factuality, providing false information in the output. Even though LLM hallucinations have captivated significant interest in literature, multimodal settings, such as vision-language (VL) hallucinations, have not been adequately explored yet. Especially during the timely transition towards Large VL Models (LVLMs) [ 31 , 37 , 3 , 1 ] , impressive capabilities in VL understanding and generation are unavoidably accompanied by unfaithful outputs that are even more difficult in detection compared to LLM hallucinations due to intra-modality ambiguity and alignment challenges. The literature on hallucinations of VL models so far addresses some fundamental research questions regarding evaluation [ 43 , 25 , 48 , 14 , 32 , 53 ] and mitigation [ 56 , 44 , 21 , 28 , 41 , 35 ] . Nevertheless, it grapples with inherent limitations, notably in terms of the interpretability and granularity of metrics employed, hindering a comprehensive understanding of the nuanced challenges posed by hallucinatory phenomena in VL models. We argue that the current VL hallucinations research gaps emphasize the need for an explainable evaluation strategy [ 34 ] , which not only interprets inner workings behind hallucination occurrence, but also paves the way towards effective hallucination mitigation approaches. At the same time, we recognize some related endeavors in recent VL evaluation literature [ 33 ] , even though the term \"hallucination\" is not explicitly used. In this work, we set the scene for an explainable evaluation framework of VL hallucinations by applying our approach to image captioning, a task associated with hallucination challenges, as in Figure 1 . We borrow techniques from prior work in VL hallucination evaluation, specifically targeting image generation from language [ 33 ] , showcasing their effortless applicability in the reverse task of language generation from text. Since most current hallucination evaluation research focuses on object hallucination, i.e. the appearance of extraneous objects or, on the contrary, objects consistently missing in the generated output, while only a few assess role hallucinations [ 50 ] , referring to spatial relationships or actions, we aim to construct a unified framework named HalCECE, incorporating both of them through their projection on graph edits . In our proposed HalCECE framework, we retain fundamental properties of conceptual counterfactuals [ 10 ] and graph-driven edits [ 8 ] , which will be analyzed in subsequent sections. Overall, we present the following contributions: • We propose the adoption of explainable evaluation in image captioning hallucination detection contrasting typical captioning evaluation metrics. • We decompose concepts existing in captions to allow fine-grained evaluation and quantification of hallucination. • We substantiate our findings by applying our proposed evaluation framework to various image captioners of increasing model size.",
                "Hallucinations have emerged as a significant concern in artificial intelligence, particularly in Large Language Models (LLMs) and Large Vision-Language Models (LVLMs). Research has recognized three hallucination categories: Input-Conflicting, Context-Conflicting, and Fact-Conflicting. VL models are challenging to detect due to intra-modality ambiguity and alignment challenges. Current research addresses evaluation and mitigation of hallucinations but grapples with limitations in interpretability and granularity of metrics. A comprehensive understanding is needed, emphasizing the need for an explainable evaluation strategy.\n\nThe summary includes:\n* Hallucination categories: Input-Conflicting, Context-Conflicting, Fact-Conflicting\n* Challenges with VL models: intra-modality ambiguity and alignment challenges\n* Current research limitations: interpretability and granularity of metrics\n* Need for an explainable evaluation strategy."
            ],
            [
                "3 Explainable hallucinations evaluation",
                "Many of the contributions analyzed above harness LLMs at some point of the hallucination evaluation process. These approaches inevitably induce uncertainty related to the prompt used, while simultaneously facing the possibility of LLMs also hallucinating and ultimately hindering the robustness and trustworthiness of the affected module, and thus the evaluation framework itself. In our framework, we deviate from the usage of LLMs, sacrificing the simplicity they provide in order to enhance the determinism and reliability of the evaluation process. Other than that, both metrics evaluating linguistic quality, as well as metrics for VL hallucinations lack explainability aspects, since they do not suggest the direction of change towards dehallucinated generation. This direction of change should primarily be measurable and meaningful , while its optimal usage prescribes notions of optimality , translated to semantically minimal changes , as well as the fewest possible number of edits leading to the desired outcome. We will analyze these desiderata:",
                "LLMs induce uncertainty and hallucinations, affecting robustness and trustworthiness.\nOur framework deviates from LLMs for determinism and reliability.\nMetrics evaluating linguistic quality and VL hallucinations lack explainability."
            ],
            [
                "6 Conclusion",
                "In conclusion, our novel HalCECE framework designed for detecting hallucinations in image captioning represents a pioneering stride towards explainable evaluation of ever evolving VL models. By delving into the hallucination mechanisms, we decompose related phenomena based on conceptual properties enabled by the incorporation of external hierarchical knowledge. Our proposed method imposes semantically minimal and meaningful edits to transit from hallucinated concepts present in captions to non-hallucinated ground truth ones, employing the explanatory power of conceptual counterfactuals. Moreover, previously overlooked role hallucinations are analyzed, revealing that widely-used image captioners tend to generate erroneous object interconnections more often than not. Overall, we view our current analysis as a crucial first step in the direction of accurately detecting hallucinations in VL models in a conceptual and explainable manner, paving the way for future hallucination mitigation strategies. {credits}",
                "Our HalCECE framework detects hallucinations in image captioning, making it easier to evaluate ever-evolving Vision Language (VL) models. It breaks down hallucinations based on conceptual properties and minimizes edits from hallucinated concepts to correct ones. This approach reveals that image captioners often generate incorrect object connections."
            ]
        ],
        "general_summary": "**Research Overview**\nThis research addresses the crucial problem of hallucination detection in vision-language (VL) models, particularly in image captioning. The emergence of hallucinations has become a significant concern in artificial intelligence, affecting the trustworthiness and robustness of current models.\n\n**Methodology & Approach**\nHalCECE, the proposed framework, leverages conceptual counterfactual explanations to address VL hallucinations. This approach employs a deterministic and efficient method that suggests semantically minimal edits based on hierarchical knowledge. The framework provides a black-box manner for transitioning from a hallucinated caption to a non-hallucinated one.\n\n**Key Contributions & Findings**\nThe HalCECE framework offers an explainable direction in addressing VL hallucinations, providing semantically meaningful edits and hierarchical decomposition of hallucinated concepts. This approach enables thorough hallucination analysis and is one of the first works to investigate role hallucinations by considering interconnections between visual concepts. The framework also reveals that image captioners often generate incorrect object connections.\n\n**Technical Details**\nThe conceptual counterfactuals backbone used in HalCECE allows for hierarchical knowledge-driven semantically minimal edits, enabling a black-box manner of transitioning from a hallucinated caption to a non-hallucinated one. This approach breaks down hallucinations based on conceptual properties and minimizes edits from hallucinated concepts to correct ones.\n\n**Limitations & Future Work**\nCurrent research faces limitations in interpretability and granularity of metrics. Further investigation is needed to address the challenges of intra-modality ambiguity and alignment challenges in VL models.\n\n**Practical Implications**\nThe HalCECE framework fosters trustworthy evaluation of current and future VL systems by providing an explainable approach to hallucination detection, which can have significant implications for the development and deployment of AI-powered image captioning systems."
    },
    {
        "id": "2506.06539v1",
        "title": "Beyond Facts: Evaluating Intent Hallucination in Large Language Models",
        "authors": [
            "Yijie Hao",
            "Haofei Yu",
            "Jiaxuan You"
        ],
        "url": "https://arxiv.org/abs/2506.06539v1",
        "Abstract": "Abstract When exposed to complex queries containing multiple conditions, today’s large language models (LLMs) tend to produce responses that only partially satisfy the query while neglecting certain conditions. We therefore introduce the concept of Intent Hallucination . In this phenomenon, LLMs either omit (neglecting to address certain parts) or misinterpret (responding to invented query parts) elements of the given query, leading to intent hallucinated generation. To systematically evaluate intent hallucination, we introduce FaithQA , a novel benchmark for intent hallucination that contains 20,068 problems, covering both query-only and retrieval-augmented generation (RAG) setups with varying topics and difficulty. FaithQA is the first hallucination benchmark that goes beyond factual verification, tailored to identify the fundamental cause of intent hallucination. By evaluating various LLMs on FaithQA , we find that (1) intent hallucination is a common issue even for state-of-the-art models, and (2) the phenomenon stems from omission or misinterpretation of LLMs. To facilitate future research, we introduce an automatic LLM generation evaluation metric, Constraint Score , for detecting intent hallucination. Human evaluation results demonstrate that Constraint Score is closer to human performance for intent hallucination compared to baselines.",
        "Main": {
            "1 Introduction": "Large Language Models (LLMs) have demonstrated utility across various applications (OpenAI et al., 2024 ; Dubey et al., 2024 ) . However, hallucination remains a significant challenge (Ji et al., 2023 ; Huang et al., 2023 ) . In particular, for complex queries containing multiple conditions (Figure 1 ), LLM outputs often deviate from the query, yielding unsatisfactory results. We term this phenomenon “Intent Hallucination”, which has received little attention in current research (Min et al., 2023 ; Hou et al., 2024 ; Manakul et al., 2023 ) . Unlike factual hallucination (Li et al., 2023 ; Cao et al., 2021 ) , which researchers can directly detect through search-based fact-checking (Sellam et al., 2020 ; Min et al., 2023 ) , detecting and evaluating intent hallucination poses more challenges. This is because complex queries often contain duplicate intents, and LLMs often satisfy only a portion of them, making dissatisfaction difficult to detect or quantify. Furthermore, as LLMs continue to advance, users tend to provide these stronger LLMs with increasingly complicated queries, which even humans find difficult to understand. This trend highlights the need for LLMs to be not only factually correct but also intentionally aligned with human beings. Our paper addresses two under-explored questions: (1) Why do LLMs tend to exhibit Intent Hallucination? and (2) How can we detect Intent Hallucination? Answering these questions is vital for LLM applications that rely on both factual accuracy and faithful intent alignment. For the first question, we propose that LLMs’ omission ( e.g. , ignoring query components) or misinterpretation ( e.g. , responding to invented query components) over word-level meaning constitutes the fundamental cause of intent hallucination. To further investigate, we introduce FaithQA , the first benchmark specifically designed to address intent hallucination’s two key scenarios: omission and misinterpretation. FaithQA consists of 20,068 queries, validated through extensive human evaluations to ensure quality. FaithQA covers a wide range of topics with varying levels of difficulty and proves challenging even for state-of-the-art models. Our benchmark reveals that increasing query complexity correlates with a higher rate of intent hallucination. To address the second question, we introduce Constraint Score , a new evaluation metric that focuses on detecting intent hallucination. Our approach involves two major steps: (1) decomposing the query by concepts and actions, then converting it into a series of short statements, each representing a specific requirement the generation must meet; and (2) assigning an importance-weighted binary label to each constraint, which enables fine-grained evaluation. Our human evaluation shows that Constraint Score significantly outperforms LLM-as-the-judge (Manakul et al., 2023 ; Mishra et al., 2024 ; Sriramanan et al., 2024 ) , as LLM judgers tend to provide biased evaluations compared with human judgment. Taken together, our key contributions include: (1) We propose the concept of intent hallucination beyond the existing category of factual hallucination; (2) We develop FaithQA , the first benchmark that focuses on evaluating intent hallucination. Our result shows that intent hallucination represents a prevalent phenomenon even for state-of-the-art LLMs; (3) We introduce Constraint Score , a novel evaluation metric that automatically assesses LLM generations by breaking down the query into intent constraints and computing a weighted score. Our analysis shows that Constraint Score significantly outperforms pure LLM grading baselines, which tend to be biased.",
            "2 Related Works": "Hallucinations in LLMs . In LLMs, \"hallucination\" refers to outputs that are nonfactual, irrelevant, or fabricated. This issue arises in tasks such as question answering (Sellam et al., 2020 ) , translation (Lee et al., 2018 ) , summarization (Durmus et al., 2020 ) , and dialogue (Balakrishnan et al., 2019 ) , as noted in several studies (Ji et al., 2023 ; Azaria and Mitchell, 2023 ; Huang et al., 2023 ; Cao et al., 2021 ) . To address this issue, many efforts aim to detect and mitigate hallucinations. Min et al. ( 2023 ) evaluate factual accuracy by checking core facts (atomic facts) in each sentence against reliable sources such as Wikipedia. Hou et al. ( 2024 ) propose a hidden Markov tree model that breaks statements into premises and assigns a factuality score based on the probability of all parent premises. Manakul et al. ( 2023 ) detects hallucinations by sampling multiple responses and using self-consistency to identify discrepancies. Despite these significant efforts, limitations remain. Most existing work either focuses solely on factual precision or on in-context recall, overlooking the role of the query in generation (Li et al., 2023 ; Yang et al., 2023 ; Niu et al., 2024 ) ( e.g. , scoring both outputs equally in Figure 2 ), or treats the query as a whole (Zhang et al., 2024a ) , which results in coarse-grained evaluation. Hallucination benchmarks . Recent work on hallucination detection for LLMs includes HaluEval (Li et al., 2023 ) (synthetic and natural responses), FELM (Chen et al., 2023 ) (natural responses across domains), RAGTruth (Niu et al., 2024 ) (RAG hallucinations), and InfoBench (Qin et al., 2024 ) (instruction-following via query decomposition). These benchmarks mainly focus on factual hallucinations or require manual annotation. In contrast, FaithQA is, to our knowledge, the first to assess non-factual hallucinations from a query-centric perspective. Although Zhang et al. ( 2024b ) also discusses a related topic, their work primarily explores the causes of intent hallucination from a training corpus perspective. In contrast, our paper provides a comprehensive evaluation metric along with an extensive benchmark for systematic testing. FaithEval (Ming et al., 2025 ) investigates hallucination in RAG settings by evaluating whether model outputs remain faithful to externally retrieved contexts, particularly under conditions involving unanswerable or contradictory evidence. FaithQA adopts a similar RAG setup but shifts the focus from context alignment to query alignment. It introduces a novel, query-centric perspective that evaluates whether model responses accurately fulfill the user’s query. Our experimental results align with the findings from FaithEval and reveal that when LLMs are presented with relevant yet incomplete or noisy retrievals, they frequently exhibit omission-style intent hallucination, failing to address all aspects of the original query.",
            "3 Preliminary": "For a complex query containing multiple conditions, studies report that the model produces responses that only partially satisfy the conditions. To further investigate this, we outline our key insights for intent hallucination in this paper.",
            "4 Detecting Intent Hallucination": "Based on the definition of intent constraints and intent hallucinations, we introduce Constraint Score , a new evaluation metric that detects intent hallucination based on intent constraints. To operationalize the constraint mapping function C ⁢ ( ⋅ ) 𝐶 ⋅ C(\\cdot) italic_C ( ⋅ ) defined earlier, we develop a multi-step process that systematically extracts and categorizes the intent constraint set from queries. Our method has high flexibility and accommodates different queries involving RAG. The prompt template appears in Appendix§ D.2 .",
            "5 FaithQA Benchmark": "We here introduce FaithQA benchmark, the first benchmark that focuses on intent hallucination, with 20,068 queries across four different task setups. The primary goal of FaithQA is to elicit the two fundamental causes of intent hallucination: (1) Omission , where the LLM ignores part of the query, and (2) Misinterpretation , where the LLM misunderstands parts of the query. Please refer to Table 1 for statistical details, and Table 2 for representative examples from FaithQA . Please refer to Appendix§ E for dataset construction details.",
            "6 Experiment Settings": "Baselines . Following Li et al. ( 2023 ); Mündler et al. ( 2024 ); Yang et al. ( 2023 ) , we adopt a zero-shot prompting strategy as the baseline for detecting intent hallucination. The baseline setup resembles Constraint Score by determining, on a scale from 1 to 10, to what extent the response addresses the query. To ensure the robustness of the baseline, we adopt the Self-Consistency strategy. Please refer to Appendix§ C for more details. Models and hyper-parameters . We evaluate several LLMs, mostly state-of-the-art models in the FaithQA Benchmark: OpenAI’s (OpenAI et al., 2024 ) GPT-4o 1 1 1 We refer to gpt-4o-2024-05-13 and GPT-4o-mini, Meta’s LLaMA3-70B 2 2 2 We refer to Meta-Llama-3-70B-Instruct-Turbo and LLaMA3-7B 3 3 3 We refer to Meta-Llama-3-8B-Instruct-Turbo (Dubey et al., 2024 ) , Anthropic’s Claude-3.5 4 4 4 We refer to claude-3-5-sonnet-20240620 and Claude-3 5 5 5 We refer to claude-3-sonnet-20240229 , and Mistral-7B 6 6 6 We refer to Mistral-7B-Instruct-v0.3 (Jiang et al., 2023 ) . For all baselines, we set the temperature τ = 0.3 𝜏 0.3 \\tau=0.3 italic_τ = 0.3 . For Constraint Score , we use GPT-4o as the default model with temperature τ = 0.3 𝜏 0.3 \\tau=0.3 italic_τ = 0.3 to generate and evaluate. We evaluate LLMs on the test set (150 randomly sampled questions) of FaithQA across every single category and difficulty due to monetary costs, while we encourage future research to leverage the extended version for enhanced evaluation. Evaluation metrics . We report (1) Perfect , indicating the rate of perfect responses (no hallucinated responses, Constraint Score = = = 10 ), and (2) Constraint Scores (CS), the average score of all responses to provide a quantitative perspective. Overview results are in Table 3 . For the Omission dataset’s Fact QA setup, we additionally report the Factual Verifiable Hallucination Rate (Fact)—the proportion of hallucinated responses that are factually accurate upon verification—in Table 4 . Please refer to Appendix§ G for the results of statistical significance tests.",
            "7 Experimental Results": "Baseline is biased . We conduct a human evaluation to grade 1,000 randomly sampled responses. Specifically, we sample 1,000 prompt-response pairs from the Omission Dataset, with 500 from Fact QA and Creative Writing, respectively. The evaluation rubric for human annotators requires calculating the Constraint Score based on how well the response addresses each of the decomposed intent constraints. Figure 3 shows the distribution of deviations from human scores for both the Baseline and Constraint Score , using Kernel Density Estimation (KDE). Constraint Score demonstrates a much tighter distribution centered closer to zero, with 66.3% of the scores falling within one standard deviation. In contrast, the Baseline method displays a wider spread, with a mean deviation of -0.73, whereas the mean deviation for Constraint Score is 0.47. This indicates that the Baseline tends to underestimate compared to human scores. Given the discrete nature of the scores, we choose Mean Squared Error (MSE) for performance evaluation. The MSE for Constraint Score is 0.50, which is significantly lower than the Baseline’s MSE of 4.72. This result highlights that Constraint Score outperforms the Baseline and aligns more closely with human judgment. The number of intent constraints matters . From Table 4 , we observe that as the number of intent constraints increases (from Easy to Hard), the Perfect rate consistently declines. This trend is further corroborated by Table 3 , where we analyze RAG setups on the Misinterpretation Dataset—featuring longer and more complex input queries—and observe an even more pronounced drop in the Perfect rate. These findings suggest a clear pattern: LLM performance tends to degrade as the number of intent constraints grows. Factual check is less effective for larger models . We perform an additional factual check for Fact QA responses; implementation details appear in Appendix§ D.2.3 . An important finding we observe is that as language models increase in size, they tend to produce fewer factually incorrect responses. Table 4 illustrates this trend across models within the same family ( e.g. , GPT-4o vs GPT-4o-mini). Larger models consistently show a lower Factual Verifiable Hallucination Rate, which means it becomes more challenging to detect hallucinations through factual checks as model size grows—they tend to generate intent-hallucinated responses.",
            "8 Discussion and Analysis": "In this section, we report the major hallucination patterns we find in LLM generations. For detailed examples please refer to Appendix§ F.3 .",
            "9 Conclusion": "In this paper, we introduce the concept of Intent Hallucination , a specific form of hallucination that arises when Large Language Models (LLMs) omit or misinterpret crucial elements of complex queries, leading to outputs that diverge from users’ intentions despite potentially being factually accurate. Unlike factual hallucinations, intent hallucinations are subtle, harder to detect, and have largely been overlooked in existing research. To address this gap, we develop FaithQA , the first comprehensive benchmark explicitly designed to evaluate intent hallucination. Comprising 20,068 human-validated queries, FaithQA spans a diverse range of topics and complexity levels, serving as a robust platform for evaluating how effectively models maintain query intent integrity. Our experiments on state-of-the-art models demonstrate that intent hallucination is prevalent and worsens as query complexity increases. Additionally, we introduce Constraint Score , an innovative evaluation metric tailored specifically for detecting intent hallucination. Constraint Score systematically decomposes complex queries into atomic intents, assigns importance-weighted labels to these individual components, and assesses model outputs through fine-grained intent alignment scores. Our evaluation reveals that Constraint Score notably surpasses traditional evaluation methodologies that employ LLM-as-the-judge, which exhibit significant bias compared to human judgment. Through our research, we underscore the necessity for future LLM developments to emphasize not only factual correctness but also intentional alignment with human queries. By providing FaithQA and Constraint Score , we lay a foundation for rigorous, nuanced evaluations of LLM performance, encouraging more precise alignment between model outputs and user intentions. Ultimately, addressing intent hallucination effectively enhances the reliability and applicability of LLMs across diverse, real-world applications.",
            "Limitation": "While we present a first step toward investigating intent hallucinations in LLM, our category is still at a rather coarse level with only 2 types of major causes (omit, misinterpret) and 4 types of tasks (Fact QA, Creative Writing, Response Evaluation, Content Analysis). Future work should investigate sub-categorizations of these tasks, or other new tasks under new setups (like inference time reasoning). Future work can also investigate how to better quantify and detect intent hallucination in an even more fine-grained way, like from layer-level detection. Finally, we did not include any reasoning models ( e.g. , o1 series or deepseek-r1) due to their release date (there was only o1 three months ago, deepseek-r1 was not released until last month) and computational cost.",
            "Ethics Statement": "Based on direct communication with our institution’s IRB office, this line of research is exempt from IRB, and the information obtained during our study is recorded in such a manner that the identity of the human subjects cannot readily be ascertained, directly or through identifiers linked to the subjects. There is no potential risk to participants, and we do not collect any identifiable information from annotators."
        },
        "Tuples": [
            [
                "1 Introduction",
                "Large Language Models (LLMs) have demonstrated utility across various applications (OpenAI et al., 2024 ; Dubey et al., 2024 ) . However, hallucination remains a significant challenge (Ji et al., 2023 ; Huang et al., 2023 ) . In particular, for complex queries containing multiple conditions (Figure 1 ), LLM outputs often deviate from the query, yielding unsatisfactory results. We term this phenomenon “Intent Hallucination”, which has received little attention in current research (Min et al., 2023 ; Hou et al., 2024 ; Manakul et al., 2023 ) . Unlike factual hallucination (Li et al., 2023 ; Cao et al., 2021 ) , which researchers can directly detect through search-based fact-checking (Sellam et al., 2020 ; Min et al., 2023 ) , detecting and evaluating intent hallucination poses more challenges. This is because complex queries often contain duplicate intents, and LLMs often satisfy only a portion of them, making dissatisfaction difficult to detect or quantify. Furthermore, as LLMs continue to advance, users tend to provide these stronger LLMs with increasingly complicated queries, which even humans find difficult to understand. This trend highlights the need for LLMs to be not only factually correct but also intentionally aligned with human beings. Our paper addresses two under-explored questions: (1) Why do LLMs tend to exhibit Intent Hallucination? and (2) How can we detect Intent Hallucination? Answering these questions is vital for LLM applications that rely on both factual accuracy and faithful intent alignment. For the first question, we propose that LLMs’ omission ( e.g. , ignoring query components) or misinterpretation ( e.g. , responding to invented query components) over word-level meaning constitutes the fundamental cause of intent hallucination. To further investigate, we introduce FaithQA , the first benchmark specifically designed to address intent hallucination’s two key scenarios: omission and misinterpretation. FaithQA consists of 20,068 queries, validated through extensive human evaluations to ensure quality. FaithQA covers a wide range of topics with varying levels of difficulty and proves challenging even for state-of-the-art models. Our benchmark reveals that increasing query complexity correlates with a higher rate of intent hallucination. To address the second question, we introduce Constraint Score , a new evaluation metric that focuses on detecting intent hallucination. Our approach involves two major steps: (1) decomposing the query by concepts and actions, then converting it into a series of short statements, each representing a specific requirement the generation must meet; and (2) assigning an importance-weighted binary label to each constraint, which enables fine-grained evaluation. Our human evaluation shows that Constraint Score significantly outperforms LLM-as-the-judge (Manakul et al., 2023 ; Mishra et al., 2024 ; Sriramanan et al., 2024 ) , as LLM judgers tend to provide biased evaluations compared with human judgment. Taken together, our key contributions include: (1) We propose the concept of intent hallucination beyond the existing category of factual hallucination; (2) We develop FaithQA , the first benchmark that focuses on evaluating intent hallucination. Our result shows that intent hallucination represents a prevalent phenomenon even for state-of-the-art LLMs; (3) We introduce Constraint Score , a novel evaluation metric that automatically assesses LLM generations by breaking down the query into intent constraints and computing a weighted score. Our analysis shows that Constraint Score significantly outperforms pure LLM grading baselines, which tend to be biased."
            ],
            [
                "2 Related Works",
                "Hallucinations in LLMs . In LLMs, \"hallucination\" refers to outputs that are nonfactual, irrelevant, or fabricated. This issue arises in tasks such as question answering (Sellam et al., 2020 ) , translation (Lee et al., 2018 ) , summarization (Durmus et al., 2020 ) , and dialogue (Balakrishnan et al., 2019 ) , as noted in several studies (Ji et al., 2023 ; Azaria and Mitchell, 2023 ; Huang et al., 2023 ; Cao et al., 2021 ) . To address this issue, many efforts aim to detect and mitigate hallucinations. Min et al. ( 2023 ) evaluate factual accuracy by checking core facts (atomic facts) in each sentence against reliable sources such as Wikipedia. Hou et al. ( 2024 ) propose a hidden Markov tree model that breaks statements into premises and assigns a factuality score based on the probability of all parent premises. Manakul et al. ( 2023 ) detects hallucinations by sampling multiple responses and using self-consistency to identify discrepancies. Despite these significant efforts, limitations remain. Most existing work either focuses solely on factual precision or on in-context recall, overlooking the role of the query in generation (Li et al., 2023 ; Yang et al., 2023 ; Niu et al., 2024 ) ( e.g. , scoring both outputs equally in Figure 2 ), or treats the query as a whole (Zhang et al., 2024a ) , which results in coarse-grained evaluation. Hallucination benchmarks . Recent work on hallucination detection for LLMs includes HaluEval (Li et al., 2023 ) (synthetic and natural responses), FELM (Chen et al., 2023 ) (natural responses across domains), RAGTruth (Niu et al., 2024 ) (RAG hallucinations), and InfoBench (Qin et al., 2024 ) (instruction-following via query decomposition). These benchmarks mainly focus on factual hallucinations or require manual annotation. In contrast, FaithQA is, to our knowledge, the first to assess non-factual hallucinations from a query-centric perspective. Although Zhang et al. ( 2024b ) also discusses a related topic, their work primarily explores the causes of intent hallucination from a training corpus perspective. In contrast, our paper provides a comprehensive evaluation metric along with an extensive benchmark for systematic testing. FaithEval (Ming et al., 2025 ) investigates hallucination in RAG settings by evaluating whether model outputs remain faithful to externally retrieved contexts, particularly under conditions involving unanswerable or contradictory evidence. FaithQA adopts a similar RAG setup but shifts the focus from context alignment to query alignment. It introduces a novel, query-centric perspective that evaluates whether model responses accurately fulfill the user’s query. Our experimental results align with the findings from FaithEval and reveal that when LLMs are presented with relevant yet incomplete or noisy retrievals, they frequently exhibit omission-style intent hallucination, failing to address all aspects of the original query."
            ],
            [
                "3 Preliminary",
                "For a complex query containing multiple conditions, studies report that the model produces responses that only partially satisfy the conditions. To further investigate this, we outline our key insights for intent hallucination in this paper."
            ],
            [
                "4 Detecting Intent Hallucination",
                "Based on the definition of intent constraints and intent hallucinations, we introduce Constraint Score , a new evaluation metric that detects intent hallucination based on intent constraints. To operationalize the constraint mapping function C ⁢ ( ⋅ ) 𝐶 ⋅ C(\\cdot) italic_C ( ⋅ ) defined earlier, we develop a multi-step process that systematically extracts and categorizes the intent constraint set from queries. Our method has high flexibility and accommodates different queries involving RAG. The prompt template appears in Appendix§ D.2 ."
            ],
            [
                "5 FaithQA Benchmark",
                "We here introduce FaithQA benchmark, the first benchmark that focuses on intent hallucination, with 20,068 queries across four different task setups. The primary goal of FaithQA is to elicit the two fundamental causes of intent hallucination: (1) Omission , where the LLM ignores part of the query, and (2) Misinterpretation , where the LLM misunderstands parts of the query. Please refer to Table 1 for statistical details, and Table 2 for representative examples from FaithQA . Please refer to Appendix§ E for dataset construction details."
            ],
            [
                "6 Experiment Settings",
                "Baselines . Following Li et al. ( 2023 ); Mündler et al. ( 2024 ); Yang et al. ( 2023 ) , we adopt a zero-shot prompting strategy as the baseline for detecting intent hallucination. The baseline setup resembles Constraint Score by determining, on a scale from 1 to 10, to what extent the response addresses the query. To ensure the robustness of the baseline, we adopt the Self-Consistency strategy. Please refer to Appendix§ C for more details. Models and hyper-parameters . We evaluate several LLMs, mostly state-of-the-art models in the FaithQA Benchmark: OpenAI’s (OpenAI et al., 2024 ) GPT-4o 1 1 1 We refer to gpt-4o-2024-05-13 and GPT-4o-mini, Meta’s LLaMA3-70B 2 2 2 We refer to Meta-Llama-3-70B-Instruct-Turbo and LLaMA3-7B 3 3 3 We refer to Meta-Llama-3-8B-Instruct-Turbo (Dubey et al., 2024 ) , Anthropic’s Claude-3.5 4 4 4 We refer to claude-3-5-sonnet-20240620 and Claude-3 5 5 5 We refer to claude-3-sonnet-20240229 , and Mistral-7B 6 6 6 We refer to Mistral-7B-Instruct-v0.3 (Jiang et al., 2023 ) . For all baselines, we set the temperature τ = 0.3 𝜏 0.3 \\tau=0.3 italic_τ = 0.3 . For Constraint Score , we use GPT-4o as the default model with temperature τ = 0.3 𝜏 0.3 \\tau=0.3 italic_τ = 0.3 to generate and evaluate. We evaluate LLMs on the test set (150 randomly sampled questions) of FaithQA across every single category and difficulty due to monetary costs, while we encourage future research to leverage the extended version for enhanced evaluation. Evaluation metrics . We report (1) Perfect , indicating the rate of perfect responses (no hallucinated responses, Constraint Score = = = 10 ), and (2) Constraint Scores (CS), the average score of all responses to provide a quantitative perspective. Overview results are in Table 3 . For the Omission dataset’s Fact QA setup, we additionally report the Factual Verifiable Hallucination Rate (Fact)—the proportion of hallucinated responses that are factually accurate upon verification—in Table 4 . Please refer to Appendix§ G for the results of statistical significance tests."
            ],
            [
                "7 Experimental Results",
                "Baseline is biased . We conduct a human evaluation to grade 1,000 randomly sampled responses. Specifically, we sample 1,000 prompt-response pairs from the Omission Dataset, with 500 from Fact QA and Creative Writing, respectively. The evaluation rubric for human annotators requires calculating the Constraint Score based on how well the response addresses each of the decomposed intent constraints. Figure 3 shows the distribution of deviations from human scores for both the Baseline and Constraint Score , using Kernel Density Estimation (KDE). Constraint Score demonstrates a much tighter distribution centered closer to zero, with 66.3% of the scores falling within one standard deviation. In contrast, the Baseline method displays a wider spread, with a mean deviation of -0.73, whereas the mean deviation for Constraint Score is 0.47. This indicates that the Baseline tends to underestimate compared to human scores. Given the discrete nature of the scores, we choose Mean Squared Error (MSE) for performance evaluation. The MSE for Constraint Score is 0.50, which is significantly lower than the Baseline’s MSE of 4.72. This result highlights that Constraint Score outperforms the Baseline and aligns more closely with human judgment. The number of intent constraints matters . From Table 4 , we observe that as the number of intent constraints increases (from Easy to Hard), the Perfect rate consistently declines. This trend is further corroborated by Table 3 , where we analyze RAG setups on the Misinterpretation Dataset—featuring longer and more complex input queries—and observe an even more pronounced drop in the Perfect rate. These findings suggest a clear pattern: LLM performance tends to degrade as the number of intent constraints grows. Factual check is less effective for larger models . We perform an additional factual check for Fact QA responses; implementation details appear in Appendix§ D.2.3 . An important finding we observe is that as language models increase in size, they tend to produce fewer factually incorrect responses. Table 4 illustrates this trend across models within the same family ( e.g. , GPT-4o vs GPT-4o-mini). Larger models consistently show a lower Factual Verifiable Hallucination Rate, which means it becomes more challenging to detect hallucinations through factual checks as model size grows—they tend to generate intent-hallucinated responses."
            ],
            [
                "8 Discussion and Analysis",
                "In this section, we report the major hallucination patterns we find in LLM generations. For detailed examples please refer to Appendix§ F.3 ."
            ],
            [
                "9 Conclusion",
                "In this paper, we introduce the concept of Intent Hallucination , a specific form of hallucination that arises when Large Language Models (LLMs) omit or misinterpret crucial elements of complex queries, leading to outputs that diverge from users’ intentions despite potentially being factually accurate. Unlike factual hallucinations, intent hallucinations are subtle, harder to detect, and have largely been overlooked in existing research. To address this gap, we develop FaithQA , the first comprehensive benchmark explicitly designed to evaluate intent hallucination. Comprising 20,068 human-validated queries, FaithQA spans a diverse range of topics and complexity levels, serving as a robust platform for evaluating how effectively models maintain query intent integrity. Our experiments on state-of-the-art models demonstrate that intent hallucination is prevalent and worsens as query complexity increases. Additionally, we introduce Constraint Score , an innovative evaluation metric tailored specifically for detecting intent hallucination. Constraint Score systematically decomposes complex queries into atomic intents, assigns importance-weighted labels to these individual components, and assesses model outputs through fine-grained intent alignment scores. Our evaluation reveals that Constraint Score notably surpasses traditional evaluation methodologies that employ LLM-as-the-judge, which exhibit significant bias compared to human judgment. Through our research, we underscore the necessity for future LLM developments to emphasize not only factual correctness but also intentional alignment with human queries. By providing FaithQA and Constraint Score , we lay a foundation for rigorous, nuanced evaluations of LLM performance, encouraging more precise alignment between model outputs and user intentions. Ultimately, addressing intent hallucination effectively enhances the reliability and applicability of LLMs across diverse, real-world applications."
            ],
            [
                "Limitation",
                "While we present a first step toward investigating intent hallucinations in LLM, our category is still at a rather coarse level with only 2 types of major causes (omit, misinterpret) and 4 types of tasks (Fact QA, Creative Writing, Response Evaluation, Content Analysis). Future work should investigate sub-categorizations of these tasks, or other new tasks under new setups (like inference time reasoning). Future work can also investigate how to better quantify and detect intent hallucination in an even more fine-grained way, like from layer-level detection. Finally, we did not include any reasoning models ( e.g. , o1 series or deepseek-r1) due to their release date (there was only o1 three months ago, deepseek-r1 was not released until last month) and computational cost."
            ],
            [
                "Ethics Statement",
                "Based on direct communication with our institution’s IRB office, this line of research is exempt from IRB, and the information obtained during our study is recorded in such a manner that the identity of the human subjects cannot readily be ascertained, directly or through identifiers linked to the subjects. There is no potential risk to participants, and we do not collect any identifiable information from annotators."
            ]
        ],
        "section_summaries": [
            [
                "1 Introduction",
                "Large Language Models (LLMs) have demonstrated utility across various applications (OpenAI et al., 2024 ; Dubey et al., 2024 ) . However, hallucination remains a significant challenge (Ji et al., 2023 ; Huang et al., 2023 ) . In particular, for complex queries containing multiple conditions (Figure 1 ), LLM outputs often deviate from the query, yielding unsatisfactory results. We term this phenomenon “Intent Hallucination”, which has received little attention in current research (Min et al., 2023 ; Hou et al., 2024 ; Manakul et al., 2023 ) . Unlike factual hallucination (Li et al., 2023 ; Cao et al., 2021 ) , which researchers can directly detect through search-based fact-checking (Sellam et al., 2020 ; Min et al., 2023 ) , detecting and evaluating intent hallucination poses more challenges. This is because complex queries often contain duplicate intents, and LLMs often satisfy only a portion of them, making dissatisfaction difficult to detect or quantify. Furthermore, as LLMs continue to advance, users tend to provide these stronger LLMs with increasingly complicated queries, which even humans find difficult to understand. This trend highlights the need for LLMs to be not only factually correct but also intentionally aligned with human beings. Our paper addresses two under-explored questions: (1) Why do LLMs tend to exhibit Intent Hallucination? and (2) How can we detect Intent Hallucination? Answering these questions is vital for LLM applications that rely on both factual accuracy and faithful intent alignment. For the first question, we propose that LLMs’ omission ( e.g. , ignoring query components) or misinterpretation ( e.g. , responding to invented query components) over word-level meaning constitutes the fundamental cause of intent hallucination. To further investigate, we introduce FaithQA , the first benchmark specifically designed to address intent hallucination’s two key scenarios: omission and misinterpretation. FaithQA consists of 20,068 queries, validated through extensive human evaluations to ensure quality. FaithQA covers a wide range of topics with varying levels of difficulty and proves challenging even for state-of-the-art models. Our benchmark reveals that increasing query complexity correlates with a higher rate of intent hallucination. To address the second question, we introduce Constraint Score , a new evaluation metric that focuses on detecting intent hallucination. Our approach involves two major steps: (1) decomposing the query by concepts and actions, then converting it into a series of short statements, each representing a specific requirement the generation must meet; and (2) assigning an importance-weighted binary label to each constraint, which enables fine-grained evaluation. Our human evaluation shows that Constraint Score significantly outperforms LLM-as-the-judge (Manakul et al., 2023 ; Mishra et al., 2024 ; Sriramanan et al., 2024 ) , as LLM judgers tend to provide biased evaluations compared with human judgment. Taken together, our key contributions include: (1) We propose the concept of intent hallucination beyond the existing category of factual hallucination; (2) We develop FaithQA , the first benchmark that focuses on evaluating intent hallucination. Our result shows that intent hallucination represents a prevalent phenomenon even for state-of-the-art LLMs; (3) We introduce Constraint Score , a novel evaluation metric that automatically assesses LLM generations by breaking down the query into intent constraints and computing a weighted score. Our analysis shows that Constraint Score significantly outperforms pure LLM grading baselines, which tend to be biased.",
                "OpenAI et al. (2024), Dubey et al. (2024) demonstrated utility of Large Language Models (LLMs). However, Ji et al. (2023) and Huang et al. (2023) found that hallucination is a significant challenge. We term this \"Intent Hallucination\" (Min et al., 2023; Hou et al., 2024; Manakul et al., 2023), distinct from factual hallucination (Li et al., 2023; Cao et al., 2021). Our paper addresses two questions: why LLMs exhibit Intent Hallucination and how to detect it. We propose that LLMs' omission or misinterpretation of word-level meaning causes intent hallucination. To investigate, we introduce FaithQA (20,068 queries), the first benchmark specifically designed to address intent hallucination's two key scenarios: omission and misinterpretation. Our benchmark reveals a correlation between increasing query complexity and a higher rate of intent hallucination. We also introduce Constraint Score , a new evaluation metric that focuses on detecting intent hallucination, which outperforms LLM-as-the-judge in human evaluation.\n\n Key figures:\n- 2024 (OpenAI et al., Dubey et al.)\n- 2023 (Ji et al., Huang et al., Li et al., Min et al.)\n- 2021 (Cao et al.)\n- 20,068 (FaithQA queries)"
            ],
            [
                "2 Related Works",
                "Hallucinations in LLMs . In LLMs, \"hallucination\" refers to outputs that are nonfactual, irrelevant, or fabricated. This issue arises in tasks such as question answering (Sellam et al., 2020 ) , translation (Lee et al., 2018 ) , summarization (Durmus et al., 2020 ) , and dialogue (Balakrishnan et al., 2019 ) , as noted in several studies (Ji et al., 2023 ; Azaria and Mitchell, 2023 ; Huang et al., 2023 ; Cao et al., 2021 ) . To address this issue, many efforts aim to detect and mitigate hallucinations. Min et al. ( 2023 ) evaluate factual accuracy by checking core facts (atomic facts) in each sentence against reliable sources such as Wikipedia. Hou et al. ( 2024 ) propose a hidden Markov tree model that breaks statements into premises and assigns a factuality score based on the probability of all parent premises. Manakul et al. ( 2023 ) detects hallucinations by sampling multiple responses and using self-consistency to identify discrepancies. Despite these significant efforts, limitations remain. Most existing work either focuses solely on factual precision or on in-context recall, overlooking the role of the query in generation (Li et al., 2023 ; Yang et al., 2023 ; Niu et al., 2024 ) ( e.g. , scoring both outputs equally in Figure 2 ), or treats the query as a whole (Zhang et al., 2024a ) , which results in coarse-grained evaluation. Hallucination benchmarks . Recent work on hallucination detection for LLMs includes HaluEval (Li et al., 2023 ) (synthetic and natural responses), FELM (Chen et al., 2023 ) (natural responses across domains), RAGTruth (Niu et al., 2024 ) (RAG hallucinations), and InfoBench (Qin et al., 2024 ) (instruction-following via query decomposition). These benchmarks mainly focus on factual hallucinations or require manual annotation. In contrast, FaithQA is, to our knowledge, the first to assess non-factual hallucinations from a query-centric perspective. Although Zhang et al. ( 2024b ) also discusses a related topic, their work primarily explores the causes of intent hallucination from a training corpus perspective. In contrast, our paper provides a comprehensive evaluation metric along with an extensive benchmark for systematic testing. FaithEval (Ming et al., 2025 ) investigates hallucination in RAG settings by evaluating whether model outputs remain faithful to externally retrieved contexts, particularly under conditions involving unanswerable or contradictory evidence. FaithQA adopts a similar RAG setup but shifts the focus from context alignment to query alignment. It introduces a novel, query-centric perspective that evaluates whether model responses accurately fulfill the user’s query. Our experimental results align with the findings from FaithEval and reveal that when LLMs are presented with relevant yet incomplete or noisy retrievals, they frequently exhibit omission-style intent hallucination, failing to address all aspects of the original query.",
                "Hallucinations in LLMs refer to nonfactual, irrelevant, or fabricated outputs. This issue arises in tasks like question answering and translation. Efforts aim to detect and mitigate hallucinations through methods such as factuality scoring and hidden Markov tree models. Existing work focuses on factual precision or in-context recall but overlooks the query's role. Recent benchmarks include HaluEval, FELM, RAGTruth, and InfoBench, which mainly focus on factual hallucinations. FaithQA assesses non-factual hallucinations from a query-centric perspective and introduces a novel evaluation metric for systematic testing."
            ],
            [
                "3 Preliminary",
                "For a complex query containing multiple conditions, studies report that the model produces responses that only partially satisfy the conditions. To further investigate this, we outline our key insights for intent hallucination in this paper.",
                "Studies report that the model produces partial responses to complex queries with multiple conditions, leading to intent hallucination."
            ],
            [
                "4 Detecting Intent Hallucination",
                "Based on the definition of intent constraints and intent hallucinations, we introduce Constraint Score , a new evaluation metric that detects intent hallucination based on intent constraints. To operationalize the constraint mapping function C ⁢ ( ⋅ ) 𝐶 ⋅ C(\\cdot) italic_C ( ⋅ ) defined earlier, we develop a multi-step process that systematically extracts and categorizes the intent constraint set from queries. Our method has high flexibility and accommodates different queries involving RAG. The prompt template appears in Appendix§ D.2 .",
                "Constraint Score metric detects intent hallucination based on intent constraints, operationalizing a multi-step process to extract and categorize intent constraint sets from queries, accommodating different queries involving RAG."
            ],
            [
                "5 FaithQA Benchmark",
                "We here introduce FaithQA benchmark, the first benchmark that focuses on intent hallucination, with 20,068 queries across four different task setups. The primary goal of FaithQA is to elicit the two fundamental causes of intent hallucination: (1) Omission , where the LLM ignores part of the query, and (2) Misinterpretation , where the LLM misunderstands parts of the query. Please refer to Table 1 for statistical details, and Table 2 for representative examples from FaithQA . Please refer to Appendix§ E for dataset construction details.",
                "FaithQA has 20,068 queries across four task setups, focusing on intent hallucination due to Omission or Misinterpretation."
            ],
            [
                "6 Experiment Settings",
                "Baselines . Following Li et al. ( 2023 ); Mündler et al. ( 2024 ); Yang et al. ( 2023 ) , we adopt a zero-shot prompting strategy as the baseline for detecting intent hallucination. The baseline setup resembles Constraint Score by determining, on a scale from 1 to 10, to what extent the response addresses the query. To ensure the robustness of the baseline, we adopt the Self-Consistency strategy. Please refer to Appendix§ C for more details. Models and hyper-parameters . We evaluate several LLMs, mostly state-of-the-art models in the FaithQA Benchmark: OpenAI’s (OpenAI et al., 2024 ) GPT-4o 1 1 1 We refer to gpt-4o-2024-05-13 and GPT-4o-mini, Meta’s LLaMA3-70B 2 2 2 We refer to Meta-Llama-3-70B-Instruct-Turbo and LLaMA3-7B 3 3 3 We refer to Meta-Llama-3-8B-Instruct-Turbo (Dubey et al., 2024 ) , Anthropic’s Claude-3.5 4 4 4 We refer to claude-3-5-sonnet-20240620 and Claude-3 5 5 5 We refer to claude-3-sonnet-20240229 , and Mistral-7B 6 6 6 We refer to Mistral-7B-Instruct-v0.3 (Jiang et al., 2023 ) . For all baselines, we set the temperature τ = 0.3 𝜏 0.3 \\tau=0.3 italic_τ = 0.3 . For Constraint Score , we use GPT-4o as the default model with temperature τ = 0.3 𝜏 0.3 \\tau=0.3 italic_τ = 0.3 to generate and evaluate. We evaluate LLMs on the test set (150 randomly sampled questions) of FaithQA across every single category and difficulty due to monetary costs, while we encourage future research to leverage the extended version for enhanced evaluation. Evaluation metrics . We report (1) Perfect , indicating the rate of perfect responses (no hallucinated responses, Constraint Score = = = 10 ), and (2) Constraint Scores (CS), the average score of all responses to provide a quantitative perspective. Overview results are in Table 3 . For the Omission dataset’s Fact QA setup, we additionally report the Factual Verifiable Hallucination Rate (Fact)—the proportion of hallucinated responses that are factually accurate upon verification—in Table 4 . Please refer to Appendix§ G for the results of statistical significance tests.",
                "Li et al. (2023) and Mündler et al. (2024) baselines adopt zero-shot prompting, using Constraint Score with a scale from 1 to 10. \nThe Self-Consistency strategy is used for robustness.\nLLMs evaluated include OpenAI's GPT-4o, Meta's LLaMA3-70B and -7B, Anthropic's Claude-3.5 and Claude-3, and Mistral-7B.\nTemperature τ = 0.3 is set for all baselines.\nEvaluation metrics include Perfect and Constraint Scores.\nResults are reported in Tables 3 and 4."
            ],
            [
                "7 Experimental Results",
                "Baseline is biased . We conduct a human evaluation to grade 1,000 randomly sampled responses. Specifically, we sample 1,000 prompt-response pairs from the Omission Dataset, with 500 from Fact QA and Creative Writing, respectively. The evaluation rubric for human annotators requires calculating the Constraint Score based on how well the response addresses each of the decomposed intent constraints. Figure 3 shows the distribution of deviations from human scores for both the Baseline and Constraint Score , using Kernel Density Estimation (KDE). Constraint Score demonstrates a much tighter distribution centered closer to zero, with 66.3% of the scores falling within one standard deviation. In contrast, the Baseline method displays a wider spread, with a mean deviation of -0.73, whereas the mean deviation for Constraint Score is 0.47. This indicates that the Baseline tends to underestimate compared to human scores. Given the discrete nature of the scores, we choose Mean Squared Error (MSE) for performance evaluation. The MSE for Constraint Score is 0.50, which is significantly lower than the Baseline’s MSE of 4.72. This result highlights that Constraint Score outperforms the Baseline and aligns more closely with human judgment. The number of intent constraints matters . From Table 4 , we observe that as the number of intent constraints increases (from Easy to Hard), the Perfect rate consistently declines. This trend is further corroborated by Table 3 , where we analyze RAG setups on the Misinterpretation Dataset—featuring longer and more complex input queries—and observe an even more pronounced drop in the Perfect rate. These findings suggest a clear pattern: LLM performance tends to degrade as the number of intent constraints grows. Factual check is less effective for larger models . We perform an additional factual check for Fact QA responses; implementation details appear in Appendix§ D.2.3 . An important finding we observe is that as language models increase in size, they tend to produce fewer factually incorrect responses. Table 4 illustrates this trend across models within the same family ( e.g. , GPT-4o vs GPT-4o-mini). Larger models consistently show a lower Factual Verifiable Hallucination Rate, which means it becomes more challenging to detect hallucinations through factual checks as model size grows—they tend to generate intent-hallucinated responses.",
                "Baseline method is biased and underestimates compared to human scores. Constraint Score outperforms Baseline with lower Mean Squared Error (MSE) of 0.50, compared to MSE of 4.72 for Baseline. LLM performance degrades as the number of intent constraints increases. Larger language models produce fewer factually incorrect responses and generate more challenging hallucinations to detect through factual checks. \n\n Key numbers: \n - 66.3% (scores falling within one standard deviation)\n - -0.73 (mean deviation for Baseline)\n - 0.47 (mean deviation for Constraint Score)\n - 0.50 (MSE for Constraint Score)\n - 4.72 (MSE for Baseline)\n - 1,000 (randomly sampled responses)\n\n Key abbreviations: \n- LLM\n- KDE (Kernel Density Estimation)"
            ],
            [
                "8 Discussion and Analysis",
                "In this section, we report the major hallucination patterns we find in LLM generations. For detailed examples please refer to Appendix§ F.3 .",
                "Major hallucination patterns are reported in LLM generations."
            ],
            [
                "9 Conclusion",
                "In this paper, we introduce the concept of Intent Hallucination , a specific form of hallucination that arises when Large Language Models (LLMs) omit or misinterpret crucial elements of complex queries, leading to outputs that diverge from users’ intentions despite potentially being factually accurate. Unlike factual hallucinations, intent hallucinations are subtle, harder to detect, and have largely been overlooked in existing research. To address this gap, we develop FaithQA , the first comprehensive benchmark explicitly designed to evaluate intent hallucination. Comprising 20,068 human-validated queries, FaithQA spans a diverse range of topics and complexity levels, serving as a robust platform for evaluating how effectively models maintain query intent integrity. Our experiments on state-of-the-art models demonstrate that intent hallucination is prevalent and worsens as query complexity increases. Additionally, we introduce Constraint Score , an innovative evaluation metric tailored specifically for detecting intent hallucination. Constraint Score systematically decomposes complex queries into atomic intents, assigns importance-weighted labels to these individual components, and assesses model outputs through fine-grained intent alignment scores. Our evaluation reveals that Constraint Score notably surpasses traditional evaluation methodologies that employ LLM-as-the-judge, which exhibit significant bias compared to human judgment. Through our research, we underscore the necessity for future LLM developments to emphasize not only factual correctness but also intentional alignment with human queries. By providing FaithQA and Constraint Score , we lay a foundation for rigorous, nuanced evaluations of LLM performance, encouraging more precise alignment between model outputs and user intentions. Ultimately, addressing intent hallucination effectively enhances the reliability and applicability of LLMs across diverse, real-world applications.",
                "Intent Hallucination is a specific type of hallucination where Large Language Models (LLMs) misinterpret or omit crucial elements in complex queries. FaithQA, a benchmark with 20,068 human-validated queries, evaluates intent hallucination and demonstrates that it worsens as query complexity increases. Constraint Score, an innovative evaluation metric, detects intent hallucination more effectively than traditional methodologies."
            ],
            [
                "Limitation",
                "While we present a first step toward investigating intent hallucinations in LLM, our category is still at a rather coarse level with only 2 types of major causes (omit, misinterpret) and 4 types of tasks (Fact QA, Creative Writing, Response Evaluation, Content Analysis). Future work should investigate sub-categorizations of these tasks, or other new tasks under new setups (like inference time reasoning). Future work can also investigate how to better quantify and detect intent hallucination in an even more fine-grained way, like from layer-level detection. Finally, we did not include any reasoning models ( e.g. , o1 series or deepseek-r1) due to their release date (there was only o1 three months ago, deepseek-r1 was not released until last month) and computational cost.",
                "Our category has 2 major causes and 4 tasks. Future work can sub-categorize these tasks or investigate new ones. We did not include reasoning models due to their recent release (o1 series released 3 months ago, deepseek-r1 last month) and computational cost."
            ],
            [
                "Ethics Statement",
                "Based on direct communication with our institution’s IRB office, this line of research is exempt from IRB, and the information obtained during our study is recorded in such a manner that the identity of the human subjects cannot readily be ascertained, directly or through identifiers linked to the subjects. There is no potential risk to participants, and we do not collect any identifiable information from annotators.",
                "Our research is exempt from IRB (Institutional Review Board), there's no risk to participants, and no identifiable information is collected from annotators."
            ]
        ],
        "general_summary": "**Research Overview**\n\nThis paper evaluates intent hallucination in large language models (LLMs), a phenomenon where LLMs omit or misinterpret crucial elements in complex queries. The authors introduce FaithQA, a novel benchmark containing 20,068 human-validated queries that assesses intent hallucination due to omission or misinterpretation.\n\n**Methodology & Approach**\n\nTo evaluate intent hallucination, the authors propose the Constraint Score metric, which detects intent hallucination based on intent constraints extracted from queries. They also introduce FaithQA, a benchmark with four task setups focusing on intent hallucination due to Omission or Misinterpretation. The authors use various LLMs, including OpenAI's GPT-4o and Meta's LLaMA3-70B, and evaluate their performance using the Constraint Score metric.\n\n**Key Contributions & Findings**\n\nThe key findings of this research are:\n\n* Intent hallucination is a common issue in state-of-the-art LLMs.\n* The phenomenon stems from omission or misinterpretation of LLMs.\n* The authors introduce an automatic evaluation metric, Constraint Score, which detects intent hallucination more effectively than traditional methodologies.\n\n**Technical Details**\n\nThe paper uses a multi-step process to extract and categorize intent constraint sets from queries, accommodating different queries involving retrieval-augmented generation (RAG). The Constraint Score metric is operationalized using a scale from 1 to 10. The authors also use the Self-Consistency strategy for robustness.\n\n**Limitations & Future Work**\n\nThe current limitations of this research are that it only considers two major causes and four tasks, which can be further sub-categorized or investigated in future work. The authors also note that they did not include reasoning models due to their recent release and computational cost.\n\n**Practical Implications**\n\nThis research has practical implications for the development of more accurate and reliable LLMs. By detecting intent hallucination, developers can improve the performance of LLMs and reduce the risk of generating factually incorrect responses."
    }
]